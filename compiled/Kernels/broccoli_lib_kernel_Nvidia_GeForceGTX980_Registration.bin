//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_52, texmode_independent
.address_size 32

	// .globl	CalculatePhaseDifferencesAndCertainties
.global .samplerref volume_sampler_nearest = { addr_mode_0 = clamp_to_edge, addr_mode_1 = clamp_to_edge, addr_mode_2 = clamp_to_edge, filter_mode = nearest, force_unnormalized_coords = 1 };
.global .samplerref volume_sampler_linear = { addr_mode_0 = clamp_to_edge, addr_mode_1 = clamp_to_edge, addr_mode_2 = clamp_to_edge, filter_mode = linear, force_unnormalized_coords = 1 };
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.entry CalculatePhaseDifferencesAndCertainties(
	.param .u32 .ptr .global .align 4 CalculatePhaseDifferencesAndCertainties_param_0,
	.param .u32 .ptr .global .align 4 CalculatePhaseDifferencesAndCertainties_param_1,
	.param .u32 .ptr .global .align 8 CalculatePhaseDifferencesAndCertainties_param_2,
	.param .u32 .ptr .global .align 8 CalculatePhaseDifferencesAndCertainties_param_3,
	.param .u32 CalculatePhaseDifferencesAndCertainties_param_4,
	.param .u32 CalculatePhaseDifferencesAndCertainties_param_5,
	.param .u32 CalculatePhaseDifferencesAndCertainties_param_6
)
{
	.local .align 4 .b8 	__local_depot0[28];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<29>;
	.reg .f32 	%f<102>;
	.reg .b32 	%r<155>;


	mov.u32 	%r154, __local_depot0;
	cvta.local.u32 	%SP, %r154;
	ld.param.u32 	%r42, [CalculatePhaseDifferencesAndCertainties_param_0];
	ld.param.u32 	%r43, [CalculatePhaseDifferencesAndCertainties_param_1];
	ld.param.u32 	%r44, [CalculatePhaseDifferencesAndCertainties_param_2];
	ld.param.u32 	%r45, [CalculatePhaseDifferencesAndCertainties_param_3];
	ld.param.u32 	%r46, [CalculatePhaseDifferencesAndCertainties_param_4];
	ld.param.u32 	%r47, [CalculatePhaseDifferencesAndCertainties_param_5];
	ld.param.u32 	%r48, [CalculatePhaseDifferencesAndCertainties_param_6];
	add.u32 	%r49, %SP, 0;
	cvta.to.local.u32 	%r1, %r49;
	mov.u32 	%r50, %ctaid.x;
	mov.u32 	%r51, %ntid.x;
	mov.b32	%r52, %envreg3;
	mad.lo.s32 	%r53, %r50, %r51, %r52;
	mov.u32 	%r54, %tid.x;
	add.s32 	%r2, %r53, %r54;
	mov.u32 	%r55, %ctaid.y;
	mov.u32 	%r56, %ntid.y;
	mov.b32	%r57, %envreg4;
	mad.lo.s32 	%r58, %r55, %r56, %r57;
	mov.u32 	%r59, %tid.y;
	add.s32 	%r3, %r58, %r59;
	mov.u32 	%r60, %ctaid.z;
	mov.u32 	%r61, %ntid.z;
	mov.b32	%r62, %envreg5;
	mad.lo.s32 	%r63, %r60, %r61, %r62;
	mov.u32 	%r64, %tid.z;
	add.s32 	%r4, %r63, %r64;
	setp.ge.s32	%p1, %r2, %r46;
	setp.ge.s32	%p2, %r3, %r47;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r4, %r48;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB0_27;

	mad.lo.s32 	%r65, %r4, %r47, %r3;
	mad.lo.s32 	%r5, %r65, %r46, %r2;
	shl.b32 	%r66, %r5, 3;
	add.s32 	%r67, %r44, %r66;
	add.s32 	%r68, %r45, %r66;
	ld.global.v2.f32 	{%f32, %f33}, [%r67];
	ld.global.v2.f32 	{%f34, %f35}, [%r68];
	mul.f32 	%f4, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f34, %f4;
	mul.f32 	%f5, %f32, %f35;
	neg.f32 	%f38, %f5;
	fma.rn.f32 	%f39, %f33, %f34, %f38;
	abs.f32 	%f6, %f37;
	abs.f32 	%f7, %f39;
	setp.eq.f32	%p6, %f6, 0f00000000;
	setp.eq.f32	%p7, %f7, 0f00000000;
	and.pred  	%p8, %p6, %p7;
	mov.b32 	 %r6, %f37;
	mov.b32 	 %r69, %f39;
	and.b32  	%r7, %r69, -2147483648;
	@%p8 bra 	BB0_5;
	bra.uni 	BB0_2;

BB0_5:
	shr.s32 	%r76, %r6, 31;
	and.b32  	%r77, %r76, 1078530011;
	or.b32  	%r78, %r77, %r7;
	mov.b32 	 %f96, %r78;
	bra.uni 	BB0_6;

BB0_2:
	setp.eq.f32	%p9, %f6, 0f7F800000;
	setp.eq.f32	%p10, %f7, 0f7F800000;
	and.pred  	%p11, %p9, %p10;
	@%p11 bra 	BB0_4;
	bra.uni 	BB0_3;

BB0_4:
	shr.s32 	%r72, %r6, 31;
	and.b32  	%r73, %r72, 13483017;
	add.s32 	%r74, %r73, 1061752795;
	or.b32  	%r75, %r74, %r7;
	mov.b32 	 %f96, %r75;
	bra.uni 	BB0_6;

BB0_3:
	max.f32 	%f40, %f7, %f6;
	min.f32 	%f41, %f7, %f6;
	div.full.f32 	%f42, %f41, %f40;
	mul.rn.f32 	%f43, %f42, %f42;
	mov.f32 	%f44, 0fC0B59883;
	mov.f32 	%f45, 0fBF52C7EA;
	fma.rn.f32 	%f46, %f43, %f45, %f44;
	mov.f32 	%f47, 0fC0D21907;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	mul.f32 	%f49, %f43, %f48;
	mul.f32 	%f50, %f42, %f49;
	add.f32 	%f51, %f43, 0f41355DC0;
	mov.f32 	%f52, 0f41E6BD60;
	fma.rn.f32 	%f53, %f51, %f43, %f52;
	mov.f32 	%f54, 0f419D92C8;
	fma.rn.f32 	%f55, %f53, %f43, %f54;
	rcp.approx.f32 	%f56, %f55;
	fma.rn.f32 	%f57, %f50, %f56, %f42;
	mov.f32 	%f58, 0f3FC90FDB;
	sub.f32 	%f59, %f58, %f57;
	setp.gt.f32	%p12, %f7, %f6;
	selp.f32	%f60, %f59, %f57, %p12;
	mov.f32 	%f61, 0f40490FDB;
	sub.f32 	%f62, %f61, %f60;
	setp.lt.s32	%p13, %r6, 0;
	selp.f32	%f63, %f62, %f60, %p13;
	mov.b32 	 %r70, %f63;
	or.b32  	%r71, %r70, %r7;
	mov.b32 	 %f64, %r71;
	add.f32 	%f65, %f6, %f7;
	setp.gtu.f32	%p14, %f65, 0f7F800000;
	selp.f32	%f96, %f65, %f64, %p14;

BB0_6:
	neg.f32 	%f66, %f4;
	fma.rn.f32 	%f12, %f32, %f34, %f66;
	fma.rn.f32 	%f13, %f33, %f34, %f5;
	mul.f32 	%f97, %f96, 0f3F000000;
	abs.f32 	%f67, %f97;
	setp.neu.f32	%p15, %f67, 0f7F800000;
	@%p15 bra 	BB0_8;

	mov.f32 	%f68, 0f00000000;
	mul.rn.f32 	%f97, %f97, %f68;

BB0_8:
	mul.f32 	%f69, %f97, 0f3F22F983;
	cvt.rni.s32.f32	%r153, %f69;
	cvt.rn.f32.s32	%f70, %r153;
	neg.f32 	%f71, %f70;
	mov.f32 	%f72, 0f3FC90FDA;
	fma.rn.f32 	%f73, %f71, %f72, %f97;
	mov.f32 	%f74, 0f33A22168;
	fma.rn.f32 	%f75, %f71, %f74, %f73;
	mov.f32 	%f76, 0f27C234C5;
	fma.rn.f32 	%f98, %f71, %f76, %f75;
	abs.f32 	%f77, %f97;
	setp.leu.f32	%p16, %f77, 0f47CE4780;
	@%p16 bra 	BB0_16;

	mov.b32 	 %r9, %f97;
	and.b32  	%r10, %r9, -2147483648;
	shl.b32 	%r82, %r9, 8;
	or.b32  	%r11, %r82, -2147483648;
	mov.u32 	%r146, 0;
	mov.u32 	%r147, %r146;
	mov.u32 	%r143, %r146;
	mov.u32 	%r144, %r1;
	mov.pred 	%p17, 0;
	@%p17 bra 	BB0_11;

BB0_10:
	shl.b32 	%r88, %r143, 2;
	mov.u32 	%r89, __cudart_i2opi_f;
	add.s32 	%r90, %r89, %r88;
	ld.const.u32 	%r85, [%r90];
	// inline asm
	{
	mad.lo.cc.u32   %r83, %r85, %r11, %r147;
	madc.hi.u32     %r147, %r85, %r11,  0;
	}
	// inline asm
	st.local.u32 	[%r144], %r83;
	add.s32 	%r143, %r143, 1;
	setp.lt.s32	%p18, %r143, 6;
	add.s32 	%r91, %r88, %r1;
	add.s32 	%r144, %r91, 4;
	mov.u32 	%r146, %r147;
	@%p18 bra 	BB0_10;

BB0_11:
	bfe.u32 	%r92, %r9, 23, 8;
	add.s32 	%r93, %r92, -128;
	shr.u32 	%r94, %r93, 5;
	mov.u32 	%r95, 4;
	sub.s32 	%r96, %r95, %r94;
	st.local.u32 	[%r144], %r146;
	shl.b32 	%r97, %r96, 2;
	add.s32 	%r98, %r97, %r1;
	ld.local.u32 	%r148, [%r98+8];
	ld.local.u32 	%r149, [%r98+4];
	bfe.u32 	%r24, %r9, 23, 5;
	setp.eq.s32	%p19, %r24, 0;
	@%p19 bra 	BB0_13;

	mov.u32 	%r99, 32;
	sub.s32 	%r100, %r99, %r24;
	shr.u32 	%r101, %r149, %r100;
	shl.b32 	%r102, %r148, %r24;
	add.s32 	%r148, %r101, %r102;
	add.s32 	%r142, %r98, 8;
	ld.local.u32 	%r103, [%r142+-8];
	shr.u32 	%r104, %r103, %r100;
	shl.b32 	%r105, %r149, %r24;
	add.s32 	%r149, %r104, %r105;

BB0_13:
	shr.u32 	%r106, %r149, 30;
	shl.b32 	%r107, %r148, 2;
	add.s32 	%r150, %r106, %r107;
	shl.b32 	%r30, %r149, 2;
	shr.u32 	%r108, %r150, 31;
	shr.u32 	%r109, %r148, 30;
	add.s32 	%r31, %r108, %r109;
	setp.eq.s32	%p20, %r108, 0;
	mov.u32 	%r151, %r10;
	mov.u32 	%r152, %r30;
	@%p20 bra 	BB0_15;

	not.b32 	%r110, %r150;
	neg.s32 	%r32, %r30;
	setp.eq.s32	%p21, %r30, 0;
	selp.u32	%r111, 1, 0, %p21;
	add.s32 	%r150, %r111, %r110;
	xor.b32  	%r34, %r10, -2147483648;
	mov.u32 	%r151, %r34;
	mov.u32 	%r152, %r32;

BB0_15:
	mov.u32 	%r36, %r151;
	neg.s32 	%r112, %r31;
	setp.ne.s32	%p22, %r10, 0;
	selp.b32	%r153, %r112, %r31, %p22;
	clz.b32 	%r113, %r150;
	setp.ne.s32	%p23, %r113, 0;
	shl.b32 	%r114, %r150, %r113;
	mov.u32 	%r115, 32;
	sub.s32 	%r116, %r115, %r113;
	shr.u32 	%r117, %r152, %r116;
	add.s32 	%r118, %r117, %r114;
	selp.b32	%r119, %r118, %r150, %p23;
	mul.lo.s32 	%r120, %r119, -921707870;
	mov.u32 	%r121, -921707870;
	mul.hi.u32 	%r122, %r119, %r121;
	setp.gt.s32	%p24, %r122, 0;
	shl.b32 	%r123, %r122, 1;
	shr.u32 	%r124, %r120, 31;
	add.s32 	%r125, %r124, %r123;
	selp.b32	%r126, %r125, %r122, %p24;
	selp.b32	%r127, -1, 0, %p24;
	mov.u32 	%r128, 126;
	sub.s32 	%r129, %r128, %r113;
	add.s32 	%r130, %r129, %r127;
	shl.b32 	%r131, %r130, 23;
	add.s32 	%r132, %r126, 1;
	shr.u32 	%r133, %r132, 7;
	add.s32 	%r134, %r133, 1;
	shr.u32 	%r135, %r134, 1;
	add.s32 	%r136, %r135, %r131;
	or.b32  	%r137, %r136, %r36;
	mov.b32 	 %f98, %r137;

BB0_16:
	mul.rn.f32 	%f20, %f98, %f98;
	add.s32 	%r40, %r153, 1;
	and.b32  	%r41, %r40, 1;
	setp.eq.s32	%p25, %r41, 0;
	@%p25 bra 	BB0_18;

	mov.f32 	%f78, 0fBAB6061A;
	mov.f32 	%f79, 0f37CCF5CE;
	fma.rn.f32 	%f99, %f79, %f20, %f78;
	bra.uni 	BB0_19;

BB0_18:
	mov.f32 	%f80, 0f3C08839E;
	mov.f32 	%f81, 0fB94CA1F9;
	fma.rn.f32 	%f99, %f81, %f20, %f80;

BB0_19:
	@%p25 bra 	BB0_21;

	mov.f32 	%f82, 0f3D2AAAA5;
	fma.rn.f32 	%f83, %f99, %f20, %f82;
	mov.f32 	%f84, 0fBF000000;
	fma.rn.f32 	%f100, %f83, %f20, %f84;
	bra.uni 	BB0_22;

BB0_21:
	mov.f32 	%f85, 0fBE2AAAA3;
	fma.rn.f32 	%f86, %f99, %f20, %f85;
	mov.f32 	%f87, 0f00000000;
	fma.rn.f32 	%f100, %f86, %f20, %f87;

BB0_22:
	fma.rn.f32 	%f101, %f100, %f98, %f98;
	@%p25 bra 	BB0_24;

	mov.f32 	%f88, 0f3F800000;
	fma.rn.f32 	%f101, %f100, %f20, %f88;

BB0_24:
	and.b32  	%r138, %r40, 2;
	setp.eq.s32	%p28, %r138, 0;
	@%p28 bra 	BB0_26;

	mov.f32 	%f89, 0f00000000;
	mov.f32 	%f90, 0fBF800000;
	fma.rn.f32 	%f101, %f101, %f90, %f89;

BB0_26:
	shl.b32 	%r139, %r5, 2;
	add.s32 	%r140, %r42, %r139;
	st.global.f32 	[%r140], %f96;
	mul.f32 	%f91, %f13, %f13;
	fma.rn.f32 	%f92, %f12, %f12, %f91;
	sqrt.approx.f32 	%f93, %f92;
	mul.f32 	%f94, %f101, %f93;
	mul.f32 	%f95, %f101, %f94;
	add.s32 	%r141, %r43, %r139;
	st.global.f32 	[%r141], %f95;

BB0_27:
	ret;
}

	// .globl	CalculatePhaseGradientsX
.entry CalculatePhaseGradientsX(
	.param .u32 .ptr .global .align 4 CalculatePhaseGradientsX_param_0,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsX_param_1,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsX_param_2,
	.param .u32 CalculatePhaseGradientsX_param_3,
	.param .u32 CalculatePhaseGradientsX_param_4,
	.param .u32 CalculatePhaseGradientsX_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<45>;


	ld.param.u32 	%r7, [CalculatePhaseGradientsX_param_0];
	ld.param.u32 	%r8, [CalculatePhaseGradientsX_param_1];
	ld.param.u32 	%r9, [CalculatePhaseGradientsX_param_2];
	ld.param.u32 	%r10, [CalculatePhaseGradientsX_param_3];
	ld.param.u32 	%r11, [CalculatePhaseGradientsX_param_4];
	ld.param.u32 	%r12, [CalculatePhaseGradientsX_param_5];
	mov.b32	%r13, %envreg3;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %ntid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	mov.u32 	%r17, %tid.x;
	add.s32 	%r1, %r16, %r17;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %ntid.y;
	mov.b32	%r20, %envreg4;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %tid.y;
	add.s32 	%r2, %r21, %r22;
	mov.u32 	%r23, %ctaid.z;
	mov.u32 	%r24, %ntid.z;
	mov.b32	%r25, %envreg5;
	mad.lo.s32 	%r26, %r23, %r24, %r25;
	mov.u32 	%r27, %tid.z;
	add.s32 	%r3, %r26, %r27;
	setp.ge.s32	%p1, %r1, %r10;
	add.s32 	%r28, %r1, 1;
	setp.ge.s32	%p2, %r28, %r10;
	or.pred  	%p3, %p1, %p2;
	setp.lt.s32	%p4, %r1, 1;
	or.pred  	%p5, %p3, %p4;
	setp.ge.s32	%p6, %r2, %r11;
	or.pred  	%p7, %p5, %p6;
	setp.ge.s32	%p8, %r3, %r12;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB1_7;

	mad.lo.s32 	%r29, %r3, %r11, %r2;
	mad.lo.s32 	%r4, %r29, %r10, %r1;
	shl.b32 	%r30, %r4, 3;
	add.s32 	%r31, %r8, %r30;
	ld.global.v2.f32 	{%f7, %f8}, [%r31+8];
	ld.global.v2.f32 	{%f10, %f11}, [%r31];
	mul.f32 	%f15, %f8, %f11;
	fma.rn.f32 	%f16, %f7, %f10, %f15;
	add.f32 	%f17, %f16, 0f00000000;
	mul.f32 	%f18, %f7, %f11;
	neg.f32 	%f19, %f18;
	fma.rn.f32 	%f20, %f8, %f10, %f19;
	add.f32 	%f21, %f20, 0f00000000;
	ld.global.v2.f32 	{%f22, %f23}, [%r31+-8];
	mul.f32 	%f26, %f11, %f23;
	fma.rn.f32 	%f27, %f10, %f22, %f26;
	add.f32 	%f28, %f17, %f27;
	mul.f32 	%f29, %f10, %f23;
	neg.f32 	%f30, %f29;
	fma.rn.f32 	%f31, %f11, %f22, %f30;
	add.f32 	%f32, %f21, %f31;
	add.s32 	%r32, %r9, %r30;
	ld.global.v2.f32 	{%f33, %f34}, [%r32+8];
	ld.global.v2.f32 	{%f36, %f37}, [%r32];
	mul.f32 	%f41, %f34, %f37;
	fma.rn.f32 	%f42, %f33, %f36, %f41;
	add.f32 	%f43, %f28, %f42;
	mul.f32 	%f44, %f33, %f37;
	neg.f32 	%f45, %f44;
	fma.rn.f32 	%f46, %f34, %f36, %f45;
	add.f32 	%f47, %f32, %f46;
	ld.global.v2.f32 	{%f48, %f49}, [%r32+-8];
	mul.f32 	%f52, %f37, %f49;
	fma.rn.f32 	%f53, %f36, %f48, %f52;
	add.f32 	%f54, %f43, %f53;
	mul.f32 	%f55, %f36, %f49;
	neg.f32 	%f56, %f55;
	fma.rn.f32 	%f57, %f37, %f48, %f56;
	add.f32 	%f58, %f47, %f57;
	abs.f32 	%f1, %f54;
	abs.f32 	%f2, %f58;
	setp.eq.f32	%p10, %f1, 0f00000000;
	setp.eq.f32	%p11, %f2, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.b32 	 %r5, %f54;
	mov.b32 	 %r33, %f58;
	and.b32  	%r6, %r33, -2147483648;
	@%p12 bra 	BB1_5;
	bra.uni 	BB1_2;

BB1_5:
	shr.s32 	%r40, %r5, 31;
	and.b32  	%r41, %r40, 1078530011;
	or.b32  	%r42, %r41, %r6;
	mov.b32 	 %f85, %r42;
	bra.uni 	BB1_6;

BB1_2:
	setp.eq.f32	%p13, %f1, 0f7F800000;
	setp.eq.f32	%p14, %f2, 0f7F800000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB1_4;
	bra.uni 	BB1_3;

BB1_4:
	shr.s32 	%r36, %r5, 31;
	and.b32  	%r37, %r36, 13483017;
	add.s32 	%r38, %r37, 1061752795;
	or.b32  	%r39, %r38, %r6;
	mov.b32 	 %f85, %r39;
	bra.uni 	BB1_6;

BB1_3:
	max.f32 	%f59, %f2, %f1;
	min.f32 	%f60, %f2, %f1;
	div.full.f32 	%f61, %f60, %f59;
	mul.rn.f32 	%f62, %f61, %f61;
	mov.f32 	%f63, 0fC0B59883;
	mov.f32 	%f64, 0fBF52C7EA;
	fma.rn.f32 	%f65, %f62, %f64, %f63;
	mov.f32 	%f66, 0fC0D21907;
	fma.rn.f32 	%f67, %f65, %f62, %f66;
	mul.f32 	%f68, %f62, %f67;
	mul.f32 	%f69, %f61, %f68;
	add.f32 	%f70, %f62, 0f41355DC0;
	mov.f32 	%f71, 0f41E6BD60;
	fma.rn.f32 	%f72, %f70, %f62, %f71;
	mov.f32 	%f73, 0f419D92C8;
	fma.rn.f32 	%f74, %f72, %f62, %f73;
	rcp.approx.f32 	%f75, %f74;
	fma.rn.f32 	%f76, %f69, %f75, %f61;
	mov.f32 	%f77, 0f3FC90FDB;
	sub.f32 	%f78, %f77, %f76;
	setp.gt.f32	%p16, %f2, %f1;
	selp.f32	%f79, %f78, %f76, %p16;
	mov.f32 	%f80, 0f40490FDB;
	sub.f32 	%f81, %f80, %f79;
	setp.lt.s32	%p17, %r5, 0;
	selp.f32	%f82, %f81, %f79, %p17;
	mov.b32 	 %r34, %f82;
	or.b32  	%r35, %r34, %r6;
	mov.b32 	 %f83, %r35;
	add.f32 	%f84, %f1, %f2;
	setp.gtu.f32	%p18, %f84, 0f7F800000;
	selp.f32	%f85, %f84, %f83, %p18;

BB1_6:
	shl.b32 	%r43, %r4, 2;
	add.s32 	%r44, %r7, %r43;
	st.global.f32 	[%r44], %f85;

BB1_7:
	ret;
}

	// .globl	CalculatePhaseGradientsY
.entry CalculatePhaseGradientsY(
	.param .u32 .ptr .global .align 4 CalculatePhaseGradientsY_param_0,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsY_param_1,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsY_param_2,
	.param .u32 CalculatePhaseGradientsY_param_3,
	.param .u32 CalculatePhaseGradientsY_param_4,
	.param .u32 CalculatePhaseGradientsY_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;


	ld.param.u32 	%r8, [CalculatePhaseGradientsY_param_0];
	ld.param.u32 	%r9, [CalculatePhaseGradientsY_param_1];
	ld.param.u32 	%r10, [CalculatePhaseGradientsY_param_2];
	ld.param.u32 	%r11, [CalculatePhaseGradientsY_param_3];
	ld.param.u32 	%r12, [CalculatePhaseGradientsY_param_4];
	ld.param.u32 	%r13, [CalculatePhaseGradientsY_param_5];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r17, %r15, %r16, %r14;
	mov.u32 	%r18, %tid.x;
	add.s32 	%r1, %r17, %r18;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %ntid.y;
	mov.b32	%r21, %envreg4;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %tid.y;
	add.s32 	%r2, %r22, %r23;
	mov.u32 	%r24, %ctaid.z;
	mov.u32 	%r25, %ntid.z;
	mov.b32	%r26, %envreg5;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %tid.z;
	add.s32 	%r3, %r27, %r28;
	setp.ge.s32	%p1, %r1, %r11;
	setp.ge.s32	%p2, %r2, %r12;
	or.pred  	%p3, %p1, %p2;
	add.s32 	%r4, %r2, 1;
	setp.ge.s32	%p4, %r4, %r12;
	or.pred  	%p5, %p3, %p4;
	setp.lt.s32	%p6, %r2, 1;
	or.pred  	%p7, %p5, %p6;
	setp.ge.s32	%p8, %r3, %r13;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB2_7;

	mul.lo.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r2;
	mad.lo.s32 	%r5, %r30, %r11, %r1;
	add.s32 	%r31, %r29, %r4;
	mad.lo.s32 	%r32, %r31, %r11, %r1;
	shl.b32 	%r33, %r32, 3;
	add.s32 	%r34, %r9, %r33;
	shl.b32 	%r35, %r11, 3;
	sub.s32 	%r36, %r34, %r35;
	ld.global.v2.f32 	{%f7, %f8}, [%r34];
	ld.global.v2.f32 	{%f10, %f11}, [%r36];
	mul.f32 	%f15, %f8, %f11;
	fma.rn.f32 	%f16, %f7, %f10, %f15;
	add.f32 	%f17, %f16, 0f00000000;
	mul.f32 	%f18, %f7, %f11;
	neg.f32 	%f19, %f18;
	fma.rn.f32 	%f20, %f8, %f10, %f19;
	add.f32 	%f21, %f20, 0f00000000;
	sub.s32 	%r37, %r36, %r35;
	ld.global.v2.f32 	{%f22, %f23}, [%r37];
	mul.f32 	%f26, %f11, %f23;
	fma.rn.f32 	%f27, %f10, %f22, %f26;
	add.f32 	%f28, %f17, %f27;
	mul.f32 	%f29, %f10, %f23;
	neg.f32 	%f30, %f29;
	fma.rn.f32 	%f31, %f11, %f22, %f30;
	add.f32 	%f32, %f21, %f31;
	add.s32 	%r38, %r10, %r33;
	sub.s32 	%r39, %r38, %r35;
	ld.global.v2.f32 	{%f33, %f34}, [%r38];
	ld.global.v2.f32 	{%f36, %f37}, [%r39];
	mul.f32 	%f41, %f34, %f37;
	fma.rn.f32 	%f42, %f33, %f36, %f41;
	add.f32 	%f43, %f28, %f42;
	mul.f32 	%f44, %f33, %f37;
	neg.f32 	%f45, %f44;
	fma.rn.f32 	%f46, %f34, %f36, %f45;
	add.f32 	%f47, %f32, %f46;
	sub.s32 	%r40, %r39, %r35;
	ld.global.v2.f32 	{%f48, %f49}, [%r40];
	mul.f32 	%f52, %f37, %f49;
	fma.rn.f32 	%f53, %f36, %f48, %f52;
	add.f32 	%f54, %f43, %f53;
	mul.f32 	%f55, %f36, %f49;
	neg.f32 	%f56, %f55;
	fma.rn.f32 	%f57, %f37, %f48, %f56;
	add.f32 	%f58, %f47, %f57;
	abs.f32 	%f1, %f54;
	abs.f32 	%f2, %f58;
	setp.eq.f32	%p10, %f1, 0f00000000;
	setp.eq.f32	%p11, %f2, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.b32 	 %r6, %f54;
	mov.b32 	 %r41, %f58;
	and.b32  	%r7, %r41, -2147483648;
	@%p12 bra 	BB2_5;
	bra.uni 	BB2_2;

BB2_5:
	shr.s32 	%r48, %r6, 31;
	and.b32  	%r49, %r48, 1078530011;
	or.b32  	%r50, %r49, %r7;
	mov.b32 	 %f85, %r50;
	bra.uni 	BB2_6;

BB2_2:
	setp.eq.f32	%p13, %f1, 0f7F800000;
	setp.eq.f32	%p14, %f2, 0f7F800000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB2_4;
	bra.uni 	BB2_3;

BB2_4:
	shr.s32 	%r44, %r6, 31;
	and.b32  	%r45, %r44, 13483017;
	add.s32 	%r46, %r45, 1061752795;
	or.b32  	%r47, %r46, %r7;
	mov.b32 	 %f85, %r47;
	bra.uni 	BB2_6;

BB2_3:
	max.f32 	%f59, %f2, %f1;
	min.f32 	%f60, %f2, %f1;
	div.full.f32 	%f61, %f60, %f59;
	mul.rn.f32 	%f62, %f61, %f61;
	mov.f32 	%f63, 0fC0B59883;
	mov.f32 	%f64, 0fBF52C7EA;
	fma.rn.f32 	%f65, %f62, %f64, %f63;
	mov.f32 	%f66, 0fC0D21907;
	fma.rn.f32 	%f67, %f65, %f62, %f66;
	mul.f32 	%f68, %f62, %f67;
	mul.f32 	%f69, %f61, %f68;
	add.f32 	%f70, %f62, 0f41355DC0;
	mov.f32 	%f71, 0f41E6BD60;
	fma.rn.f32 	%f72, %f70, %f62, %f71;
	mov.f32 	%f73, 0f419D92C8;
	fma.rn.f32 	%f74, %f72, %f62, %f73;
	rcp.approx.f32 	%f75, %f74;
	fma.rn.f32 	%f76, %f69, %f75, %f61;
	mov.f32 	%f77, 0f3FC90FDB;
	sub.f32 	%f78, %f77, %f76;
	setp.gt.f32	%p16, %f2, %f1;
	selp.f32	%f79, %f78, %f76, %p16;
	mov.f32 	%f80, 0f40490FDB;
	sub.f32 	%f81, %f80, %f79;
	setp.lt.s32	%p17, %r6, 0;
	selp.f32	%f82, %f81, %f79, %p17;
	mov.b32 	 %r42, %f82;
	or.b32  	%r43, %r42, %r7;
	mov.b32 	 %f83, %r43;
	add.f32 	%f84, %f1, %f2;
	setp.gtu.f32	%p18, %f84, 0f7F800000;
	selp.f32	%f85, %f84, %f83, %p18;

BB2_6:
	shl.b32 	%r51, %r5, 2;
	add.s32 	%r52, %r8, %r51;
	st.global.f32 	[%r52], %f85;

BB2_7:
	ret;
}

	// .globl	CalculatePhaseGradientsZ
.entry CalculatePhaseGradientsZ(
	.param .u32 .ptr .global .align 4 CalculatePhaseGradientsZ_param_0,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsZ_param_1,
	.param .u32 .ptr .global .align 8 CalculatePhaseGradientsZ_param_2,
	.param .u32 CalculatePhaseGradientsZ_param_3,
	.param .u32 CalculatePhaseGradientsZ_param_4,
	.param .u32 CalculatePhaseGradientsZ_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;


	ld.param.u32 	%r8, [CalculatePhaseGradientsZ_param_0];
	ld.param.u32 	%r9, [CalculatePhaseGradientsZ_param_1];
	ld.param.u32 	%r10, [CalculatePhaseGradientsZ_param_2];
	ld.param.u32 	%r11, [CalculatePhaseGradientsZ_param_3];
	ld.param.u32 	%r12, [CalculatePhaseGradientsZ_param_4];
	ld.param.u32 	%r13, [CalculatePhaseGradientsZ_param_5];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r17, %r15, %r16, %r14;
	mov.u32 	%r18, %tid.x;
	add.s32 	%r1, %r17, %r18;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %ntid.y;
	mov.b32	%r21, %envreg4;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %tid.y;
	add.s32 	%r2, %r22, %r23;
	mov.u32 	%r24, %ctaid.z;
	mov.u32 	%r25, %ntid.z;
	mov.b32	%r26, %envreg5;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %tid.z;
	add.s32 	%r3, %r27, %r28;
	setp.ge.s32	%p1, %r1, %r11;
	setp.ge.s32	%p2, %r2, %r12;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r13;
	or.pred  	%p5, %p3, %p4;
	add.s32 	%r4, %r3, 1;
	setp.ge.s32	%p6, %r4, %r13;
	or.pred  	%p7, %p5, %p6;
	setp.lt.s32	%p8, %r3, 1;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB3_7;

	mad.lo.s32 	%r29, %r3, %r12, %r2;
	mad.lo.s32 	%r5, %r29, %r11, %r1;
	mad.lo.s32 	%r30, %r4, %r12, %r2;
	mad.lo.s32 	%r31, %r30, %r11, %r1;
	shl.b32 	%r32, %r31, 3;
	add.s32 	%r33, %r9, %r32;
	mul.lo.s32 	%r34, %r12, %r11;
	shl.b32 	%r35, %r34, 3;
	sub.s32 	%r36, %r33, %r35;
	ld.global.v2.f32 	{%f7, %f8}, [%r33];
	ld.global.v2.f32 	{%f10, %f11}, [%r36];
	mul.f32 	%f15, %f8, %f11;
	fma.rn.f32 	%f16, %f7, %f10, %f15;
	add.f32 	%f17, %f16, 0f00000000;
	mul.f32 	%f18, %f7, %f11;
	neg.f32 	%f19, %f18;
	fma.rn.f32 	%f20, %f8, %f10, %f19;
	add.f32 	%f21, %f20, 0f00000000;
	sub.s32 	%r37, %r36, %r35;
	ld.global.v2.f32 	{%f22, %f23}, [%r37];
	mul.f32 	%f26, %f11, %f23;
	fma.rn.f32 	%f27, %f10, %f22, %f26;
	add.f32 	%f28, %f17, %f27;
	mul.f32 	%f29, %f10, %f23;
	neg.f32 	%f30, %f29;
	fma.rn.f32 	%f31, %f11, %f22, %f30;
	add.f32 	%f32, %f21, %f31;
	add.s32 	%r38, %r10, %r32;
	sub.s32 	%r39, %r38, %r35;
	ld.global.v2.f32 	{%f33, %f34}, [%r38];
	ld.global.v2.f32 	{%f36, %f37}, [%r39];
	mul.f32 	%f41, %f34, %f37;
	fma.rn.f32 	%f42, %f33, %f36, %f41;
	add.f32 	%f43, %f28, %f42;
	mul.f32 	%f44, %f33, %f37;
	neg.f32 	%f45, %f44;
	fma.rn.f32 	%f46, %f34, %f36, %f45;
	add.f32 	%f47, %f32, %f46;
	sub.s32 	%r40, %r39, %r35;
	ld.global.v2.f32 	{%f48, %f49}, [%r40];
	mul.f32 	%f52, %f37, %f49;
	fma.rn.f32 	%f53, %f36, %f48, %f52;
	add.f32 	%f54, %f43, %f53;
	mul.f32 	%f55, %f36, %f49;
	neg.f32 	%f56, %f55;
	fma.rn.f32 	%f57, %f37, %f48, %f56;
	add.f32 	%f58, %f47, %f57;
	abs.f32 	%f1, %f54;
	abs.f32 	%f2, %f58;
	setp.eq.f32	%p10, %f1, 0f00000000;
	setp.eq.f32	%p11, %f2, 0f00000000;
	and.pred  	%p12, %p10, %p11;
	mov.b32 	 %r6, %f54;
	mov.b32 	 %r41, %f58;
	and.b32  	%r7, %r41, -2147483648;
	@%p12 bra 	BB3_5;
	bra.uni 	BB3_2;

BB3_5:
	shr.s32 	%r48, %r6, 31;
	and.b32  	%r49, %r48, 1078530011;
	or.b32  	%r50, %r49, %r7;
	mov.b32 	 %f85, %r50;
	bra.uni 	BB3_6;

BB3_2:
	setp.eq.f32	%p13, %f1, 0f7F800000;
	setp.eq.f32	%p14, %f2, 0f7F800000;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	BB3_4;
	bra.uni 	BB3_3;

BB3_4:
	shr.s32 	%r44, %r6, 31;
	and.b32  	%r45, %r44, 13483017;
	add.s32 	%r46, %r45, 1061752795;
	or.b32  	%r47, %r46, %r7;
	mov.b32 	 %f85, %r47;
	bra.uni 	BB3_6;

BB3_3:
	max.f32 	%f59, %f2, %f1;
	min.f32 	%f60, %f2, %f1;
	div.full.f32 	%f61, %f60, %f59;
	mul.rn.f32 	%f62, %f61, %f61;
	mov.f32 	%f63, 0fC0B59883;
	mov.f32 	%f64, 0fBF52C7EA;
	fma.rn.f32 	%f65, %f62, %f64, %f63;
	mov.f32 	%f66, 0fC0D21907;
	fma.rn.f32 	%f67, %f65, %f62, %f66;
	mul.f32 	%f68, %f62, %f67;
	mul.f32 	%f69, %f61, %f68;
	add.f32 	%f70, %f62, 0f41355DC0;
	mov.f32 	%f71, 0f41E6BD60;
	fma.rn.f32 	%f72, %f70, %f62, %f71;
	mov.f32 	%f73, 0f419D92C8;
	fma.rn.f32 	%f74, %f72, %f62, %f73;
	rcp.approx.f32 	%f75, %f74;
	fma.rn.f32 	%f76, %f69, %f75, %f61;
	mov.f32 	%f77, 0f3FC90FDB;
	sub.f32 	%f78, %f77, %f76;
	setp.gt.f32	%p16, %f2, %f1;
	selp.f32	%f79, %f78, %f76, %p16;
	mov.f32 	%f80, 0f40490FDB;
	sub.f32 	%f81, %f80, %f79;
	setp.lt.s32	%p17, %r6, 0;
	selp.f32	%f82, %f81, %f79, %p17;
	mov.b32 	 %r42, %f82;
	or.b32  	%r43, %r42, %r7;
	mov.b32 	 %f83, %r43;
	add.f32 	%f84, %f1, %f2;
	setp.gtu.f32	%p18, %f84, 0f7F800000;
	selp.f32	%f85, %f84, %f83, %p18;

BB3_6:
	shl.b32 	%r51, %r5, 2;
	add.s32 	%r52, %r8, %r51;
	st.global.f32 	[%r52], %f85;

BB3_7:
	ret;
}

	// .globl	CalculateAMatrixAndHVector2DValuesX
.entry CalculateAMatrixAndHVector2DValuesX(
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesX_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesX_param_1,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesX_param_2,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesX_param_3,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesX_param_4,
	.param .u32 CalculateAMatrixAndHVector2DValuesX_param_5,
	.param .u32 CalculateAMatrixAndHVector2DValuesX_param_6,
	.param .u32 CalculateAMatrixAndHVector2DValuesX_param_7,
	.param .u32 CalculateAMatrixAndHVector2DValuesX_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<60>;


	ld.param.u32 	%r18, [CalculateAMatrixAndHVector2DValuesX_param_0];
	ld.param.u32 	%r19, [CalculateAMatrixAndHVector2DValuesX_param_1];
	ld.param.u32 	%r20, [CalculateAMatrixAndHVector2DValuesX_param_2];
	ld.param.u32 	%r21, [CalculateAMatrixAndHVector2DValuesX_param_3];
	ld.param.u32 	%r22, [CalculateAMatrixAndHVector2DValuesX_param_4];
	ld.param.u32 	%r23, [CalculateAMatrixAndHVector2DValuesX_param_5];
	ld.param.u32 	%r24, [CalculateAMatrixAndHVector2DValuesX_param_6];
	ld.param.u32 	%r25, [CalculateAMatrixAndHVector2DValuesX_param_7];
	ld.param.u32 	%r26, [CalculateAMatrixAndHVector2DValuesX_param_8];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.b32	%r3, %envreg1;
	add.s32 	%r4, %r2, %r3;
	add.s32 	%r27, %r26, -1;
	shr.u32 	%r28, %r27, 31;
	add.s32 	%r29, %r27, %r28;
	shr.s32 	%r59, %r29, 1;
	sub.s32 	%r30, %r24, %r59;
	setp.ge.s32	%p1, %r1, %r59;
	setp.lt.s32	%p2, %r1, %r30;
	and.pred  	%p3, %p2, %p1;
	setp.ge.s32	%p4, %r4, %r59;
	and.pred  	%p5, %p3, %p4;
	sub.s32 	%r31, %r25, %r59;
	setp.lt.s32	%p6, %r4, %r31;
	and.pred  	%p7, %p6, %p5;
	@!%p7 bra 	BB4_5;
	bra.uni 	BB4_1;

BB4_1:
	cvt.rn.f32.s32	%f63, %r1;
	cvt.rn.f32.s32	%f64, %r24;
	add.f32 	%f65, %f64, 0fBF800000;
	neg.f32 	%f66, %f65;
	fma.rn.f32 	%f1, %f66, 0f3F000000, %f63;
	cvt.rn.f32.s32	%f67, %r25;
	add.f32 	%f68, %f67, 0fBF800000;
	neg.f32 	%f69, %f68;
	cvt.rn.f32.s32	%f70, %r4;
	fma.rn.f32 	%f2, %f69, 0f3F000000, %f70;
	sub.s32 	%r6, %r23, %r59;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;
	setp.ge.s32	%p8, %r59, %r6;
	@%p8 bra 	BB4_4;

	cvt.rn.f32.s32	%f85, %r23;
	add.f32 	%f86, %f85, 0fBF800000;
	neg.f32 	%f3, %f86;
	mul.f32 	%f4, %f1, %f1;
	mul.f32 	%f5, %f1, %f2;
	mul.f32 	%f6, %f2, %f2;
	mad.lo.s32 	%r33, %r24, %r4, %r1;
	mad.lo.s32 	%r34, %r23, %r33, %r59;
	shl.b32 	%r35, %r34, 2;
	add.s32 	%r58, %r22, %r35;
	add.s32 	%r57, %r21, %r35;
	add.s32 	%r56, %r20, %r35;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;

BB4_3:
	cvt.rn.f32.s32	%f87, %r59;
	fma.rn.f32 	%f88, %f3, 0f3F000000, %f87;
	ld.global.f32 	%f89, [%r58];
	ld.global.f32 	%f90, [%r57];
	mul.f32 	%f91, %f90, %f89;
	mul.f32 	%f92, %f90, %f91;
	ld.global.f32 	%f93, [%r56];
	mul.f32 	%f94, %f93, %f91;
	add.f32 	%f102, %f102, %f92;
	fma.rn.f32 	%f103, %f88, %f92, %f103;
	fma.rn.f32 	%f104, %f1, %f92, %f104;
	fma.rn.f32 	%f105, %f2, %f92, %f105;
	mul.f32 	%f95, %f88, %f88;
	fma.rn.f32 	%f106, %f95, %f92, %f106;
	mul.f32 	%f96, %f1, %f88;
	fma.rn.f32 	%f107, %f96, %f92, %f107;
	mul.f32 	%f97, %f2, %f88;
	fma.rn.f32 	%f108, %f97, %f92, %f108;
	fma.rn.f32 	%f109, %f4, %f92, %f109;
	fma.rn.f32 	%f110, %f5, %f92, %f110;
	fma.rn.f32 	%f111, %f6, %f92, %f111;
	add.f32 	%f98, %f98, %f94;
	fma.rn.f32 	%f99, %f88, %f94, %f99;
	fma.rn.f32 	%f100, %f1, %f94, %f100;
	fma.rn.f32 	%f101, %f2, %f94, %f101;
	add.s32 	%r58, %r58, 4;
	add.s32 	%r57, %r57, 4;
	add.s32 	%r56, %r56, 4;
	add.s32 	%r59, %r59, 1;
	setp.lt.s32	%p9, %r59, %r6;
	@%p9 bra 	BB4_3;

BB4_4:
	mad.lo.s32 	%r36, %r4, %r24, %r1;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r38, %r18, %r37;
	st.global.f32 	[%r38], %f102;
	mul.lo.s32 	%r39, %r25, %r24;
	shl.b32 	%r40, %r39, 2;
	add.s32 	%r41, %r38, %r40;
	st.global.f32 	[%r41], %f103;
	add.s32 	%r42, %r41, %r40;
	st.global.f32 	[%r42], %f104;
	add.s32 	%r43, %r42, %r40;
	st.global.f32 	[%r43], %f105;
	add.s32 	%r44, %r43, %r40;
	st.global.f32 	[%r44], %f106;
	add.s32 	%r45, %r44, %r40;
	st.global.f32 	[%r45], %f107;
	add.s32 	%r46, %r45, %r40;
	st.global.f32 	[%r46], %f108;
	add.s32 	%r47, %r46, %r40;
	st.global.f32 	[%r47], %f109;
	add.s32 	%r48, %r47, %r40;
	st.global.f32 	[%r48], %f110;
	add.s32 	%r49, %r48, %r40;
	st.global.f32 	[%r49], %f111;
	add.s32 	%r50, %r19, %r37;
	st.global.f32 	[%r50], %f98;
	mad.lo.s32 	%r51, %r39, 3, %r36;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r53, %r19, %r52;
	st.global.f32 	[%r53], %f99;
	add.s32 	%r54, %r53, %r40;
	st.global.f32 	[%r54], %f100;
	add.s32 	%r55, %r54, %r40;
	st.global.f32 	[%r55], %f101;

BB4_5:
	ret;
}

	// .globl	CalculateAMatrixAndHVector2DValuesY
.entry CalculateAMatrixAndHVector2DValuesY(
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesY_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesY_param_1,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesY_param_2,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesY_param_3,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesY_param_4,
	.param .u32 CalculateAMatrixAndHVector2DValuesY_param_5,
	.param .u32 CalculateAMatrixAndHVector2DValuesY_param_6,
	.param .u32 CalculateAMatrixAndHVector2DValuesY_param_7,
	.param .u32 CalculateAMatrixAndHVector2DValuesY_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<64>;


	ld.param.u32 	%r18, [CalculateAMatrixAndHVector2DValuesY_param_0];
	ld.param.u32 	%r19, [CalculateAMatrixAndHVector2DValuesY_param_1];
	ld.param.u32 	%r20, [CalculateAMatrixAndHVector2DValuesY_param_2];
	ld.param.u32 	%r21, [CalculateAMatrixAndHVector2DValuesY_param_3];
	ld.param.u32 	%r22, [CalculateAMatrixAndHVector2DValuesY_param_4];
	ld.param.u32 	%r23, [CalculateAMatrixAndHVector2DValuesY_param_5];
	ld.param.u32 	%r24, [CalculateAMatrixAndHVector2DValuesY_param_6];
	ld.param.u32 	%r25, [CalculateAMatrixAndHVector2DValuesY_param_7];
	ld.param.u32 	%r26, [CalculateAMatrixAndHVector2DValuesY_param_8];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.b32	%r3, %envreg1;
	add.s32 	%r4, %r2, %r3;
	add.s32 	%r27, %r26, -1;
	shr.u32 	%r28, %r27, 31;
	add.s32 	%r29, %r27, %r28;
	shr.s32 	%r63, %r29, 1;
	sub.s32 	%r30, %r24, %r63;
	setp.ge.s32	%p1, %r1, %r63;
	setp.lt.s32	%p2, %r1, %r30;
	and.pred  	%p3, %p2, %p1;
	setp.ge.s32	%p4, %r4, %r63;
	and.pred  	%p5, %p3, %p4;
	sub.s32 	%r31, %r25, %r63;
	setp.lt.s32	%p6, %r4, %r31;
	and.pred  	%p7, %p6, %p5;
	@!%p7 bra 	BB5_5;
	bra.uni 	BB5_1;

BB5_1:
	cvt.rn.f32.s32	%f63, %r1;
	cvt.rn.f32.s32	%f64, %r24;
	add.f32 	%f65, %f64, 0fBF800000;
	neg.f32 	%f66, %f65;
	fma.rn.f32 	%f1, %f66, 0f3F000000, %f63;
	cvt.rn.f32.s32	%f67, %r25;
	add.f32 	%f68, %f67, 0fBF800000;
	neg.f32 	%f69, %f68;
	cvt.rn.f32.s32	%f70, %r4;
	fma.rn.f32 	%f2, %f69, 0f3F000000, %f70;
	sub.s32 	%r6, %r23, %r63;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;
	setp.ge.s32	%p8, %r63, %r6;
	@%p8 bra 	BB5_4;

	cvt.rn.f32.s32	%f85, %r23;
	add.f32 	%f86, %f85, 0fBF800000;
	neg.f32 	%f3, %f86;
	mul.f32 	%f4, %f1, %f1;
	mul.f32 	%f5, %f1, %f2;
	mul.f32 	%f6, %f2, %f2;
	mad.lo.s32 	%r33, %r24, %r4, %r1;
	mad.lo.s32 	%r34, %r23, %r33, %r63;
	shl.b32 	%r35, %r34, 2;
	add.s32 	%r62, %r22, %r35;
	add.s32 	%r61, %r21, %r35;
	add.s32 	%r60, %r20, %r35;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;

BB5_3:
	cvt.rn.f32.s32	%f87, %r63;
	fma.rn.f32 	%f88, %f3, 0f3F000000, %f87;
	ld.global.f32 	%f89, [%r62];
	ld.global.f32 	%f90, [%r61];
	mul.f32 	%f91, %f90, %f89;
	mul.f32 	%f92, %f90, %f91;
	ld.global.f32 	%f93, [%r60];
	mul.f32 	%f94, %f93, %f91;
	add.f32 	%f102, %f102, %f92;
	fma.rn.f32 	%f103, %f88, %f92, %f103;
	fma.rn.f32 	%f104, %f1, %f92, %f104;
	fma.rn.f32 	%f105, %f2, %f92, %f105;
	mul.f32 	%f95, %f88, %f88;
	fma.rn.f32 	%f106, %f95, %f92, %f106;
	mul.f32 	%f96, %f1, %f88;
	fma.rn.f32 	%f107, %f96, %f92, %f107;
	mul.f32 	%f97, %f2, %f88;
	fma.rn.f32 	%f108, %f97, %f92, %f108;
	fma.rn.f32 	%f109, %f4, %f92, %f109;
	fma.rn.f32 	%f110, %f5, %f92, %f110;
	fma.rn.f32 	%f111, %f6, %f92, %f111;
	add.f32 	%f98, %f98, %f94;
	fma.rn.f32 	%f99, %f88, %f94, %f99;
	fma.rn.f32 	%f100, %f1, %f94, %f100;
	fma.rn.f32 	%f101, %f2, %f94, %f101;
	add.s32 	%r62, %r62, 4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r63, %r63, 1;
	setp.lt.s32	%p9, %r63, %r6;
	@%p9 bra 	BB5_3;

BB5_4:
	mul.lo.s32 	%r36, %r24, 10;
	mad.lo.s32 	%r37, %r4, %r24, %r1;
	mad.lo.s32 	%r38, %r36, %r25, %r37;
	shl.b32 	%r39, %r38, 2;
	add.s32 	%r40, %r18, %r39;
	st.global.f32 	[%r40], %f102;
	mul.lo.s32 	%r41, %r25, %r24;
	shl.b32 	%r42, %r41, 2;
	add.s32 	%r43, %r40, %r42;
	st.global.f32 	[%r43], %f103;
	add.s32 	%r44, %r43, %r42;
	st.global.f32 	[%r44], %f104;
	add.s32 	%r45, %r44, %r42;
	st.global.f32 	[%r45], %f105;
	add.s32 	%r46, %r45, %r42;
	st.global.f32 	[%r46], %f106;
	add.s32 	%r47, %r46, %r42;
	st.global.f32 	[%r47], %f107;
	add.s32 	%r48, %r47, %r42;
	st.global.f32 	[%r48], %f108;
	add.s32 	%r49, %r48, %r42;
	st.global.f32 	[%r49], %f109;
	add.s32 	%r50, %r49, %r42;
	st.global.f32 	[%r50], %f110;
	add.s32 	%r51, %r50, %r42;
	st.global.f32 	[%r51], %f111;
	add.s32 	%r52, %r37, %r41;
	shl.b32 	%r53, %r52, 2;
	add.s32 	%r54, %r19, %r53;
	st.global.f32 	[%r54], %f98;
	mad.lo.s32 	%r55, %r41, 5, %r52;
	shl.b32 	%r56, %r55, 2;
	add.s32 	%r57, %r19, %r56;
	st.global.f32 	[%r57], %f99;
	add.s32 	%r58, %r57, %r42;
	st.global.f32 	[%r58], %f100;
	add.s32 	%r59, %r58, %r42;
	st.global.f32 	[%r59], %f101;

BB5_5:
	ret;
}

	// .globl	CalculateAMatrixAndHVector2DValuesZ
.entry CalculateAMatrixAndHVector2DValuesZ(
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesZ_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesZ_param_1,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesZ_param_2,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesZ_param_3,
	.param .u32 .ptr .global .align 4 CalculateAMatrixAndHVector2DValuesZ_param_4,
	.param .u32 CalculateAMatrixAndHVector2DValuesZ_param_5,
	.param .u32 CalculateAMatrixAndHVector2DValuesZ_param_6,
	.param .u32 CalculateAMatrixAndHVector2DValuesZ_param_7,
	.param .u32 CalculateAMatrixAndHVector2DValuesZ_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<65>;


	ld.param.u32 	%r18, [CalculateAMatrixAndHVector2DValuesZ_param_0];
	ld.param.u32 	%r19, [CalculateAMatrixAndHVector2DValuesZ_param_1];
	ld.param.u32 	%r20, [CalculateAMatrixAndHVector2DValuesZ_param_2];
	ld.param.u32 	%r21, [CalculateAMatrixAndHVector2DValuesZ_param_3];
	ld.param.u32 	%r22, [CalculateAMatrixAndHVector2DValuesZ_param_4];
	ld.param.u32 	%r23, [CalculateAMatrixAndHVector2DValuesZ_param_5];
	ld.param.u32 	%r24, [CalculateAMatrixAndHVector2DValuesZ_param_6];
	ld.param.u32 	%r25, [CalculateAMatrixAndHVector2DValuesZ_param_7];
	ld.param.u32 	%r26, [CalculateAMatrixAndHVector2DValuesZ_param_8];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.b32	%r3, %envreg1;
	add.s32 	%r4, %r2, %r3;
	add.s32 	%r27, %r26, -1;
	shr.u32 	%r28, %r27, 31;
	add.s32 	%r29, %r27, %r28;
	shr.s32 	%r64, %r29, 1;
	sub.s32 	%r30, %r24, %r64;
	setp.ge.s32	%p1, %r1, %r64;
	setp.lt.s32	%p2, %r1, %r30;
	and.pred  	%p3, %p2, %p1;
	setp.ge.s32	%p4, %r4, %r64;
	and.pred  	%p5, %p3, %p4;
	sub.s32 	%r31, %r25, %r64;
	setp.lt.s32	%p6, %r4, %r31;
	and.pred  	%p7, %p6, %p5;
	@!%p7 bra 	BB6_5;
	bra.uni 	BB6_1;

BB6_1:
	cvt.rn.f32.s32	%f63, %r1;
	cvt.rn.f32.s32	%f64, %r24;
	add.f32 	%f65, %f64, 0fBF800000;
	neg.f32 	%f66, %f65;
	fma.rn.f32 	%f1, %f66, 0f3F000000, %f63;
	cvt.rn.f32.s32	%f67, %r25;
	add.f32 	%f68, %f67, 0fBF800000;
	neg.f32 	%f69, %f68;
	cvt.rn.f32.s32	%f70, %r4;
	fma.rn.f32 	%f2, %f69, 0f3F000000, %f70;
	sub.s32 	%r6, %r23, %r64;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;
	setp.ge.s32	%p8, %r64, %r6;
	@%p8 bra 	BB6_4;

	cvt.rn.f32.s32	%f85, %r23;
	add.f32 	%f86, %f85, 0fBF800000;
	neg.f32 	%f3, %f86;
	mul.f32 	%f4, %f1, %f1;
	mul.f32 	%f5, %f1, %f2;
	mul.f32 	%f6, %f2, %f2;
	mad.lo.s32 	%r33, %r24, %r4, %r1;
	mad.lo.s32 	%r34, %r23, %r33, %r64;
	shl.b32 	%r35, %r34, 2;
	add.s32 	%r63, %r22, %r35;
	add.s32 	%r62, %r21, %r35;
	add.s32 	%r61, %r20, %r35;
	mov.f32 	%f111, 0f00000000;
	mov.f32 	%f110, %f111;
	mov.f32 	%f109, %f111;
	mov.f32 	%f108, %f111;
	mov.f32 	%f107, %f111;
	mov.f32 	%f106, %f111;
	mov.f32 	%f105, %f111;
	mov.f32 	%f104, %f111;
	mov.f32 	%f103, %f111;
	mov.f32 	%f102, %f111;
	mov.f32 	%f101, %f111;
	mov.f32 	%f100, %f111;
	mov.f32 	%f99, %f111;
	mov.f32 	%f98, %f111;

BB6_3:
	cvt.rn.f32.s32	%f87, %r64;
	fma.rn.f32 	%f88, %f3, 0f3F000000, %f87;
	ld.global.f32 	%f89, [%r63];
	ld.global.f32 	%f90, [%r62];
	mul.f32 	%f91, %f90, %f89;
	mul.f32 	%f92, %f90, %f91;
	ld.global.f32 	%f93, [%r61];
	mul.f32 	%f94, %f93, %f91;
	add.f32 	%f102, %f102, %f92;
	fma.rn.f32 	%f103, %f88, %f92, %f103;
	fma.rn.f32 	%f104, %f1, %f92, %f104;
	fma.rn.f32 	%f105, %f2, %f92, %f105;
	mul.f32 	%f95, %f88, %f88;
	fma.rn.f32 	%f106, %f95, %f92, %f106;
	mul.f32 	%f96, %f1, %f88;
	fma.rn.f32 	%f107, %f96, %f92, %f107;
	mul.f32 	%f97, %f2, %f88;
	fma.rn.f32 	%f108, %f97, %f92, %f108;
	fma.rn.f32 	%f109, %f4, %f92, %f109;
	fma.rn.f32 	%f110, %f5, %f92, %f110;
	fma.rn.f32 	%f111, %f6, %f92, %f111;
	add.f32 	%f98, %f98, %f94;
	fma.rn.f32 	%f99, %f88, %f94, %f99;
	fma.rn.f32 	%f100, %f1, %f94, %f100;
	fma.rn.f32 	%f101, %f2, %f94, %f101;
	add.s32 	%r63, %r63, 4;
	add.s32 	%r62, %r62, 4;
	add.s32 	%r61, %r61, 4;
	add.s32 	%r64, %r64, 1;
	setp.lt.s32	%p9, %r64, %r6;
	@%p9 bra 	BB6_3;

BB6_4:
	mul.lo.s32 	%r36, %r24, 20;
	mad.lo.s32 	%r37, %r4, %r24, %r1;
	mad.lo.s32 	%r38, %r36, %r25, %r37;
	shl.b32 	%r39, %r38, 2;
	add.s32 	%r40, %r18, %r39;
	st.global.f32 	[%r40], %f102;
	mul.lo.s32 	%r41, %r25, %r24;
	shl.b32 	%r42, %r41, 2;
	add.s32 	%r43, %r40, %r42;
	st.global.f32 	[%r43], %f103;
	add.s32 	%r44, %r43, %r42;
	st.global.f32 	[%r44], %f104;
	add.s32 	%r45, %r44, %r42;
	st.global.f32 	[%r45], %f105;
	add.s32 	%r46, %r45, %r42;
	st.global.f32 	[%r46], %f106;
	add.s32 	%r47, %r46, %r42;
	st.global.f32 	[%r47], %f107;
	add.s32 	%r48, %r47, %r42;
	st.global.f32 	[%r48], %f108;
	add.s32 	%r49, %r48, %r42;
	st.global.f32 	[%r49], %f109;
	add.s32 	%r50, %r49, %r42;
	st.global.f32 	[%r50], %f110;
	add.s32 	%r51, %r50, %r42;
	st.global.f32 	[%r51], %f111;
	shl.b32 	%r52, %r24, 1;
	mad.lo.s32 	%r53, %r52, %r25, %r37;
	shl.b32 	%r54, %r53, 2;
	add.s32 	%r55, %r19, %r54;
	st.global.f32 	[%r55], %f98;
	mad.lo.s32 	%r56, %r41, 7, %r53;
	shl.b32 	%r57, %r56, 2;
	add.s32 	%r58, %r19, %r57;
	st.global.f32 	[%r58], %f99;
	add.s32 	%r59, %r58, %r42;
	st.global.f32 	[%r59], %f100;
	add.s32 	%r60, %r59, %r42;
	st.global.f32 	[%r60], %f101;

BB6_5:
	ret;
}

	// .globl	CalculateAMatrix1DValues
.entry CalculateAMatrix1DValues(
	.param .u32 .ptr .global .align 4 CalculateAMatrix1DValues_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatrix1DValues_param_1,
	.param .u32 CalculateAMatrix1DValues_param_2,
	.param .u32 CalculateAMatrix1DValues_param_3,
	.param .u32 CalculateAMatrix1DValues_param_4,
	.param .u32 CalculateAMatrix1DValues_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<30>;


	ld.param.u32 	%r11, [CalculateAMatrix1DValues_param_0];
	ld.param.u32 	%r12, [CalculateAMatrix1DValues_param_1];
	ld.param.u32 	%r13, [CalculateAMatrix1DValues_param_3];
	ld.param.u32 	%r14, [CalculateAMatrix1DValues_param_4];
	ld.param.u32 	%r15, [CalculateAMatrix1DValues_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.b32	%r3, %envreg1;
	add.s32 	%r16, %r15, -1;
	shr.u32 	%r17, %r16, 31;
	add.s32 	%r18, %r16, %r17;
	shr.s32 	%r29, %r18, 1;
	setp.ge.s32	%p1, %r1, %r29;
	sub.s32 	%r19, %r14, %r29;
	setp.lt.s32	%p2, %r1, %r19;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB7_5;
	bra.uni 	BB7_1;

BB7_1:
	sub.s32 	%r5, %r13, %r29;
	mov.f32 	%f7, 0f00000000;
	setp.ge.s32	%p4, %r29, %r5;
	@%p4 bra 	BB7_4;

	add.s32 	%r20, %r2, %r3;
	mad.lo.s32 	%r21, %r14, %r20, %r1;
	mad.lo.s32 	%r22, %r13, %r21, %r29;
	shl.b32 	%r23, %r22, 2;
	add.s32 	%r28, %r12, %r23;
	mov.f32 	%f7, 0f00000000;

BB7_3:
	ld.global.f32 	%f6, [%r28];
	add.f32 	%f7, %f7, %f6;
	add.s32 	%r28, %r28, 4;
	add.s32 	%r29, %r29, 1;
	setp.lt.s32	%p5, %r29, %r5;
	@%p5 bra 	BB7_3;

BB7_4:
	add.s32 	%r24, %r2, %r3;
	mad.lo.s32 	%r25, %r24, %r14, %r1;
	shl.b32 	%r26, %r25, 2;
	add.s32 	%r27, %r11, %r26;
	st.global.f32 	[%r27], %f7;

BB7_5:
	ret;
}

	// .globl	CalculateAMatrix
.entry CalculateAMatrix(
	.param .u32 .ptr .global .align 4 CalculateAMatrix_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatrix_param_1,
	.param .u32 CalculateAMatrix_param_2,
	.param .u32 CalculateAMatrix_param_3,
	.param .u32 CalculateAMatrix_param_4,
	.param .u32 CalculateAMatrix_param_5
)
{
	.reg .pred 	%p<44>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<87>;


	ld.param.u32 	%r11, [CalculateAMatrix_param_0];
	ld.param.u32 	%r12, [CalculateAMatrix_param_1];
	ld.param.u32 	%r13, [CalculateAMatrix_param_4];
	ld.param.u32 	%r14, [CalculateAMatrix_param_5];
	mov.u32 	%r1, %tid.x;
	add.s32 	%r15, %r14, -1;
	shr.u32 	%r16, %r15, 31;
	add.s32 	%r17, %r15, %r16;
	shr.s32 	%r84, %r17, 1;
	sub.s32 	%r3, %r13, %r84;
	mov.f32 	%f7, 0f00000000;
	setp.ge.s32	%p1, %r84, %r3;
	@%p1 bra 	BB8_3;

	mad.lo.s32 	%r18, %r1, %r13, %r84;
	shl.b32 	%r19, %r18, 2;
	add.s32 	%r83, %r12, %r19;
	mov.f32 	%f7, 0f00000000;

BB8_2:
	ld.global.f32 	%f6, [%r83];
	add.f32 	%f7, %f7, %f6;
	add.s32 	%r83, %r83, 4;
	add.s32 	%r84, %r84, 1;
	setp.lt.s32	%p2, %r84, %r3;
	@%p2 bra 	BB8_2;

BB8_3:
	mov.u32 	%r86, 0;
	mov.u32 	%r85, %r86;
	setp.gt.s32	%p3, %r1, 14;
	@%p3 bra 	BB8_29;

	setp.gt.s32	%p25, %r1, 7;
	@%p25 bra 	BB8_17;

	setp.gt.s32	%p35, %r1, 3;
	@%p35 bra 	BB8_10;

	setp.eq.s32	%p41, %r1, 1;
	@%p41 bra 	BB8_71;

	setp.eq.s32	%p42, %r1, 2;
	@%p42 bra 	BB8_70;
	bra.uni 	BB8_8;

BB8_70:
	mov.u32 	%r85, 4;
	bra.uni 	BB8_74;

BB8_29:
	setp.gt.s32	%p4, %r1, 21;
	@%p4 bra 	BB8_42;

	setp.gt.s32	%p16, %r1, 17;
	@%p16 bra 	BB8_35;

	setp.eq.s32	%p22, %r1, 15;
	@%p22 bra 	BB8_63;

	setp.eq.s32	%p23, %r1, 16;
	@%p23 bra 	BB8_62;
	bra.uni 	BB8_33;

BB8_62:
	mov.u32 	%r86, 72;
	mov.u32 	%r85, 8;
	bra.uni 	BB8_74;

BB8_17:
	setp.gt.s32	%p26, %r1, 10;
	@%p26 bra 	BB8_22;

	setp.eq.s32	%p32, %r1, 8;
	@%p32 bra 	BB8_67;

	setp.eq.s32	%p33, %r1, 9;
	@%p33 bra 	BB8_66;
	bra.uni 	BB8_20;

BB8_66:
	mov.u32 	%r86, 60;
	mov.u32 	%r85, 5;
	bra.uni 	BB8_74;

BB8_42:
	setp.gt.s32	%p5, %r1, 25;
	@%p5 bra 	BB8_50;

	setp.gt.s32	%p11, %r1, 23;
	@%p11 bra 	BB8_47;

	setp.eq.s32	%p14, %r1, 22;
	@%p14 bra 	BB8_59;
	bra.uni 	BB8_45;

BB8_59:
	mov.u32 	%r86, 24;
	mov.u32 	%r85, 10;
	bra.uni 	BB8_74;

BB8_10:
	setp.gt.s32	%p36, %r1, 5;
	@%p36 bra 	BB8_14;

	setp.eq.s32	%p39, %r1, 4;
	@%p39 bra 	BB8_69;
	bra.uni 	BB8_12;

BB8_69:
	mov.u32 	%r86, 36;
	mov.u32 	%r85, 3;
	bra.uni 	BB8_74;

BB8_35:
	setp.gt.s32	%p17, %r1, 19;
	@%p17 bra 	BB8_39;

	setp.eq.s32	%p20, %r1, 18;
	@%p20 bra 	BB8_61;
	bra.uni 	BB8_37;

BB8_61:
	mov.u32 	%r86, 84;
	mov.u32 	%r85, 8;
	bra.uni 	BB8_74;

BB8_22:
	setp.gt.s32	%p27, %r1, 12;
	@%p27 bra 	BB8_26;

	setp.eq.s32	%p30, %r1, 11;
	@%p30 bra 	BB8_65;
	bra.uni 	BB8_24;

BB8_65:
	mov.u32 	%r86, 12;
	mov.u32 	%r85, 6;
	bra.uni 	BB8_74;

BB8_50:
	setp.gt.s32	%p6, %r1, 27;
	@%p6 bra 	BB8_54;

	setp.eq.s32	%p9, %r1, 26;
	@%p9 bra 	BB8_57;
	bra.uni 	BB8_52;

BB8_57:
	mov.u32 	%r86, 108;
	bra.uni 	BB8_73;

BB8_47:
	setp.eq.s32	%p12, %r1, 24;
	@%p12 bra 	BB8_58;
	bra.uni 	BB8_48;

BB8_58:
	mov.u32 	%r86, 108;
	mov.u32 	%r85, 9;
	bra.uni 	BB8_74;

BB8_14:
	setp.eq.s32	%p37, %r1, 6;
	@%p37 bra 	BB8_68;
	bra.uni 	BB8_15;

BB8_68:
	mov.u32 	%r86, 36;
	mov.u32 	%r85, 5;
	bra.uni 	BB8_74;

BB8_39:
	setp.eq.s32	%p18, %r1, 20;
	@%p18 bra 	BB8_60;
	bra.uni 	BB8_40;

BB8_60:
	mov.u32 	%r86, 24;
	mov.u32 	%r85, 2;
	bra.uni 	BB8_74;

BB8_26:
	setp.eq.s32	%p28, %r1, 13;
	@%p28 bra 	BB8_64;
	bra.uni 	BB8_27;

BB8_64:
	mov.u32 	%r86, 12;
	mov.u32 	%r85, 8;
	bra.uni 	BB8_74;

BB8_54:
	setp.eq.s32	%p7, %r1, 29;
	@%p7 bra 	BB8_72;
	bra.uni 	BB8_55;

BB8_72:
	mov.u32 	%r86, 132;
	bra.uni 	BB8_73;

BB8_71:
	mov.u32 	%r85, 3;
	bra.uni 	BB8_74;

BB8_8:
	setp.eq.s32	%p43, %r1, 3;
	@%p43 bra 	BB8_9;
	bra.uni 	BB8_74;

BB8_9:
	mov.u32 	%r85, 5;
	bra.uni 	BB8_74;

BB8_63:
	mov.u32 	%r86, 72;
	mov.u32 	%r85, 7;
	bra.uni 	BB8_74;

BB8_33:
	setp.eq.s32	%p24, %r1, 17;
	@%p24 bra 	BB8_34;
	bra.uni 	BB8_74;

BB8_34:
	mov.u32 	%r86, 84;
	mov.u32 	%r85, 7;
	bra.uni 	BB8_74;

BB8_67:
	mov.u32 	%r86, 48;
	mov.u32 	%r85, 5;
	bra.uni 	BB8_74;

BB8_20:
	setp.eq.s32	%p34, %r1, 10;
	@%p34 bra 	BB8_21;
	bra.uni 	BB8_74;

BB8_21:
	mov.u32 	%r86, 12;
	mov.u32 	%r85, 1;
	bra.uni 	BB8_74;

BB8_45:
	setp.eq.s32	%p15, %r1, 23;
	@%p15 bra 	BB8_46;
	bra.uni 	BB8_74;

BB8_46:
	mov.u32 	%r86, 24;
	bra.uni 	BB8_73;

BB8_12:
	setp.eq.s32	%p40, %r1, 5;
	@%p40 bra 	BB8_13;
	bra.uni 	BB8_74;

BB8_13:
	mov.u32 	%r86, 36;
	mov.u32 	%r85, 4;
	bra.uni 	BB8_74;

BB8_37:
	setp.eq.s32	%p21, %r1, 19;
	@%p21 bra 	BB8_38;
	bra.uni 	BB8_74;

BB8_38:
	mov.u32 	%r86, 96;
	mov.u32 	%r85, 8;
	bra.uni 	BB8_74;

BB8_24:
	setp.eq.s32	%p31, %r1, 12;
	@%p31 bra 	BB8_25;
	bra.uni 	BB8_74;

BB8_25:
	mov.u32 	%r86, 12;
	mov.u32 	%r85, 7;
	bra.uni 	BB8_74;

BB8_52:
	setp.eq.s32	%p10, %r1, 27;
	@%p10 bra 	BB8_53;
	bra.uni 	BB8_74;

BB8_53:
	mov.u32 	%r86, 120;
	mov.u32 	%r85, 10;
	bra.uni 	BB8_74;

BB8_48:
	setp.eq.s32	%p13, %r1, 25;
	@%p13 bra 	BB8_49;
	bra.uni 	BB8_74;

BB8_49:
	mov.u32 	%r86, 108;
	mov.u32 	%r85, 10;
	bra.uni 	BB8_74;

BB8_15:
	setp.eq.s32	%p38, %r1, 7;
	@%p38 bra 	BB8_16;
	bra.uni 	BB8_74;

BB8_16:
	mov.u32 	%r86, 48;
	mov.u32 	%r85, 4;
	bra.uni 	BB8_74;

BB8_40:
	setp.eq.s32	%p19, %r1, 21;
	@%p19 bra 	BB8_41;
	bra.uni 	BB8_74;

BB8_41:
	mov.u32 	%r86, 24;
	mov.u32 	%r85, 9;
	bra.uni 	BB8_74;

BB8_27:
	setp.eq.s32	%p29, %r1, 14;
	@%p29 bra 	BB8_28;
	bra.uni 	BB8_74;

BB8_28:
	mov.u32 	%r86, 72;
	mov.u32 	%r85, 6;
	bra.uni 	BB8_74;

BB8_55:
	setp.ne.s32	%p8, %r1, 28;
	@%p8 bra 	BB8_74;

	mov.u32 	%r86, 120;

BB8_73:
	mov.u32 	%r85, 11;

BB8_74:
	add.s32 	%r80, %r86, %r85;
	shl.b32 	%r81, %r80, 2;
	add.s32 	%r82, %r11, %r81;
	st.global.f32 	[%r82], %f7;
	ret;
}

	// .globl	CalculateHVector1DValues
.entry CalculateHVector1DValues(
	.param .u32 .ptr .global .align 4 CalculateHVector1DValues_param_0,
	.param .u32 .ptr .global .align 4 CalculateHVector1DValues_param_1,
	.param .u32 CalculateHVector1DValues_param_2,
	.param .u32 CalculateHVector1DValues_param_3,
	.param .u32 CalculateHVector1DValues_param_4,
	.param .u32 CalculateHVector1DValues_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<33>;


	ld.param.u32 	%r11, [CalculateHVector1DValues_param_0];
	ld.param.u32 	%r12, [CalculateHVector1DValues_param_1];
	ld.param.u32 	%r13, [CalculateHVector1DValues_param_3];
	ld.param.u32 	%r14, [CalculateHVector1DValues_param_4];
	ld.param.u32 	%r15, [CalculateHVector1DValues_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %ntid.y;
	mov.b32	%r18, %envreg4;
	mad.lo.s32 	%r2, %r16, %r17, %r18;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r19, %r15, -1;
	shr.u32 	%r20, %r19, 31;
	add.s32 	%r21, %r19, %r20;
	shr.s32 	%r32, %r21, 1;
	setp.ge.s32	%p1, %r1, %r32;
	sub.s32 	%r22, %r14, %r32;
	setp.lt.s32	%p2, %r1, %r22;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB9_5;
	bra.uni 	BB9_1;

BB9_1:
	sub.s32 	%r5, %r13, %r32;
	mov.f32 	%f7, 0f00000000;
	setp.ge.s32	%p4, %r32, %r5;
	@%p4 bra 	BB9_4;

	add.s32 	%r23, %r2, %r3;
	mad.lo.s32 	%r24, %r14, %r23, %r1;
	mad.lo.s32 	%r25, %r13, %r24, %r32;
	shl.b32 	%r26, %r25, 2;
	add.s32 	%r31, %r12, %r26;
	mov.f32 	%f7, 0f00000000;

BB9_3:
	ld.global.f32 	%f6, [%r31];
	add.f32 	%f7, %f7, %f6;
	add.s32 	%r31, %r31, 4;
	add.s32 	%r32, %r32, 1;
	setp.lt.s32	%p5, %r32, %r5;
	@%p5 bra 	BB9_3;

BB9_4:
	add.s32 	%r27, %r2, %r3;
	mad.lo.s32 	%r28, %r27, %r14, %r1;
	shl.b32 	%r29, %r28, 2;
	add.s32 	%r30, %r11, %r29;
	st.global.f32 	[%r30], %f7;

BB9_5:
	ret;
}

	// .globl	CalculateHVector
.entry CalculateHVector(
	.param .u32 .ptr .global .align 4 CalculateHVector_param_0,
	.param .u32 .ptr .global .align 4 CalculateHVector_param_1,
	.param .u32 CalculateHVector_param_2,
	.param .u32 CalculateHVector_param_3,
	.param .u32 CalculateHVector_param_4,
	.param .u32 CalculateHVector_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<22>;


	ld.param.u32 	%r9, [CalculateHVector_param_0];
	ld.param.u32 	%r10, [CalculateHVector_param_1];
	ld.param.u32 	%r11, [CalculateHVector_param_4];
	ld.param.u32 	%r12, [CalculateHVector_param_5];
	mov.u32 	%r1, %tid.x;
	add.s32 	%r13, %r12, -1;
	shr.u32 	%r14, %r13, 31;
	add.s32 	%r15, %r13, %r14;
	shr.s32 	%r21, %r15, 1;
	sub.s32 	%r3, %r11, %r21;
	mov.f32 	%f7, 0f00000000;
	setp.ge.s32	%p1, %r21, %r3;
	@%p1 bra 	BB10_3;

	mad.lo.s32 	%r16, %r1, %r11, %r21;
	shl.b32 	%r17, %r16, 2;
	add.s32 	%r20, %r10, %r17;
	mov.f32 	%f7, 0f00000000;

BB10_2:
	ld.global.f32 	%f6, [%r20];
	add.f32 	%f7, %f7, %f6;
	add.s32 	%r20, %r20, 4;
	add.s32 	%r21, %r21, 1;
	setp.lt.s32	%p2, %r21, %r3;
	@%p2 bra 	BB10_2;

BB10_3:
	shl.b32 	%r18, %r1, 2;
	add.s32 	%r19, %r9, %r18;
	st.global.f32 	[%r19], %f7;
	ret;
}

	// .globl	CalculateTensorComponents
.entry CalculateTensorComponents(
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_0,
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_1,
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_2,
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_3,
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_4,
	.param .u32 .ptr .global .align 4 CalculateTensorComponents_param_5,
	.param .u32 .ptr .global .align 8 CalculateTensorComponents_param_6,
	.param .u32 .ptr .global .align 8 CalculateTensorComponents_param_7,
	.param .f32 CalculateTensorComponents_param_8,
	.param .f32 CalculateTensorComponents_param_9,
	.param .f32 CalculateTensorComponents_param_10,
	.param .f32 CalculateTensorComponents_param_11,
	.param .f32 CalculateTensorComponents_param_12,
	.param .f32 CalculateTensorComponents_param_13,
	.param .u32 CalculateTensorComponents_param_14,
	.param .u32 CalculateTensorComponents_param_15,
	.param .u32 CalculateTensorComponents_param_16
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<40>;


	ld.param.u32 	%r4, [CalculateTensorComponents_param_0];
	ld.param.u32 	%r5, [CalculateTensorComponents_param_1];
	ld.param.u32 	%r6, [CalculateTensorComponents_param_2];
	ld.param.u32 	%r7, [CalculateTensorComponents_param_3];
	ld.param.u32 	%r8, [CalculateTensorComponents_param_4];
	ld.param.u32 	%r9, [CalculateTensorComponents_param_5];
	ld.param.u32 	%r10, [CalculateTensorComponents_param_7];
	ld.param.f32 	%f1, [CalculateTensorComponents_param_8];
	ld.param.f32 	%f2, [CalculateTensorComponents_param_9];
	ld.param.f32 	%f3, [CalculateTensorComponents_param_10];
	ld.param.f32 	%f4, [CalculateTensorComponents_param_11];
	ld.param.f32 	%f5, [CalculateTensorComponents_param_12];
	ld.param.f32 	%f6, [CalculateTensorComponents_param_13];
	ld.param.u32 	%r11, [CalculateTensorComponents_param_14];
	ld.param.u32 	%r12, [CalculateTensorComponents_param_15];
	ld.param.u32 	%r13, [CalculateTensorComponents_param_16];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r17, %r15, %r16, %r14;
	mov.u32 	%r18, %tid.x;
	add.s32 	%r1, %r17, %r18;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %ntid.y;
	mov.b32	%r21, %envreg4;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %tid.y;
	add.s32 	%r2, %r22, %r23;
	mov.u32 	%r24, %ctaid.z;
	mov.u32 	%r25, %ntid.z;
	mov.b32	%r26, %envreg5;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %tid.z;
	add.s32 	%r3, %r27, %r28;
	setp.ge.s32	%p1, %r1, %r11;
	setp.ge.s32	%p2, %r2, %r12;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r13;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB11_2;

	mad.lo.s32 	%r29, %r3, %r12, %r2;
	mad.lo.s32 	%r30, %r29, %r11, %r1;
	shl.b32 	%r31, %r30, 3;
	add.s32 	%r32, %r10, %r31;
	ld.global.v2.f32 	{%f7, %f8}, [%r32];
	mul.f32 	%f11, %f8, %f8;
	fma.rn.f32 	%f12, %f7, %f7, %f11;
	sqrt.approx.f32 	%f13, %f12;
	shl.b32 	%r33, %r30, 2;
	add.s32 	%r34, %r4, %r33;
	ld.global.f32 	%f14, [%r34];
	fma.rn.f32 	%f15, %f13, %f1, %f14;
	st.global.f32 	[%r34], %f15;
	add.s32 	%r35, %r5, %r33;
	ld.global.f32 	%f16, [%r35];
	fma.rn.f32 	%f17, %f13, %f2, %f16;
	st.global.f32 	[%r35], %f17;
	add.s32 	%r36, %r6, %r33;
	ld.global.f32 	%f18, [%r36];
	fma.rn.f32 	%f19, %f13, %f3, %f18;
	st.global.f32 	[%r36], %f19;
	add.s32 	%r37, %r7, %r33;
	ld.global.f32 	%f20, [%r37];
	fma.rn.f32 	%f21, %f13, %f4, %f20;
	st.global.f32 	[%r37], %f21;
	add.s32 	%r38, %r8, %r33;
	ld.global.f32 	%f22, [%r38];
	fma.rn.f32 	%f23, %f13, %f5, %f22;
	st.global.f32 	[%r38], %f23;
	add.s32 	%r39, %r9, %r33;
	ld.global.f32 	%f24, [%r39];
	fma.rn.f32 	%f25, %f13, %f6, %f24;
	st.global.f32 	[%r39], %f25;

BB11_2:
	ret;
}

	// .globl	CalculateTensorNorms
.entry CalculateTensorNorms(
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_0,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_1,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_2,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_3,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_4,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_5,
	.param .u32 .ptr .global .align 4 CalculateTensorNorms_param_6,
	.param .u32 CalculateTensorNorms_param_7,
	.param .u32 CalculateTensorNorms_param_8,
	.param .u32 CalculateTensorNorms_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<39>;


	ld.param.u32 	%r4, [CalculateTensorNorms_param_0];
	ld.param.u32 	%r5, [CalculateTensorNorms_param_1];
	ld.param.u32 	%r6, [CalculateTensorNorms_param_2];
	ld.param.u32 	%r7, [CalculateTensorNorms_param_3];
	ld.param.u32 	%r8, [CalculateTensorNorms_param_4];
	ld.param.u32 	%r9, [CalculateTensorNorms_param_5];
	ld.param.u32 	%r10, [CalculateTensorNorms_param_6];
	ld.param.u32 	%r11, [CalculateTensorNorms_param_7];
	ld.param.u32 	%r12, [CalculateTensorNorms_param_8];
	ld.param.u32 	%r13, [CalculateTensorNorms_param_9];
	mov.b32	%r14, %envreg3;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %ntid.x;
	mad.lo.s32 	%r17, %r15, %r16, %r14;
	mov.u32 	%r18, %tid.x;
	add.s32 	%r1, %r17, %r18;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %ntid.y;
	mov.b32	%r21, %envreg4;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %tid.y;
	add.s32 	%r2, %r22, %r23;
	mov.u32 	%r24, %ctaid.z;
	mov.u32 	%r25, %ntid.z;
	mov.b32	%r26, %envreg5;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %tid.z;
	add.s32 	%r3, %r27, %r28;
	setp.ge.s32	%p1, %r1, %r11;
	setp.ge.s32	%p2, %r2, %r12;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r13;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB12_2;

	mad.lo.s32 	%r29, %r3, %r12, %r2;
	mad.lo.s32 	%r30, %r29, %r11, %r1;
	shl.b32 	%r31, %r30, 2;
	add.s32 	%r32, %r5, %r31;
	add.s32 	%r33, %r6, %r31;
	add.s32 	%r34, %r7, %r31;
	add.s32 	%r35, %r8, %r31;
	add.s32 	%r36, %r9, %r31;
	add.s32 	%r37, %r10, %r31;
	ld.global.f32 	%f1, [%r33];
	add.f32 	%f2, %f1, %f1;
	mul.f32 	%f3, %f1, %f2;
	ld.global.f32 	%f4, [%r32];
	fma.rn.f32 	%f5, %f4, %f4, %f3;
	ld.global.f32 	%f6, [%r34];
	add.f32 	%f7, %f6, %f6;
	fma.rn.f32 	%f8, %f7, %f6, %f5;
	ld.global.f32 	%f9, [%r35];
	fma.rn.f32 	%f10, %f9, %f9, %f8;
	ld.global.f32 	%f11, [%r36];
	add.f32 	%f12, %f11, %f11;
	fma.rn.f32 	%f13, %f12, %f11, %f10;
	ld.global.f32 	%f14, [%r37];
	fma.rn.f32 	%f15, %f14, %f14, %f13;
	sqrt.approx.f32 	%f16, %f15;
	add.s32 	%r38, %r4, %r31;
	st.global.f32 	[%r38], %f16;

BB12_2:
	ret;
}

	// .globl	CalculateAMatricesAndHVectors
.entry CalculateAMatricesAndHVectors(
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_0,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_1,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_2,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_3,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_4,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_5,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_6,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_7,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_8,
	.param .u32 .ptr .global .align 8 CalculateAMatricesAndHVectors_param_9,
	.param .u32 .ptr .global .align 8 CalculateAMatricesAndHVectors_param_10,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_11,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_12,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_13,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_14,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_15,
	.param .u32 .ptr .global .align 4 CalculateAMatricesAndHVectors_param_16,
	.param .u32 .ptr .const .align 4 CalculateAMatricesAndHVectors_param_17,
	.param .u32 .ptr .const .align 4 CalculateAMatricesAndHVectors_param_18,
	.param .u32 .ptr .const .align 4 CalculateAMatricesAndHVectors_param_19,
	.param .u32 CalculateAMatricesAndHVectors_param_20,
	.param .u32 CalculateAMatricesAndHVectors_param_21,
	.param .u32 CalculateAMatricesAndHVectors_param_22,
	.param .u32 CalculateAMatricesAndHVectors_param_23
)
{
	.local .align 4 .b8 	__local_depot13[28];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<43>;
	.reg .f32 	%f<201>;
	.reg .b32 	%r<295>;


	mov.u32 	%r294, __local_depot13;
	cvta.local.u32 	%SP, %r294;
	ld.param.u32 	%r76, [CalculateAMatricesAndHVectors_param_0];
	ld.param.u32 	%r77, [CalculateAMatricesAndHVectors_param_1];
	ld.param.u32 	%r78, [CalculateAMatricesAndHVectors_param_2];
	ld.param.u32 	%r79, [CalculateAMatricesAndHVectors_param_3];
	ld.param.u32 	%r80, [CalculateAMatricesAndHVectors_param_4];
	ld.param.u32 	%r81, [CalculateAMatricesAndHVectors_param_5];
	ld.param.u32 	%r82, [CalculateAMatricesAndHVectors_param_6];
	ld.param.u32 	%r83, [CalculateAMatricesAndHVectors_param_7];
	ld.param.u32 	%r84, [CalculateAMatricesAndHVectors_param_8];
	ld.param.u32 	%r85, [CalculateAMatricesAndHVectors_param_9];
	ld.param.u32 	%r86, [CalculateAMatricesAndHVectors_param_10];
	ld.param.u32 	%r87, [CalculateAMatricesAndHVectors_param_11];
	ld.param.u32 	%r88, [CalculateAMatricesAndHVectors_param_12];
	ld.param.u32 	%r89, [CalculateAMatricesAndHVectors_param_13];
	ld.param.u32 	%r90, [CalculateAMatricesAndHVectors_param_14];
	ld.param.u32 	%r91, [CalculateAMatricesAndHVectors_param_15];
	ld.param.u32 	%r92, [CalculateAMatricesAndHVectors_param_16];
	ld.param.u32 	%r93, [CalculateAMatricesAndHVectors_param_17];
	ld.param.u32 	%r94, [CalculateAMatricesAndHVectors_param_18];
	ld.param.u32 	%r95, [CalculateAMatricesAndHVectors_param_19];
	ld.param.u32 	%r96, [CalculateAMatricesAndHVectors_param_20];
	ld.param.u32 	%r97, [CalculateAMatricesAndHVectors_param_21];
	ld.param.u32 	%r99, [CalculateAMatricesAndHVectors_param_22];
	ld.param.u32 	%r98, [CalculateAMatricesAndHVectors_param_23];
	add.u32 	%r100, %SP, 0;
	cvta.to.local.u32 	%r1, %r100;
	mov.u32 	%r101, %ctaid.x;
	mov.u32 	%r102, %ntid.x;
	mov.b32	%r103, %envreg3;
	mad.lo.s32 	%r104, %r101, %r102, %r103;
	mov.u32 	%r105, %tid.x;
	add.s32 	%r2, %r104, %r105;
	mov.u32 	%r106, %ctaid.y;
	mov.u32 	%r107, %ntid.y;
	mov.b32	%r108, %envreg4;
	mad.lo.s32 	%r109, %r106, %r107, %r108;
	mov.u32 	%r110, %tid.y;
	add.s32 	%r3, %r109, %r110;
	mov.u32 	%r111, %ctaid.z;
	mov.u32 	%r112, %ntid.z;
	mov.b32	%r113, %envreg5;
	mad.lo.s32 	%r114, %r111, %r112, %r113;
	mov.u32 	%r115, %tid.z;
	add.s32 	%r4, %r114, %r115;
	setp.ge.s32	%p1, %r2, %r96;
	setp.ge.s32	%p2, %r3, %r97;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r4, %r99;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB13_47;

	mad.lo.s32 	%r116, %r4, %r97, %r3;
	mad.lo.s32 	%r5, %r116, %r96, %r2;
	shl.b32 	%r117, %r5, 3;
	add.s32 	%r118, %r85, %r117;
	add.s32 	%r119, %r86, %r117;
	ld.global.v2.f32 	{%f47, %f48}, [%r118];
	ld.global.v2.f32 	{%f50, %f51}, [%r119];
	mul.f32 	%f55, %f48, %f51;
	fma.rn.f32 	%f1, %f47, %f50, %f55;
	neg.f32 	%f56, %f47;
	mul.f32 	%f57, %f48, %f50;
	fma.rn.f32 	%f2, %f56, %f51, %f57;
	abs.f32 	%f3, %f1;
	abs.f32 	%f4, %f2;
	setp.eq.f32	%p6, %f3, 0f00000000;
	setp.eq.f32	%p7, %f4, 0f00000000;
	and.pred  	%p8, %p6, %p7;
	mov.b32 	 %r6, %f1;
	mov.b32 	 %r120, %f2;
	and.b32  	%r7, %r120, -2147483648;
	@%p8 bra 	BB13_5;
	bra.uni 	BB13_2;

BB13_5:
	shr.s32 	%r127, %r6, 31;
	and.b32  	%r128, %r127, 1078530011;
	or.b32  	%r129, %r128, %r7;
	mov.b32 	 %f189, %r129;
	bra.uni 	BB13_6;

BB13_2:
	setp.eq.f32	%p9, %f3, 0f7F800000;
	setp.eq.f32	%p10, %f4, 0f7F800000;
	and.pred  	%p11, %p9, %p10;
	@%p11 bra 	BB13_4;
	bra.uni 	BB13_3;

BB13_4:
	shr.s32 	%r123, %r6, 31;
	and.b32  	%r124, %r123, 13483017;
	add.s32 	%r125, %r124, 1061752795;
	or.b32  	%r126, %r125, %r7;
	mov.b32 	 %f189, %r126;
	bra.uni 	BB13_6;

BB13_3:
	max.f32 	%f58, %f4, %f3;
	min.f32 	%f59, %f4, %f3;
	div.full.f32 	%f60, %f59, %f58;
	mul.rn.f32 	%f61, %f60, %f60;
	mov.f32 	%f62, 0fC0B59883;
	mov.f32 	%f63, 0fBF52C7EA;
	fma.rn.f32 	%f64, %f61, %f63, %f62;
	mov.f32 	%f65, 0fC0D21907;
	fma.rn.f32 	%f66, %f64, %f61, %f65;
	mul.f32 	%f67, %f61, %f66;
	mul.f32 	%f68, %f60, %f67;
	add.f32 	%f69, %f61, 0f41355DC0;
	mov.f32 	%f70, 0f41E6BD60;
	fma.rn.f32 	%f71, %f69, %f61, %f70;
	mov.f32 	%f72, 0f419D92C8;
	fma.rn.f32 	%f73, %f71, %f61, %f72;
	rcp.approx.f32 	%f74, %f73;
	fma.rn.f32 	%f75, %f68, %f74, %f60;
	mov.f32 	%f76, 0f3FC90FDB;
	sub.f32 	%f77, %f76, %f75;
	setp.gt.f32	%p12, %f4, %f3;
	selp.f32	%f78, %f77, %f75, %p12;
	mov.f32 	%f79, 0f40490FDB;
	sub.f32 	%f80, %f79, %f78;
	setp.lt.s32	%p13, %r6, 0;
	selp.f32	%f81, %f80, %f78, %p13;
	mov.b32 	 %r121, %f81;
	or.b32  	%r122, %r121, %r7;
	mov.b32 	 %f82, %r122;
	add.f32 	%f83, %f3, %f4;
	setp.gtu.f32	%p14, %f83, 0f7F800000;
	selp.f32	%f189, %f83, %f82, %p14;

BB13_6:
	mul.f32 	%f84, %f2, %f2;
	fma.rn.f32 	%f85, %f1, %f1, %f84;
	sqrt.approx.f32 	%f86, %f85;
	sqrt.approx.f32 	%f9, %f86;
	mul.f32 	%f10, %f189, 0f3F000000;
	abs.f32 	%f11, %f10;
	setp.neu.f32	%p15, %f11, 0f7F800000;
	mov.f32 	%f196, %f10;
	@%p15 bra 	BB13_8;

	mov.f32 	%f87, 0f00000000;
	mul.rn.f32 	%f12, %f10, %f87;
	mov.f32 	%f196, %f12;

BB13_8:
	mov.f32 	%f13, %f196;
	mul.f32 	%f88, %f13, 0f3F22F983;
	cvt.rni.s32.f32	%r282, %f88;
	cvt.rn.f32.s32	%f89, %r282;
	neg.f32 	%f90, %f89;
	mov.f32 	%f91, 0f3FC90FDA;
	fma.rn.f32 	%f92, %f90, %f91, %f13;
	mov.f32 	%f93, 0f33A22168;
	fma.rn.f32 	%f94, %f90, %f93, %f92;
	mov.f32 	%f95, 0f27C234C5;
	fma.rn.f32 	%f190, %f90, %f95, %f94;
	abs.f32 	%f96, %f13;
	setp.leu.f32	%p16, %f96, 0f47CE4780;
	@%p16 bra 	BB13_16;

	mov.b32 	 %r9, %f13;
	and.b32  	%r10, %r9, -2147483648;
	shl.b32 	%r133, %r9, 8;
	or.b32  	%r11, %r133, -2147483648;
	mov.u32 	%r275, 0;
	mov.u32 	%r276, %r275;
	mov.u32 	%r272, %r275;
	mov.u32 	%r273, %r1;
	mov.pred 	%p17, 0;
	@%p17 bra 	BB13_11;

BB13_10:
	shl.b32 	%r139, %r272, 2;
	mov.u32 	%r140, __cudart_i2opi_f;
	add.s32 	%r141, %r140, %r139;
	ld.const.u32 	%r136, [%r141];
	// inline asm
	{
	mad.lo.cc.u32   %r134, %r136, %r11, %r276;
	madc.hi.u32     %r276, %r136, %r11,  0;
	}
	// inline asm
	st.local.u32 	[%r273], %r134;
	add.s32 	%r272, %r272, 1;
	setp.lt.s32	%p18, %r272, 6;
	add.s32 	%r142, %r139, %r1;
	add.s32 	%r273, %r142, 4;
	mov.u32 	%r275, %r276;
	@%p18 bra 	BB13_10;

BB13_11:
	bfe.u32 	%r143, %r9, 23, 8;
	add.s32 	%r144, %r143, -128;
	shr.u32 	%r145, %r144, 5;
	mov.u32 	%r146, 4;
	sub.s32 	%r147, %r146, %r145;
	st.local.u32 	[%r273], %r275;
	shl.b32 	%r148, %r147, 2;
	add.s32 	%r149, %r148, %r1;
	ld.local.u32 	%r277, [%r149+8];
	ld.local.u32 	%r278, [%r149+4];
	bfe.u32 	%r24, %r9, 23, 5;
	setp.eq.s32	%p19, %r24, 0;
	@%p19 bra 	BB13_13;

	mov.u32 	%r150, 32;
	sub.s32 	%r151, %r150, %r24;
	shr.u32 	%r152, %r278, %r151;
	shl.b32 	%r153, %r277, %r24;
	add.s32 	%r277, %r152, %r153;
	add.s32 	%r270, %r149, 8;
	ld.local.u32 	%r154, [%r270+-8];
	shr.u32 	%r155, %r154, %r151;
	shl.b32 	%r156, %r278, %r24;
	add.s32 	%r278, %r155, %r156;

BB13_13:
	shr.u32 	%r157, %r278, 30;
	shl.b32 	%r158, %r277, 2;
	add.s32 	%r279, %r157, %r158;
	shl.b32 	%r30, %r278, 2;
	shr.u32 	%r159, %r279, 31;
	shr.u32 	%r160, %r277, 30;
	add.s32 	%r31, %r159, %r160;
	setp.eq.s32	%p20, %r159, 0;
	mov.u32 	%r280, %r10;
	mov.u32 	%r281, %r30;
	@%p20 bra 	BB13_15;

	not.b32 	%r161, %r279;
	neg.s32 	%r32, %r30;
	setp.eq.s32	%p21, %r30, 0;
	selp.u32	%r162, 1, 0, %p21;
	add.s32 	%r279, %r162, %r161;
	xor.b32  	%r34, %r10, -2147483648;
	mov.u32 	%r280, %r34;
	mov.u32 	%r281, %r32;

BB13_15:
	mov.u32 	%r36, %r280;
	neg.s32 	%r163, %r31;
	setp.ne.s32	%p22, %r10, 0;
	selp.b32	%r282, %r163, %r31, %p22;
	clz.b32 	%r164, %r279;
	setp.ne.s32	%p23, %r164, 0;
	shl.b32 	%r165, %r279, %r164;
	mov.u32 	%r166, 32;
	sub.s32 	%r167, %r166, %r164;
	shr.u32 	%r168, %r281, %r167;
	add.s32 	%r169, %r168, %r165;
	selp.b32	%r170, %r169, %r279, %p23;
	mul.lo.s32 	%r171, %r170, -921707870;
	mov.u32 	%r172, -921707870;
	mul.hi.u32 	%r173, %r170, %r172;
	setp.gt.s32	%p24, %r173, 0;
	shl.b32 	%r174, %r173, 1;
	shr.u32 	%r175, %r171, 31;
	add.s32 	%r176, %r175, %r174;
	selp.b32	%r177, %r176, %r173, %p24;
	selp.b32	%r178, -1, 0, %p24;
	mov.u32 	%r179, 126;
	sub.s32 	%r180, %r179, %r164;
	add.s32 	%r181, %r180, %r178;
	shl.b32 	%r182, %r181, 23;
	add.s32 	%r183, %r177, 1;
	shr.u32 	%r184, %r183, 7;
	add.s32 	%r185, %r184, 1;
	shr.u32 	%r186, %r185, 1;
	add.s32 	%r187, %r186, %r182;
	or.b32  	%r188, %r187, %r36;
	mov.b32 	 %f190, %r188;

BB13_16:
	mul.rn.f32 	%f17, %f190, %f190;
	add.s32 	%r40, %r282, 1;
	and.b32  	%r41, %r40, 1;
	setp.eq.s32	%p25, %r41, 0;
	@%p25 bra 	BB13_18;

	mov.f32 	%f97, 0fBAB6061A;
	mov.f32 	%f98, 0f37CCF5CE;
	fma.rn.f32 	%f191, %f98, %f17, %f97;
	bra.uni 	BB13_19;

BB13_18:
	mov.f32 	%f99, 0f3C08839E;
	mov.f32 	%f100, 0fB94CA1F9;
	fma.rn.f32 	%f191, %f100, %f17, %f99;

BB13_19:
	@%p25 bra 	BB13_21;

	mov.f32 	%f101, 0f3D2AAAA5;
	fma.rn.f32 	%f102, %f191, %f17, %f101;
	mov.f32 	%f103, 0fBF000000;
	fma.rn.f32 	%f192, %f102, %f17, %f103;
	bra.uni 	BB13_22;

BB13_21:
	mov.f32 	%f104, 0fBE2AAAA3;
	fma.rn.f32 	%f105, %f191, %f17, %f104;
	mov.f32 	%f106, 0f00000000;
	fma.rn.f32 	%f192, %f105, %f17, %f106;

BB13_22:
	fma.rn.f32 	%f193, %f192, %f190, %f190;
	@%p25 bra 	BB13_24;

	mov.f32 	%f107, 0f3F800000;
	fma.rn.f32 	%f193, %f192, %f17, %f107;

BB13_24:
	and.b32  	%r189, %r40, 2;
	setp.eq.s32	%p28, %r189, 0;
	@%p28 bra 	BB13_26;

	mov.f32 	%f108, 0f00000000;
	mov.f32 	%f109, 0fBF800000;
	fma.rn.f32 	%f193, %f193, %f109, %f108;

BB13_26:
	mul.f32 	%f29, %f9, %f193;
	mov.f32 	%f195, %f10;
	@%p15 bra 	BB13_28;

	mov.f32 	%f110, 0f00000000;
	mul.rn.f32 	%f195, %f10, %f110;

BB13_28:
	mul.f32 	%f111, %f195, 0f3F22F983;
	cvt.rni.s32.f32	%r293, %f111;
	cvt.rn.f32.s32	%f112, %r293;
	neg.f32 	%f113, %f112;
	fma.rn.f32 	%f115, %f113, %f91, %f195;
	fma.rn.f32 	%f117, %f113, %f93, %f115;
	fma.rn.f32 	%f197, %f113, %f95, %f117;
	abs.f32 	%f119, %f195;
	setp.leu.f32	%p30, %f119, 0f47CE4780;
	@%p30 bra 	BB13_36;

	mov.b32 	 %r43, %f195;
	and.b32  	%r44, %r43, -2147483648;
	shl.b32 	%r193, %r43, 8;
	or.b32  	%r45, %r193, -2147483648;
	mov.u32 	%r286, 0;
	mov.u32 	%r287, %r286;
	mov.u32 	%r283, %r286;
	mov.u32 	%r284, %r1;
	mov.pred 	%p31, 0;
	@%p31 bra 	BB13_31;

BB13_30:
	shl.b32 	%r199, %r283, 2;
	mov.u32 	%r200, __cudart_i2opi_f;
	add.s32 	%r201, %r200, %r199;
	ld.const.u32 	%r196, [%r201];
	// inline asm
	{
	mad.lo.cc.u32   %r194, %r196, %r45, %r287;
	madc.hi.u32     %r287, %r196, %r45,  0;
	}
	// inline asm
	st.local.u32 	[%r284], %r194;
	add.s32 	%r283, %r283, 1;
	setp.lt.s32	%p32, %r283, 6;
	add.s32 	%r202, %r199, %r1;
	add.s32 	%r284, %r202, 4;
	mov.u32 	%r286, %r287;
	@%p32 bra 	BB13_30;

BB13_31:
	bfe.u32 	%r203, %r43, 23, 8;
	add.s32 	%r204, %r203, -128;
	shr.u32 	%r205, %r204, 5;
	mov.u32 	%r206, 4;
	sub.s32 	%r207, %r206, %r205;
	st.local.u32 	[%r284], %r286;
	shl.b32 	%r208, %r207, 2;
	add.s32 	%r209, %r208, %r1;
	ld.local.u32 	%r288, [%r209+8];
	ld.local.u32 	%r289, [%r209+4];
	bfe.u32 	%r58, %r43, 23, 5;
	setp.eq.s32	%p33, %r58, 0;
	@%p33 bra 	BB13_33;

	mov.u32 	%r210, 32;
	sub.s32 	%r211, %r210, %r58;
	shr.u32 	%r212, %r289, %r211;
	shl.b32 	%r213, %r288, %r58;
	add.s32 	%r288, %r212, %r213;
	add.s32 	%r271, %r209, 8;
	ld.local.u32 	%r214, [%r271+-8];
	shr.u32 	%r215, %r214, %r211;
	shl.b32 	%r216, %r289, %r58;
	add.s32 	%r289, %r215, %r216;

BB13_33:
	shr.u32 	%r217, %r289, 30;
	shl.b32 	%r218, %r288, 2;
	add.s32 	%r290, %r217, %r218;
	shl.b32 	%r64, %r289, 2;
	shr.u32 	%r219, %r290, 31;
	shr.u32 	%r220, %r288, 30;
	add.s32 	%r65, %r219, %r220;
	setp.eq.s32	%p34, %r219, 0;
	mov.u32 	%r291, %r44;
	mov.u32 	%r292, %r64;
	@%p34 bra 	BB13_35;

	not.b32 	%r221, %r290;
	neg.s32 	%r66, %r64;
	setp.eq.s32	%p35, %r64, 0;
	selp.u32	%r222, 1, 0, %p35;
	add.s32 	%r290, %r222, %r221;
	xor.b32  	%r68, %r44, -2147483648;
	mov.u32 	%r291, %r68;
	mov.u32 	%r292, %r66;

BB13_35:
	mov.u32 	%r70, %r291;
	neg.s32 	%r223, %r65;
	setp.ne.s32	%p36, %r44, 0;
	selp.b32	%r293, %r223, %r65, %p36;
	clz.b32 	%r224, %r290;
	setp.ne.s32	%p37, %r224, 0;
	shl.b32 	%r225, %r290, %r224;
	mov.u32 	%r226, 32;
	sub.s32 	%r227, %r226, %r224;
	shr.u32 	%r228, %r292, %r227;
	add.s32 	%r229, %r228, %r225;
	selp.b32	%r230, %r229, %r290, %p37;
	mul.lo.s32 	%r231, %r230, -921707870;
	mov.u32 	%r232, -921707870;
	mul.hi.u32 	%r233, %r230, %r232;
	setp.gt.s32	%p38, %r233, 0;
	shl.b32 	%r234, %r233, 1;
	shr.u32 	%r235, %r231, 31;
	add.s32 	%r236, %r235, %r234;
	selp.b32	%r237, %r236, %r233, %p38;
	selp.b32	%r238, -1, 0, %p38;
	mov.u32 	%r239, 126;
	sub.s32 	%r240, %r239, %r224;
	add.s32 	%r241, %r240, %r238;
	shl.b32 	%r242, %r241, 23;
	add.s32 	%r243, %r237, 1;
	shr.u32 	%r244, %r243, 7;
	add.s32 	%r245, %r244, 1;
	shr.u32 	%r246, %r245, 1;
	add.s32 	%r247, %r246, %r242;
	or.b32  	%r248, %r247, %r70;
	mov.b32 	 %f197, %r248;

BB13_36:
	mul.rn.f32 	%f35, %f197, %f197;
	add.s32 	%r74, %r293, 1;
	and.b32  	%r75, %r74, 1;
	setp.eq.s32	%p39, %r75, 0;
	@%p39 bra 	BB13_38;

	mov.f32 	%f120, 0fBAB6061A;
	mov.f32 	%f121, 0f37CCF5CE;
	fma.rn.f32 	%f198, %f121, %f35, %f120;
	bra.uni 	BB13_39;

BB13_38:
	mov.f32 	%f122, 0f3C08839E;
	mov.f32 	%f123, 0fB94CA1F9;
	fma.rn.f32 	%f198, %f123, %f35, %f122;

BB13_39:
	@%p39 bra 	BB13_41;

	mov.f32 	%f124, 0f3D2AAAA5;
	fma.rn.f32 	%f125, %f198, %f35, %f124;
	mov.f32 	%f126, 0fBF000000;
	fma.rn.f32 	%f199, %f125, %f35, %f126;
	bra.uni 	BB13_42;

BB13_41:
	mov.f32 	%f127, 0fBE2AAAA3;
	fma.rn.f32 	%f128, %f198, %f35, %f127;
	mov.f32 	%f129, 0f00000000;
	fma.rn.f32 	%f199, %f128, %f35, %f129;

BB13_42:
	fma.rn.f32 	%f200, %f199, %f197, %f197;
	@%p39 bra 	BB13_44;

	mov.f32 	%f130, 0f3F800000;
	fma.rn.f32 	%f200, %f199, %f35, %f130;

BB13_44:
	and.b32  	%r249, %r74, 2;
	setp.eq.s32	%p42, %r249, 0;
	@%p42 bra 	BB13_46;

	mov.f32 	%f131, 0f00000000;
	mov.f32 	%f132, 0fBF800000;
	fma.rn.f32 	%f200, %f200, %f132, %f131;

BB13_46:
	shl.b32 	%r250, %r5, 2;
	add.s32 	%r251, %r88, %r250;
	ld.global.f32 	%f133, [%r251];
	mul.f32 	%f134, %f133, %f133;
	add.s32 	%r252, %r87, %r250;
	ld.global.f32 	%f135, [%r252];
	fma.rn.f32 	%f136, %f135, %f135, %f134;
	add.s32 	%r253, %r89, %r250;
	ld.global.f32 	%f137, [%r253];
	fma.rn.f32 	%f138, %f137, %f137, %f136;
	add.s32 	%r254, %r90, %r250;
	ld.global.f32 	%f139, [%r254];
	mul.f32 	%f140, %f133, %f139;
	fma.rn.f32 	%f141, %f135, %f133, %f140;
	add.s32 	%r255, %r91, %r250;
	ld.global.f32 	%f142, [%r255];
	fma.rn.f32 	%f143, %f137, %f142, %f141;
	mul.f32 	%f144, %f142, %f133;
	fma.rn.f32 	%f145, %f135, %f137, %f144;
	add.s32 	%r256, %r92, %r250;
	ld.global.f32 	%f146, [%r256];
	fma.rn.f32 	%f147, %f137, %f146, %f145;
	mul.f32 	%f148, %f139, %f139;
	fma.rn.f32 	%f149, %f133, %f133, %f148;
	fma.rn.f32 	%f150, %f142, %f142, %f149;
	mul.f32 	%f151, %f139, %f142;
	fma.rn.f32 	%f152, %f133, %f137, %f151;
	fma.rn.f32 	%f153, %f142, %f146, %f152;
	mul.f32 	%f154, %f142, %f142;
	fma.rn.f32 	%f155, %f137, %f137, %f154;
	fma.rn.f32 	%f156, %f146, %f146, %f155;
	mul.f32 	%f157, %f29, %f200;
	add.s32 	%r257, %r76, %r250;
	ld.global.f32 	%f158, [%r257];
	fma.rn.f32 	%f159, %f157, %f138, %f158;
	st.global.f32 	[%r257], %f159;
	add.s32 	%r258, %r77, %r250;
	ld.global.f32 	%f160, [%r258];
	fma.rn.f32 	%f161, %f157, %f143, %f160;
	st.global.f32 	[%r258], %f161;
	add.s32 	%r259, %r78, %r250;
	ld.global.f32 	%f162, [%r259];
	fma.rn.f32 	%f163, %f157, %f147, %f162;
	st.global.f32 	[%r259], %f163;
	add.s32 	%r260, %r79, %r250;
	ld.global.f32 	%f164, [%r260];
	fma.rn.f32 	%f165, %f157, %f150, %f164;
	st.global.f32 	[%r260], %f165;
	add.s32 	%r261, %r80, %r250;
	ld.global.f32 	%f166, [%r261];
	fma.rn.f32 	%f167, %f157, %f153, %f166;
	st.global.f32 	[%r261], %f167;
	add.s32 	%r262, %r81, %r250;
	ld.global.f32 	%f168, [%r262];
	fma.rn.f32 	%f169, %f157, %f156, %f168;
	st.global.f32 	[%r262], %f169;
	mul.f32 	%f170, %f189, %f157;
	shl.b32 	%r263, %r98, 2;
	add.s32 	%r264, %r94, %r263;
	ld.const.f32 	%f171, [%r264];
	mul.f32 	%f172, %f143, %f171;
	add.s32 	%r265, %r93, %r263;
	ld.const.f32 	%f173, [%r265];
	fma.rn.f32 	%f174, %f173, %f138, %f172;
	add.s32 	%r266, %r95, %r263;
	ld.const.f32 	%f175, [%r266];
	fma.rn.f32 	%f176, %f175, %f147, %f174;
	add.s32 	%r267, %r82, %r250;
	ld.global.f32 	%f177, [%r267];
	fma.rn.f32 	%f178, %f170, %f176, %f177;
	st.global.f32 	[%r267], %f178;
	mul.f32 	%f179, %f150, %f171;
	fma.rn.f32 	%f180, %f173, %f143, %f179;
	fma.rn.f32 	%f181, %f175, %f153, %f180;
	add.s32 	%r268, %r83, %r250;
	ld.global.f32 	%f182, [%r268];
	fma.rn.f32 	%f183, %f170, %f181, %f182;
	st.global.f32 	[%r268], %f183;
	mul.f32 	%f184, %f153, %f171;
	fma.rn.f32 	%f185, %f173, %f147, %f184;
	fma.rn.f32 	%f186, %f175, %f156, %f185;
	add.s32 	%r269, %r84, %r250;
	ld.global.f32 	%f187, [%r269];
	fma.rn.f32 	%f188, %f170, %f186, %f187;
	st.global.f32 	[%r269], %f188;

BB13_47:
	ret;
}

	// .globl	CalculateDisplacementUpdate
.entry CalculateDisplacementUpdate(
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_0,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_1,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_2,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_3,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_4,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_5,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_6,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_7,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_8,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_9,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_10,
	.param .u32 .ptr .global .align 4 CalculateDisplacementUpdate_param_11,
	.param .u32 CalculateDisplacementUpdate_param_12,
	.param .u32 CalculateDisplacementUpdate_param_13,
	.param .u32 CalculateDisplacementUpdate_param_14
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<49>;


	ld.param.u32 	%r4, [CalculateDisplacementUpdate_param_0];
	ld.param.u32 	%r5, [CalculateDisplacementUpdate_param_1];
	ld.param.u32 	%r6, [CalculateDisplacementUpdate_param_2];
	ld.param.u32 	%r7, [CalculateDisplacementUpdate_param_3];
	ld.param.u32 	%r8, [CalculateDisplacementUpdate_param_4];
	ld.param.u32 	%r9, [CalculateDisplacementUpdate_param_5];
	ld.param.u32 	%r10, [CalculateDisplacementUpdate_param_6];
	ld.param.u32 	%r11, [CalculateDisplacementUpdate_param_7];
	ld.param.u32 	%r12, [CalculateDisplacementUpdate_param_8];
	ld.param.u32 	%r13, [CalculateDisplacementUpdate_param_9];
	ld.param.u32 	%r14, [CalculateDisplacementUpdate_param_10];
	ld.param.u32 	%r15, [CalculateDisplacementUpdate_param_11];
	ld.param.u32 	%r16, [CalculateDisplacementUpdate_param_12];
	ld.param.u32 	%r17, [CalculateDisplacementUpdate_param_13];
	ld.param.u32 	%r18, [CalculateDisplacementUpdate_param_14];
	mov.b32	%r19, %envreg3;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ntid.x;
	mad.lo.s32 	%r22, %r20, %r21, %r19;
	mov.u32 	%r23, %tid.x;
	add.s32 	%r1, %r22, %r23;
	mov.u32 	%r24, %ctaid.y;
	mov.u32 	%r25, %ntid.y;
	mov.b32	%r26, %envreg4;
	mad.lo.s32 	%r27, %r24, %r25, %r26;
	mov.u32 	%r28, %tid.y;
	add.s32 	%r2, %r27, %r28;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %ntid.z;
	mov.b32	%r31, %envreg5;
	mad.lo.s32 	%r32, %r29, %r30, %r31;
	mov.u32 	%r33, %tid.z;
	add.s32 	%r3, %r32, %r33;
	setp.ge.s32	%p1, %r1, %r16;
	setp.ge.s32	%p2, %r2, %r17;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r18;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB14_2;

	mad.lo.s32 	%r34, %r3, %r17, %r2;
	mad.lo.s32 	%r35, %r34, %r16, %r1;
	shl.b32 	%r36, %r35, 2;
	add.s32 	%r37, %r7, %r36;
	add.s32 	%r38, %r8, %r36;
	add.s32 	%r39, %r9, %r36;
	add.s32 	%r40, %r10, %r36;
	add.s32 	%r41, %r11, %r36;
	add.s32 	%r42, %r12, %r36;
	add.s32 	%r43, %r13, %r36;
	add.s32 	%r44, %r14, %r36;
	add.s32 	%r45, %r15, %r36;
	ld.global.f32 	%f1, [%r40];
	ld.global.f32 	%f2, [%r37];
	mul.f32 	%f3, %f2, %f1;
	ld.global.f32 	%f4, [%r41];
	mul.f32 	%f5, %f2, %f4;
	mul.f32 	%f6, %f4, %f5;
	neg.f32 	%f7, %f6;
	ld.global.f32 	%f8, [%r42];
	fma.rn.f32 	%f9, %f3, %f8, %f7;
	ld.global.f32 	%f10, [%r38];
	mul.f32 	%f11, %f10, %f10;
	neg.f32 	%f12, %f11;
	fma.rn.f32 	%f13, %f12, %f8, %f9;
	mul.f32 	%f14, %f10, %f4;
	ld.global.f32 	%f15, [%r39];
	fma.rn.f32 	%f16, %f14, %f15, %f13;
	mul.f32 	%f17, %f10, %f15;
	fma.rn.f32 	%f18, %f17, %f4, %f16;
	mul.f32 	%f19, %f15, %f1;
	neg.f32 	%f20, %f19;
	fma.rn.f32 	%f21, %f20, %f15, %f18;
	add.f32 	%f22, %f21, 0f24E69595;
	rcp.approx.f32 	%f23, %f22;
	fma.rn.f32 	%f24, %f10, %f4, %f20;
	mul.f32 	%f25, %f15, %f4;
	neg.f32 	%f26, %f25;
	fma.rn.f32 	%f27, %f10, %f8, %f26;
	ld.global.f32 	%f28, [%r44];
	mul.f32 	%f29, %f28, %f27;
	neg.f32 	%f30, %f29;
	ld.global.f32 	%f31, [%r45];
	fma.rn.f32 	%f32, %f31, %f24, %f30;
	mul.f32 	%f33, %f4, %f4;
	neg.f32 	%f34, %f33;
	fma.rn.f32 	%f35, %f1, %f8, %f34;
	ld.global.f32 	%f36, [%r43];
	fma.rn.f32 	%f37, %f36, %f35, %f32;
	mul.f32 	%f38, %f23, %f37;
	add.s32 	%r46, %r4, %r36;
	st.global.f32 	[%r46], %f38;
	mul.f32 	%f39, %f15, %f15;
	neg.f32 	%f40, %f39;
	fma.rn.f32 	%f41, %f2, %f8, %f40;
	neg.f32 	%f42, %f17;
	fma.rn.f32 	%f43, %f2, %f4, %f42;
	mul.f32 	%f44, %f31, %f43;
	neg.f32 	%f45, %f44;
	fma.rn.f32 	%f46, %f28, %f41, %f45;
	neg.f32 	%f47, %f36;
	fma.rn.f32 	%f48, %f47, %f27, %f46;
	mul.f32 	%f49, %f23, %f48;
	add.s32 	%r47, %r5, %r36;
	st.global.f32 	[%r47], %f49;
	fma.rn.f32 	%f50, %f2, %f1, %f12;
	mul.f32 	%f51, %f28, %f43;
	neg.f32 	%f52, %f51;
	fma.rn.f32 	%f53, %f31, %f50, %f52;
	fma.rn.f32 	%f54, %f36, %f24, %f53;
	mul.f32 	%f55, %f23, %f54;
	add.s32 	%r48, %r6, %r36;
	st.global.f32 	[%r48], %f55;

BB14_2:
	ret;
}

	// .globl	InterpolateVolumeNearestLinear
.entry InterpolateVolumeNearestLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeNearestLinear_param_0,
	.param .texref InterpolateVolumeNearestLinear_param_1,
	.param .u32 .ptr .const .align 4 InterpolateVolumeNearestLinear_param_2,
	.param .u32 InterpolateVolumeNearestLinear_param_3,
	.param .u32 InterpolateVolumeNearestLinear_param_4,
	.param .u32 InterpolateVolumeNearestLinear_param_5,
	.param .u32 InterpolateVolumeNearestLinear_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<47>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [InterpolateVolumeNearestLinear_param_0];
	ld.param.u32 	%r5, [InterpolateVolumeNearestLinear_param_2];
	ld.param.u32 	%r6, [InterpolateVolumeNearestLinear_param_3];
	ld.param.u32 	%r7, [InterpolateVolumeNearestLinear_param_4];
	ld.param.u32 	%r8, [InterpolateVolumeNearestLinear_param_5];
	ld.param.u32 	%r9, [InterpolateVolumeNearestLinear_param_6];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %ntid.y;
	mov.b32	%r17, %envreg4;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %tid.y;
	add.s32 	%r2, %r18, %r19;
	mov.u32 	%r20, %ctaid.z;
	mov.u32 	%r21, %ntid.z;
	mov.b32	%r22, %envreg5;
	mad.lo.s32 	%r23, %r20, %r21, %r22;
	mov.u32 	%r24, %tid.z;
	add.s32 	%r3, %r23, %r24;
	setp.ge.s32	%p1, %r1, %r6;
	setp.ge.s32	%p2, %r2, %r7;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r8;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB15_2;

	cvt.rn.f32.s32	%f1, %r1;
	cvt.rn.f32.s32	%f2, %r6;
	add.f32 	%f3, %f2, 0fBF800000;
	neg.f32 	%f4, %f3;
	fma.rn.f32 	%f5, %f4, 0f3F000000, %f1;
	cvt.rn.f32.s32	%f6, %r7;
	add.f32 	%f7, %f6, 0fBF800000;
	neg.f32 	%f8, %f7;
	cvt.rn.f32.s32	%f9, %r2;
	fma.rn.f32 	%f10, %f8, 0f3F000000, %f9;
	cvt.rn.f32.s32	%f11, %r8;
	add.f32 	%f12, %f11, 0fBF800000;
	neg.f32 	%f13, %f12;
	cvt.rn.f32.s32	%f14, %r3;
	fma.rn.f32 	%f15, %f13, 0f3F000000, %f14;
	ld.const.f32 	%f16, [%r5];
	add.f32 	%f17, %f1, %f16;
	ld.const.f32 	%f18, [%r5+12];
	fma.rn.f32 	%f19, %f5, %f18, %f17;
	ld.const.f32 	%f20, [%r5+16];
	fma.rn.f32 	%f21, %f10, %f20, %f19;
	ld.const.f32 	%f22, [%r5+20];
	fma.rn.f32 	%f23, %f15, %f22, %f21;
	add.f32 	%f24, %f23, 0f3F000000;
	ld.const.f32 	%f25, [%r5+4];
	add.f32 	%f26, %f9, %f25;
	ld.const.f32 	%f27, [%r5+24];
	fma.rn.f32 	%f28, %f5, %f27, %f26;
	ld.const.f32 	%f29, [%r5+28];
	fma.rn.f32 	%f30, %f10, %f29, %f28;
	ld.const.f32 	%f31, [%r5+32];
	fma.rn.f32 	%f32, %f15, %f31, %f30;
	add.f32 	%f33, %f32, 0f3F000000;
	ld.const.f32 	%f34, [%r5+8];
	add.f32 	%f35, %f14, %f34;
	ld.const.f32 	%f36, [%r5+36];
	fma.rn.f32 	%f37, %f5, %f36, %f35;
	ld.const.f32 	%f38, [%r5+40];
	fma.rn.f32 	%f39, %f10, %f38, %f37;
	ld.const.f32 	%f40, [%r5+44];
	fma.rn.f32 	%f41, %f15, %f40, %f39;
	add.f32 	%f42, %f41, 0f3F000000;
	tex.3d.v4.f32.f32	{%f43, %f44, %f45, %f46}, [InterpolateVolumeNearestLinear_param_1, volume_sampler_nearest, {%f24, %f33, %f42, %f42}];
	mad.lo.s32 	%r25, %r9, %r8, %r3;
	mad.lo.s32 	%r26, %r25, %r7, %r2;
	mad.lo.s32 	%r27, %r26, %r6, %r1;
	shl.b32 	%r28, %r27, 2;
	add.s32 	%r29, %r4, %r28;
	st.global.f32 	[%r29], %f43;

BB15_2:
	ret;
}

	// .globl	InterpolateVolumeNearestNonLinear
.entry InterpolateVolumeNearestNonLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeNearestNonLinear_param_0,
	.param .texref InterpolateVolumeNearestNonLinear_param_1,
	.param .u32 .ptr .global .align 4 InterpolateVolumeNearestNonLinear_param_2,
	.param .u32 .ptr .global .align 4 InterpolateVolumeNearestNonLinear_param_3,
	.param .u32 .ptr .global .align 4 InterpolateVolumeNearestNonLinear_param_4,
	.param .u32 InterpolateVolumeNearestNonLinear_param_5,
	.param .u32 InterpolateVolumeNearestNonLinear_param_6,
	.param .u32 InterpolateVolumeNearestNonLinear_param_7,
	.param .u32 InterpolateVolumeNearestNonLinear_param_8
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [InterpolateVolumeNearestNonLinear_param_0];
	ld.param.u32 	%r5, [InterpolateVolumeNearestNonLinear_param_2];
	ld.param.u32 	%r6, [InterpolateVolumeNearestNonLinear_param_3];
	ld.param.u32 	%r7, [InterpolateVolumeNearestNonLinear_param_4];
	ld.param.u32 	%r8, [InterpolateVolumeNearestNonLinear_param_5];
	ld.param.u32 	%r9, [InterpolateVolumeNearestNonLinear_param_6];
	ld.param.u32 	%r10, [InterpolateVolumeNearestNonLinear_param_7];
	ld.param.u32 	%r11, [InterpolateVolumeNearestNonLinear_param_8];
	mov.b32	%r12, %envreg3;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r15, %r13, %r14, %r12;
	mov.u32 	%r16, %tid.x;
	add.s32 	%r1, %r15, %r16;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %ntid.y;
	mov.b32	%r19, %envreg4;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %tid.y;
	add.s32 	%r2, %r20, %r21;
	mov.u32 	%r22, %ctaid.z;
	mov.u32 	%r23, %ntid.z;
	mov.b32	%r24, %envreg5;
	mad.lo.s32 	%r25, %r22, %r23, %r24;
	mov.u32 	%r26, %tid.z;
	add.s32 	%r3, %r25, %r26;
	setp.ge.s32	%p1, %r1, %r8;
	setp.ge.s32	%p2, %r2, %r9;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r10;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB16_2;

	mad.lo.s32 	%r27, %r3, %r9, %r2;
	mad.lo.s32 	%r28, %r27, %r8, %r1;
	shl.b32 	%r29, %r28, 2;
	add.s32 	%r30, %r5, %r29;
	ld.global.f32 	%f1, [%r30];
	cvt.rn.f32.s32	%f2, %r1;
	add.f32 	%f3, %f2, %f1;
	add.f32 	%f4, %f3, 0f3F000000;
	add.s32 	%r31, %r6, %r29;
	ld.global.f32 	%f5, [%r31];
	cvt.rn.f32.s32	%f6, %r2;
	add.f32 	%f7, %f6, %f5;
	add.f32 	%f8, %f7, 0f3F000000;
	add.s32 	%r32, %r7, %r29;
	ld.global.f32 	%f9, [%r32];
	cvt.rn.f32.s32	%f10, %r3;
	add.f32 	%f11, %f10, %f9;
	add.f32 	%f12, %f11, 0f3F000000;
	tex.3d.v4.f32.f32	{%f13, %f14, %f15, %f16}, [InterpolateVolumeNearestNonLinear_param_1, volume_sampler_nearest, {%f4, %f8, %f12, %f12}];
	mad.lo.s32 	%r33, %r11, %r10, %r3;
	mad.lo.s32 	%r34, %r33, %r9, %r2;
	mad.lo.s32 	%r35, %r34, %r8, %r1;
	shl.b32 	%r36, %r35, 2;
	add.s32 	%r37, %r4, %r36;
	st.global.f32 	[%r37], %f13;

BB16_2:
	ret;
}

	// .globl	InterpolateVolumeLinearLinear
.entry InterpolateVolumeLinearLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeLinearLinear_param_0,
	.param .texref InterpolateVolumeLinearLinear_param_1,
	.param .u32 .ptr .const .align 4 InterpolateVolumeLinearLinear_param_2,
	.param .u32 InterpolateVolumeLinearLinear_param_3,
	.param .u32 InterpolateVolumeLinearLinear_param_4,
	.param .u32 InterpolateVolumeLinearLinear_param_5,
	.param .u32 InterpolateVolumeLinearLinear_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<47>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [InterpolateVolumeLinearLinear_param_0];
	ld.param.u32 	%r5, [InterpolateVolumeLinearLinear_param_2];
	ld.param.u32 	%r6, [InterpolateVolumeLinearLinear_param_3];
	ld.param.u32 	%r7, [InterpolateVolumeLinearLinear_param_4];
	ld.param.u32 	%r8, [InterpolateVolumeLinearLinear_param_5];
	ld.param.u32 	%r9, [InterpolateVolumeLinearLinear_param_6];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %ntid.y;
	mov.b32	%r17, %envreg4;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %tid.y;
	add.s32 	%r2, %r18, %r19;
	mov.u32 	%r20, %ctaid.z;
	mov.u32 	%r21, %ntid.z;
	mov.b32	%r22, %envreg5;
	mad.lo.s32 	%r23, %r20, %r21, %r22;
	mov.u32 	%r24, %tid.z;
	add.s32 	%r3, %r23, %r24;
	setp.ge.s32	%p1, %r1, %r6;
	setp.ge.s32	%p2, %r2, %r7;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r8;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB17_2;

	cvt.rn.f32.s32	%f1, %r1;
	cvt.rn.f32.s32	%f2, %r6;
	add.f32 	%f3, %f2, 0fBF800000;
	neg.f32 	%f4, %f3;
	fma.rn.f32 	%f5, %f4, 0f3F000000, %f1;
	cvt.rn.f32.s32	%f6, %r7;
	add.f32 	%f7, %f6, 0fBF800000;
	neg.f32 	%f8, %f7;
	cvt.rn.f32.s32	%f9, %r2;
	fma.rn.f32 	%f10, %f8, 0f3F000000, %f9;
	cvt.rn.f32.s32	%f11, %r8;
	add.f32 	%f12, %f11, 0fBF800000;
	neg.f32 	%f13, %f12;
	cvt.rn.f32.s32	%f14, %r3;
	fma.rn.f32 	%f15, %f13, 0f3F000000, %f14;
	ld.const.f32 	%f16, [%r5];
	add.f32 	%f17, %f1, %f16;
	ld.const.f32 	%f18, [%r5+12];
	fma.rn.f32 	%f19, %f5, %f18, %f17;
	ld.const.f32 	%f20, [%r5+16];
	fma.rn.f32 	%f21, %f10, %f20, %f19;
	ld.const.f32 	%f22, [%r5+20];
	fma.rn.f32 	%f23, %f15, %f22, %f21;
	add.f32 	%f24, %f23, 0f3F000000;
	ld.const.f32 	%f25, [%r5+4];
	add.f32 	%f26, %f9, %f25;
	ld.const.f32 	%f27, [%r5+24];
	fma.rn.f32 	%f28, %f5, %f27, %f26;
	ld.const.f32 	%f29, [%r5+28];
	fma.rn.f32 	%f30, %f10, %f29, %f28;
	ld.const.f32 	%f31, [%r5+32];
	fma.rn.f32 	%f32, %f15, %f31, %f30;
	add.f32 	%f33, %f32, 0f3F000000;
	ld.const.f32 	%f34, [%r5+8];
	add.f32 	%f35, %f14, %f34;
	ld.const.f32 	%f36, [%r5+36];
	fma.rn.f32 	%f37, %f5, %f36, %f35;
	ld.const.f32 	%f38, [%r5+40];
	fma.rn.f32 	%f39, %f10, %f38, %f37;
	ld.const.f32 	%f40, [%r5+44];
	fma.rn.f32 	%f41, %f15, %f40, %f39;
	add.f32 	%f42, %f41, 0f3F000000;
	tex.3d.v4.f32.f32	{%f43, %f44, %f45, %f46}, [InterpolateVolumeLinearLinear_param_1, volume_sampler_linear, {%f24, %f33, %f42, %f42}];
	mad.lo.s32 	%r25, %r9, %r8, %r3;
	mad.lo.s32 	%r26, %r25, %r7, %r2;
	mad.lo.s32 	%r27, %r26, %r6, %r1;
	shl.b32 	%r28, %r27, 2;
	add.s32 	%r29, %r4, %r28;
	st.global.f32 	[%r29], %f43;

BB17_2:
	ret;
}

	// .globl	InterpolateVolumeLinearNonLinear
.entry InterpolateVolumeLinearNonLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeLinearNonLinear_param_0,
	.param .texref InterpolateVolumeLinearNonLinear_param_1,
	.param .u32 .ptr .global .align 4 InterpolateVolumeLinearNonLinear_param_2,
	.param .u32 .ptr .global .align 4 InterpolateVolumeLinearNonLinear_param_3,
	.param .u32 .ptr .global .align 4 InterpolateVolumeLinearNonLinear_param_4,
	.param .u32 InterpolateVolumeLinearNonLinear_param_5,
	.param .u32 InterpolateVolumeLinearNonLinear_param_6,
	.param .u32 InterpolateVolumeLinearNonLinear_param_7,
	.param .u32 InterpolateVolumeLinearNonLinear_param_8
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [InterpolateVolumeLinearNonLinear_param_0];
	ld.param.u32 	%r5, [InterpolateVolumeLinearNonLinear_param_2];
	ld.param.u32 	%r6, [InterpolateVolumeLinearNonLinear_param_3];
	ld.param.u32 	%r7, [InterpolateVolumeLinearNonLinear_param_4];
	ld.param.u32 	%r8, [InterpolateVolumeLinearNonLinear_param_5];
	ld.param.u32 	%r9, [InterpolateVolumeLinearNonLinear_param_6];
	ld.param.u32 	%r10, [InterpolateVolumeLinearNonLinear_param_7];
	ld.param.u32 	%r11, [InterpolateVolumeLinearNonLinear_param_8];
	mov.b32	%r12, %envreg3;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mad.lo.s32 	%r15, %r13, %r14, %r12;
	mov.u32 	%r16, %tid.x;
	add.s32 	%r1, %r15, %r16;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %ntid.y;
	mov.b32	%r19, %envreg4;
	mad.lo.s32 	%r20, %r17, %r18, %r19;
	mov.u32 	%r21, %tid.y;
	add.s32 	%r2, %r20, %r21;
	mov.u32 	%r22, %ctaid.z;
	mov.u32 	%r23, %ntid.z;
	mov.b32	%r24, %envreg5;
	mad.lo.s32 	%r25, %r22, %r23, %r24;
	mov.u32 	%r26, %tid.z;
	add.s32 	%r3, %r25, %r26;
	setp.ge.s32	%p1, %r1, %r8;
	setp.ge.s32	%p2, %r2, %r9;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r10;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB18_2;

	mad.lo.s32 	%r27, %r3, %r9, %r2;
	mad.lo.s32 	%r28, %r27, %r8, %r1;
	shl.b32 	%r29, %r28, 2;
	add.s32 	%r30, %r5, %r29;
	ld.global.f32 	%f1, [%r30];
	cvt.rn.f32.s32	%f2, %r1;
	add.f32 	%f3, %f2, %f1;
	add.f32 	%f4, %f3, 0f3F000000;
	add.s32 	%r31, %r6, %r29;
	ld.global.f32 	%f5, [%r31];
	cvt.rn.f32.s32	%f6, %r2;
	add.f32 	%f7, %f6, %f5;
	add.f32 	%f8, %f7, 0f3F000000;
	add.s32 	%r32, %r7, %r29;
	ld.global.f32 	%f9, [%r32];
	cvt.rn.f32.s32	%f10, %r3;
	add.f32 	%f11, %f10, %f9;
	add.f32 	%f12, %f11, 0f3F000000;
	tex.3d.v4.f32.f32	{%f13, %f14, %f15, %f16}, [InterpolateVolumeLinearNonLinear_param_1, volume_sampler_linear, {%f4, %f8, %f12, %f12}];
	mad.lo.s32 	%r33, %r11, %r10, %r3;
	mad.lo.s32 	%r34, %r33, %r9, %r2;
	mad.lo.s32 	%r35, %r34, %r8, %r1;
	shl.b32 	%r36, %r35, 2;
	add.s32 	%r37, %r4, %r36;
	st.global.f32 	[%r37], %f13;

BB18_2:
	ret;
}

	// .globl	AddLinearAndNonLinearDisplacement
.entry AddLinearAndNonLinearDisplacement(
	.param .u32 .ptr .global .align 4 AddLinearAndNonLinearDisplacement_param_0,
	.param .u32 .ptr .global .align 4 AddLinearAndNonLinearDisplacement_param_1,
	.param .u32 .ptr .global .align 4 AddLinearAndNonLinearDisplacement_param_2,
	.param .u32 .ptr .const .align 4 AddLinearAndNonLinearDisplacement_param_3,
	.param .u32 AddLinearAndNonLinearDisplacement_param_4,
	.param .u32 AddLinearAndNonLinearDisplacement_param_5,
	.param .u32 AddLinearAndNonLinearDisplacement_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<32>;


	ld.param.u32 	%r4, [AddLinearAndNonLinearDisplacement_param_0];
	ld.param.u32 	%r5, [AddLinearAndNonLinearDisplacement_param_1];
	ld.param.u32 	%r6, [AddLinearAndNonLinearDisplacement_param_2];
	ld.param.u32 	%r7, [AddLinearAndNonLinearDisplacement_param_3];
	ld.param.u32 	%r8, [AddLinearAndNonLinearDisplacement_param_4];
	ld.param.u32 	%r9, [AddLinearAndNonLinearDisplacement_param_5];
	ld.param.u32 	%r10, [AddLinearAndNonLinearDisplacement_param_6];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r14, %r12, %r13, %r11;
	mov.u32 	%r15, %tid.x;
	add.s32 	%r1, %r14, %r15;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %ntid.y;
	mov.b32	%r18, %envreg4;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %tid.y;
	add.s32 	%r2, %r19, %r20;
	mov.u32 	%r21, %ctaid.z;
	mov.u32 	%r22, %ntid.z;
	mov.b32	%r23, %envreg5;
	mad.lo.s32 	%r24, %r21, %r22, %r23;
	mov.u32 	%r25, %tid.z;
	add.s32 	%r3, %r24, %r25;
	setp.ge.s32	%p1, %r1, %r8;
	setp.ge.s32	%p2, %r2, %r9;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r10;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB19_2;

	mad.lo.s32 	%r26, %r3, %r9, %r2;
	mad.lo.s32 	%r27, %r26, %r8, %r1;
	cvt.rn.f32.s32	%f1, %r8;
	add.f32 	%f2, %f1, 0fBF800000;
	neg.f32 	%f3, %f2;
	cvt.rn.f32.s32	%f4, %r1;
	fma.rn.f32 	%f5, %f3, 0f3F000000, %f4;
	cvt.rn.f32.s32	%f6, %r9;
	add.f32 	%f7, %f6, 0fBF800000;
	neg.f32 	%f8, %f7;
	cvt.rn.f32.s32	%f9, %r2;
	fma.rn.f32 	%f10, %f8, 0f3F000000, %f9;
	cvt.rn.f32.s32	%f11, %r10;
	add.f32 	%f12, %f11, 0fBF800000;
	neg.f32 	%f13, %f12;
	cvt.rn.f32.s32	%f14, %r3;
	fma.rn.f32 	%f15, %f13, 0f3F000000, %f14;
	ld.const.f32 	%f16, [%r7+12];
	ld.const.f32 	%f17, [%r7];
	fma.rn.f32 	%f18, %f16, %f5, %f17;
	ld.const.f32 	%f19, [%r7+16];
	fma.rn.f32 	%f20, %f19, %f10, %f18;
	ld.const.f32 	%f21, [%r7+20];
	fma.rn.f32 	%f22, %f21, %f15, %f20;
	ld.const.f32 	%f23, [%r7+24];
	ld.const.f32 	%f24, [%r7+4];
	fma.rn.f32 	%f25, %f23, %f5, %f24;
	ld.const.f32 	%f26, [%r7+28];
	fma.rn.f32 	%f27, %f26, %f10, %f25;
	ld.const.f32 	%f28, [%r7+32];
	fma.rn.f32 	%f29, %f28, %f15, %f27;
	ld.const.f32 	%f30, [%r7+36];
	ld.const.f32 	%f31, [%r7+8];
	fma.rn.f32 	%f32, %f30, %f5, %f31;
	ld.const.f32 	%f33, [%r7+40];
	fma.rn.f32 	%f34, %f33, %f10, %f32;
	ld.const.f32 	%f35, [%r7+44];
	fma.rn.f32 	%f36, %f35, %f15, %f34;
	shl.b32 	%r28, %r27, 2;
	add.s32 	%r29, %r4, %r28;
	ld.global.f32 	%f37, [%r29];
	add.f32 	%f38, %f22, %f37;
	st.global.f32 	[%r29], %f38;
	add.s32 	%r30, %r5, %r28;
	ld.global.f32 	%f39, [%r30];
	add.f32 	%f40, %f29, %f39;
	st.global.f32 	[%r30], %f40;
	add.s32 	%r31, %r6, %r28;
	ld.global.f32 	%f41, [%r31];
	add.f32 	%f42, %f36, %f41;
	st.global.f32 	[%r31], %f42;

BB19_2:
	ret;
}

	// .globl	InterpolateVolumeCubicLinear
.entry InterpolateVolumeCubicLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeCubicLinear_param_0,
	.param .texref InterpolateVolumeCubicLinear_param_1,
	.param .u32 .ptr .const .align 4 InterpolateVolumeCubicLinear_param_2,
	.param .u32 InterpolateVolumeCubicLinear_param_3,
	.param .u32 InterpolateVolumeCubicLinear_param_4,
	.param .u32 InterpolateVolumeCubicLinear_param_5,
	.param .u32 InterpolateVolumeCubicLinear_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<179>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r5, [InterpolateVolumeCubicLinear_param_0];
	ld.param.u32 	%r6, [InterpolateVolumeCubicLinear_param_2];
	ld.param.u32 	%r7, [InterpolateVolumeCubicLinear_param_3];
	ld.param.u32 	%r8, [InterpolateVolumeCubicLinear_param_4];
	ld.param.u32 	%r9, [InterpolateVolumeCubicLinear_param_5];
	ld.param.u32 	%r10, [InterpolateVolumeCubicLinear_param_6];
	mov.b32	%r11, %envreg3;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mad.lo.s32 	%r14, %r12, %r13, %r11;
	mov.u32 	%r15, %tid.x;
	add.s32 	%r1, %r14, %r15;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %ntid.y;
	mov.b32	%r18, %envreg4;
	mad.lo.s32 	%r19, %r16, %r17, %r18;
	mov.u32 	%r20, %tid.y;
	add.s32 	%r2, %r19, %r20;
	mov.u32 	%r21, %ctaid.z;
	mov.u32 	%r22, %ntid.z;
	mov.b32	%r23, %envreg5;
	mad.lo.s32 	%r24, %r21, %r22, %r23;
	mov.u32 	%r25, %tid.z;
	add.s32 	%r3, %r24, %r25;
	setp.ge.s32	%p1, %r1, %r7;
	setp.ge.s32	%p2, %r2, %r8;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r9;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB20_30;

	mad.lo.s32 	%r26, %r10, %r9, %r3;
	mad.lo.s32 	%r27, %r26, %r8, %r2;
	mad.lo.s32 	%r4, %r27, %r7, %r1;
	cvt.rn.f32.s32	%f64, %r7;
	add.f32 	%f65, %f64, 0fBF800000;
	neg.f32 	%f66, %f65;
	cvt.rn.f32.s32	%f67, %r1;
	fma.rn.f32 	%f68, %f66, 0f3F000000, %f67;
	cvt.rn.f32.s32	%f69, %r8;
	add.f32 	%f70, %f69, 0fBF800000;
	neg.f32 	%f71, %f70;
	cvt.rn.f32.s32	%f72, %r2;
	fma.rn.f32 	%f73, %f71, 0f3F000000, %f72;
	cvt.rn.f32.s32	%f74, %r9;
	add.f32 	%f75, %f74, 0fBF800000;
	neg.f32 	%f76, %f75;
	cvt.rn.f32.s32	%f77, %r3;
	fma.rn.f32 	%f78, %f76, 0f3F000000, %f77;
	ld.const.f32 	%f79, [%r6];
	add.f32 	%f80, %f67, %f79;
	ld.const.f32 	%f81, [%r6+12];
	fma.rn.f32 	%f82, %f68, %f81, %f80;
	ld.const.f32 	%f83, [%r6+16];
	fma.rn.f32 	%f84, %f73, %f83, %f82;
	ld.const.f32 	%f85, [%r6+20];
	fma.rn.f32 	%f86, %f78, %f85, %f84;
	ld.const.f32 	%f87, [%r6+4];
	add.f32 	%f88, %f72, %f87;
	ld.const.f32 	%f89, [%r6+24];
	fma.rn.f32 	%f90, %f68, %f89, %f88;
	ld.const.f32 	%f91, [%r6+28];
	fma.rn.f32 	%f92, %f73, %f91, %f90;
	ld.const.f32 	%f93, [%r6+32];
	fma.rn.f32 	%f94, %f78, %f93, %f92;
	ld.const.f32 	%f95, [%r6+8];
	add.f32 	%f96, %f77, %f95;
	ld.const.f32 	%f97, [%r6+36];
	fma.rn.f32 	%f98, %f68, %f97, %f96;
	ld.const.f32 	%f99, [%r6+40];
	fma.rn.f32 	%f100, %f73, %f99, %f98;
	ld.const.f32 	%f101, [%r6+44];
	fma.rn.f32 	%f102, %f78, %f101, %f100;
	add.f32 	%f103, %f86, 0f3F000000;
	add.f32 	%f104, %f94, 0f3F000000;
	add.f32 	%f105, %f102, 0f3F000000;
	add.f32 	%f61, %f105, 0fBF000000;
	add.f32 	%f59, %f104, 0fBF000000;
	add.f32 	%f57, %f103, 0fBF000000;
	// inline asm
	cvt.rmi.f32.f32 	%f56, %f57;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f58, %f59;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f60, %f61;
	// inline asm
	sub.f32 	%f1, %f61, %f60;
	sub.f32 	%f3, %f59, %f58;
	sub.f32 	%f106, %f57, %f56;
	add.f32 	%f2, %f60, 0f3F000000;
	add.f32 	%f4, %f58, 0f3F000000;
	add.f32 	%f107, %f56, 0f3F000000;
	mov.f32 	%f170, 0fBF800000;
	sub.f32 	%f108, %f170, %f106;
	abs.f32 	%f5, %f108;
	mov.f32 	%f109, 0f40000000;
	sub.f32 	%f6, %f109, %f5;
	mul.f32 	%f110, %f5, 0f3F000000;
	mul.f32 	%f111, %f5, %f110;
	neg.f32 	%f7, %f111;
	add.f32 	%f8, %f107, 0fBF800000;
	mov.f32 	%f112, 0f00000000;
	sub.f32 	%f113, %f112, %f106;
	abs.f32 	%f9, %f113;
	sub.f32 	%f10, %f109, %f9;
	mul.f32 	%f114, %f9, 0f3F000000;
	mul.f32 	%f115, %f9, %f114;
	neg.f32 	%f11, %f115;
	add.f32 	%f12, %f107, 0f00000000;
	mov.f32 	%f116, 0f3F800000;
	sub.f32 	%f117, %f116, %f106;
	abs.f32 	%f13, %f117;
	sub.f32 	%f14, %f109, %f13;
	mul.f32 	%f118, %f13, 0f3F000000;
	mul.f32 	%f119, %f13, %f118;
	neg.f32 	%f15, %f119;
	add.f32 	%f16, %f107, 0f3F800000;
	sub.f32 	%f120, %f109, %f106;
	abs.f32 	%f17, %f120;
	sub.f32 	%f18, %f109, %f17;
	mul.f32 	%f121, %f17, 0f3F000000;
	mul.f32 	%f122, %f17, %f121;
	neg.f32 	%f19, %f122;
	add.f32 	%f20, %f107, 0f40000000;
	mul.f32 	%f123, %f18, %f18;
	mul.f32 	%f124, %f18, %f123;
	div.full.f32 	%f21, %f124, 0f40C00000;
	mul.f32 	%f125, %f14, %f14;
	mul.f32 	%f126, %f14, %f125;
	div.full.f32 	%f22, %f126, 0f40C00000;
	mul.f32 	%f127, %f10, %f10;
	mul.f32 	%f128, %f10, %f127;
	div.full.f32 	%f23, %f128, 0f40C00000;
	mul.f32 	%f129, %f6, %f6;
	mul.f32 	%f130, %f6, %f129;
	div.full.f32 	%f24, %f130, 0f40C00000;
	fma.rn.f32 	%f42, %f7, %f6, 0f3F2AAAAB;
	fma.rn.f32 	%f45, %f11, %f10, 0f3F2AAAAB;
	fma.rn.f32 	%f48, %f15, %f14, 0f3F2AAAAB;
	fma.rn.f32 	%f51, %f19, %f18, 0f3F2AAAAB;

BB20_2:
	sub.f32 	%f131, %f170, %f1;
	abs.f32 	%f27, %f131;
	sub.f32 	%f28, %f109, %f27;
	setp.lt.f32	%p6, %f27, 0f3F800000;
	@%p6 bra 	BB20_5;
	bra.uni 	BB20_3;

BB20_5:
	mul.f32 	%f136, %f27, 0fBF000000;
	mul.f32 	%f137, %f27, %f136;
	fma.rn.f32 	%f30, %f137, %f28, 0f3F2AAAAB;
	mov.f32 	%f171, %f30;
	bra.uni 	BB20_6;

BB20_3:
	setp.geu.f32	%p7, %f27, 0f40000000;
	mov.f32 	%f171, %f112;
	@%p7 bra 	BB20_6;

	mul.f32 	%f134, %f28, %f28;
	mul.f32 	%f135, %f28, %f134;
	div.full.f32 	%f29, %f135, 0f40C00000;
	mov.f32 	%f171, %f29;

BB20_6:
	mov.f32 	%f31, %f171;
	add.f32 	%f32, %f2, %f170;
	mov.f32 	%f172, 0fBF800000;

BB20_7:
	sub.f32 	%f139, %f172, %f3;
	abs.f32 	%f35, %f139;
	sub.f32 	%f36, %f109, %f35;
	setp.lt.f32	%p8, %f35, 0f3F800000;
	@%p8 bra 	BB20_10;
	bra.uni 	BB20_8;

BB20_10:
	mul.f32 	%f144, %f35, 0fBF000000;
	mul.f32 	%f145, %f35, %f144;
	fma.rn.f32 	%f174, %f145, %f36, 0f3F2AAAAB;
	bra.uni 	BB20_11;

BB20_8:
	mov.f32 	%f174, 0f00000000;
	setp.geu.f32	%p9, %f35, 0f40000000;
	@%p9 bra 	BB20_11;

	mul.f32 	%f142, %f36, %f36;
	mul.f32 	%f143, %f36, %f142;
	div.full.f32 	%f174, %f143, 0f40C00000;

BB20_11:
	mul.f32 	%f40, %f31, %f174;
	add.f32 	%f41, %f4, %f172;
	setp.lt.f32	%p10, %f5, 0f3F800000;
	@%p10 bra 	BB20_14;
	bra.uni 	BB20_12;

BB20_14:
	mov.f32 	%f175, %f42;
	bra.uni 	BB20_15;

BB20_12:
	mov.f32 	%f146, 0f00000000;
	setp.geu.f32	%p11, %f5, 0f40000000;
	mov.f32 	%f175, %f146;
	@%p11 bra 	BB20_15;

	mov.f32 	%f175, %f24;

BB20_15:
	mov.f32 	%f43, %f175;
	tex.3d.v4.f32.f32	{%f147, %f148, %f149, %f150}, [InterpolateVolumeCubicLinear_param_1, volume_sampler_linear, {%f8, %f41, %f32, %f32}];
	mul.f32 	%f151, %f40, %f43;
	fma.rn.f32 	%f44, %f151, %f147, %f173;
	setp.lt.f32	%p12, %f9, 0f3F800000;
	@%p12 bra 	BB20_18;
	bra.uni 	BB20_16;

BB20_18:
	mov.f32 	%f176, %f45;
	bra.uni 	BB20_19;

BB20_16:
	mov.f32 	%f152, 0f00000000;
	setp.geu.f32	%p13, %f9, 0f40000000;
	mov.f32 	%f176, %f152;
	@%p13 bra 	BB20_19;

	mov.f32 	%f176, %f23;

BB20_19:
	mov.f32 	%f46, %f176;
	tex.3d.v4.f32.f32	{%f153, %f154, %f155, %f156}, [InterpolateVolumeCubicLinear_param_1, volume_sampler_linear, {%f12, %f41, %f32, %f32}];
	mul.f32 	%f157, %f40, %f46;
	fma.rn.f32 	%f47, %f157, %f153, %f44;
	setp.lt.f32	%p14, %f13, 0f3F800000;
	@%p14 bra 	BB20_22;
	bra.uni 	BB20_20;

BB20_22:
	mov.f32 	%f177, %f48;
	bra.uni 	BB20_23;

BB20_20:
	mov.f32 	%f158, 0f00000000;
	setp.geu.f32	%p15, %f13, 0f40000000;
	mov.f32 	%f177, %f158;
	@%p15 bra 	BB20_23;

	mov.f32 	%f177, %f22;

BB20_23:
	mov.f32 	%f49, %f177;
	tex.3d.v4.f32.f32	{%f159, %f160, %f161, %f162}, [InterpolateVolumeCubicLinear_param_1, volume_sampler_linear, {%f16, %f41, %f32, %f32}];
	mul.f32 	%f163, %f40, %f49;
	fma.rn.f32 	%f50, %f163, %f159, %f47;
	setp.lt.f32	%p16, %f17, 0f3F800000;
	@%p16 bra 	BB20_26;
	bra.uni 	BB20_24;

BB20_26:
	mov.f32 	%f178, %f51;
	bra.uni 	BB20_27;

BB20_24:
	mov.f32 	%f164, 0f00000000;
	setp.geu.f32	%p17, %f17, 0f40000000;
	mov.f32 	%f178, %f164;
	@%p17 bra 	BB20_27;

	mov.f32 	%f178, %f21;

BB20_27:
	mov.f32 	%f52, %f178;
	tex.3d.v4.f32.f32	{%f165, %f166, %f167, %f168}, [InterpolateVolumeCubicLinear_param_1, volume_sampler_linear, {%f20, %f41, %f32, %f32}];
	mul.f32 	%f169, %f40, %f52;
	fma.rn.f32 	%f173, %f169, %f165, %f50;
	add.f32 	%f172, %f172, 0f3F800000;
	setp.lt.f32	%p18, %f172, 0f40200000;
	@%p18 bra 	BB20_7;

	add.f32 	%f170, %f170, 0f3F800000;
	setp.lt.f32	%p19, %f170, 0f40200000;
	@%p19 bra 	BB20_2;

	shl.b32 	%r28, %r4, 2;
	add.s32 	%r29, %r5, %r28;
	st.global.f32 	[%r29], %f173;

BB20_30:
	ret;
}

	// .globl	InterpolateVolumeCubicNonLinear
.entry InterpolateVolumeCubicNonLinear(
	.param .u32 .ptr .global .align 4 InterpolateVolumeCubicNonLinear_param_0,
	.param .texref InterpolateVolumeCubicNonLinear_param_1,
	.param .u32 .ptr .global .align 4 InterpolateVolumeCubicNonLinear_param_2,
	.param .u32 InterpolateVolumeCubicNonLinear_param_3,
	.param .u32 InterpolateVolumeCubicNonLinear_param_4,
	.param .u32 InterpolateVolumeCubicNonLinear_param_5,
	.param .u32 InterpolateVolumeCubicNonLinear_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r5, [InterpolateVolumeCubicNonLinear_param_0];
	ld.param.u32 	%r6, [InterpolateVolumeCubicNonLinear_param_3];
	ld.param.u32 	%r7, [InterpolateVolumeCubicNonLinear_param_4];
	ld.param.u32 	%r8, [InterpolateVolumeCubicNonLinear_param_5];
	ld.param.u32 	%r9, [InterpolateVolumeCubicNonLinear_param_6];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %ntid.y;
	mov.b32	%r17, %envreg4;
	mad.lo.s32 	%r18, %r15, %r16, %r17;
	mov.u32 	%r19, %tid.y;
	add.s32 	%r2, %r18, %r19;
	mov.u32 	%r20, %ctaid.z;
	mov.u32 	%r21, %ntid.z;
	mov.b32	%r22, %envreg5;
	mad.lo.s32 	%r23, %r20, %r21, %r22;
	mov.u32 	%r24, %tid.z;
	add.s32 	%r3, %r23, %r24;
	setp.ge.s32	%p1, %r1, %r6;
	setp.ge.s32	%p2, %r2, %r7;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r8;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB21_30;

	mad.lo.s32 	%r25, %r9, %r8, %r3;
	mad.lo.s32 	%r26, %r25, %r7, %r2;
	mad.lo.s32 	%r4, %r26, %r6, %r1;
	// inline asm
	cvt.rmi.f32.f32 	%f56, %f64;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f58, %f65;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f60, %f66;
	// inline asm
	add.f32 	%f2, %f60, 0f3F000000;
	add.f32 	%f4, %f58, 0f3F000000;
	add.f32 	%f67, %f56, 0f3F000000;
	mov.f32 	%f137, 0fBF800000;
	sub.f32 	%f70, %f137, %f71;
	abs.f32 	%f5, %f70;
	mov.f32 	%f72, 0f40000000;
	sub.f32 	%f6, %f72, %f5;
	mul.f32 	%f73, %f5, 0f3F000000;
	mul.f32 	%f74, %f5, %f73;
	neg.f32 	%f7, %f74;
	add.f32 	%f8, %f67, 0fBF800000;
	mov.f32 	%f75, 0f00000000;
	sub.f32 	%f76, %f75, %f77;
	abs.f32 	%f9, %f76;
	sub.f32 	%f10, %f72, %f9;
	mul.f32 	%f78, %f9, 0f3F000000;
	mul.f32 	%f79, %f9, %f78;
	neg.f32 	%f11, %f79;
	add.f32 	%f12, %f67, 0f00000000;
	mov.f32 	%f80, 0f3F800000;
	sub.f32 	%f81, %f80, %f82;
	abs.f32 	%f13, %f81;
	sub.f32 	%f14, %f72, %f13;
	mul.f32 	%f83, %f13, 0f3F000000;
	mul.f32 	%f84, %f13, %f83;
	neg.f32 	%f15, %f84;
	add.f32 	%f16, %f67, 0f3F800000;
	sub.f32 	%f85, %f72, %f86;
	abs.f32 	%f17, %f85;
	sub.f32 	%f18, %f72, %f17;
	mul.f32 	%f87, %f17, 0f3F000000;
	mul.f32 	%f88, %f17, %f87;
	neg.f32 	%f19, %f88;
	add.f32 	%f20, %f67, 0f40000000;
	mul.f32 	%f89, %f18, %f18;
	mul.f32 	%f90, %f18, %f89;
	div.full.f32 	%f21, %f90, 0f40C00000;
	mul.f32 	%f91, %f14, %f14;
	mul.f32 	%f92, %f14, %f91;
	div.full.f32 	%f22, %f92, 0f40C00000;
	mul.f32 	%f93, %f10, %f10;
	mul.f32 	%f94, %f10, %f93;
	div.full.f32 	%f23, %f94, 0f40C00000;
	mul.f32 	%f95, %f6, %f6;
	mul.f32 	%f96, %f6, %f95;
	div.full.f32 	%f24, %f96, 0f40C00000;
	fma.rn.f32 	%f42, %f7, %f6, 0f3F2AAAAB;
	fma.rn.f32 	%f45, %f11, %f10, 0f3F2AAAAB;
	fma.rn.f32 	%f48, %f15, %f14, 0f3F2AAAAB;
	fma.rn.f32 	%f51, %f19, %f18, 0f3F2AAAAB;

BB21_2:
	sub.f32 	%f98, %f137, %f68;
	abs.f32 	%f27, %f98;
	sub.f32 	%f28, %f72, %f27;
	setp.lt.f32	%p6, %f27, 0f3F800000;
	@%p6 bra 	BB21_5;
	bra.uni 	BB21_3;

BB21_5:
	mul.f32 	%f103, %f27, 0fBF000000;
	mul.f32 	%f104, %f27, %f103;
	fma.rn.f32 	%f30, %f104, %f28, 0f3F2AAAAB;
	mov.f32 	%f138, %f30;
	bra.uni 	BB21_6;

BB21_3:
	setp.geu.f32	%p7, %f27, 0f40000000;
	mov.f32 	%f138, %f75;
	@%p7 bra 	BB21_6;

	mul.f32 	%f101, %f28, %f28;
	mul.f32 	%f102, %f28, %f101;
	div.full.f32 	%f29, %f102, 0f40C00000;
	mov.f32 	%f138, %f29;

BB21_6:
	mov.f32 	%f31, %f138;
	add.f32 	%f32, %f2, %f137;
	mov.f32 	%f139, 0fBF800000;

BB21_7:
	sub.f32 	%f106, %f139, %f69;
	abs.f32 	%f35, %f106;
	sub.f32 	%f36, %f72, %f35;
	setp.lt.f32	%p8, %f35, 0f3F800000;
	@%p8 bra 	BB21_10;
	bra.uni 	BB21_8;

BB21_10:
	mul.f32 	%f111, %f35, 0fBF000000;
	mul.f32 	%f112, %f35, %f111;
	fma.rn.f32 	%f141, %f112, %f36, 0f3F2AAAAB;
	bra.uni 	BB21_11;

BB21_8:
	mov.f32 	%f141, 0f00000000;
	setp.geu.f32	%p9, %f35, 0f40000000;
	@%p9 bra 	BB21_11;

	mul.f32 	%f109, %f36, %f36;
	mul.f32 	%f110, %f36, %f109;
	div.full.f32 	%f141, %f110, 0f40C00000;

BB21_11:
	mul.f32 	%f40, %f31, %f141;
	add.f32 	%f41, %f4, %f139;
	setp.lt.f32	%p10, %f5, 0f3F800000;
	@%p10 bra 	BB21_14;
	bra.uni 	BB21_12;

BB21_14:
	mov.f32 	%f142, %f42;
	bra.uni 	BB21_15;

BB21_12:
	mov.f32 	%f113, 0f00000000;
	setp.geu.f32	%p11, %f5, 0f40000000;
	mov.f32 	%f142, %f113;
	@%p11 bra 	BB21_15;

	mov.f32 	%f142, %f24;

BB21_15:
	mov.f32 	%f43, %f142;
	tex.3d.v4.f32.f32	{%f114, %f115, %f116, %f117}, [InterpolateVolumeCubicNonLinear_param_1, volume_sampler_linear, {%f8, %f41, %f32, %f32}];
	mul.f32 	%f118, %f40, %f43;
	fma.rn.f32 	%f44, %f118, %f114, %f140;
	setp.lt.f32	%p12, %f9, 0f3F800000;
	@%p12 bra 	BB21_18;
	bra.uni 	BB21_16;

BB21_18:
	mov.f32 	%f143, %f45;
	bra.uni 	BB21_19;

BB21_16:
	mov.f32 	%f119, 0f00000000;
	setp.geu.f32	%p13, %f9, 0f40000000;
	mov.f32 	%f143, %f119;
	@%p13 bra 	BB21_19;

	mov.f32 	%f143, %f23;

BB21_19:
	mov.f32 	%f46, %f143;
	tex.3d.v4.f32.f32	{%f120, %f121, %f122, %f123}, [InterpolateVolumeCubicNonLinear_param_1, volume_sampler_linear, {%f12, %f41, %f32, %f32}];
	mul.f32 	%f124, %f40, %f46;
	fma.rn.f32 	%f47, %f124, %f120, %f44;
	setp.lt.f32	%p14, %f13, 0f3F800000;
	@%p14 bra 	BB21_22;
	bra.uni 	BB21_20;

BB21_22:
	mov.f32 	%f144, %f48;
	bra.uni 	BB21_23;

BB21_20:
	mov.f32 	%f125, 0f00000000;
	setp.geu.f32	%p15, %f13, 0f40000000;
	mov.f32 	%f144, %f125;
	@%p15 bra 	BB21_23;

	mov.f32 	%f144, %f22;

BB21_23:
	mov.f32 	%f49, %f144;
	tex.3d.v4.f32.f32	{%f126, %f127, %f128, %f129}, [InterpolateVolumeCubicNonLinear_param_1, volume_sampler_linear, {%f16, %f41, %f32, %f32}];
	mul.f32 	%f130, %f40, %f49;
	fma.rn.f32 	%f50, %f130, %f126, %f47;
	setp.lt.f32	%p16, %f17, 0f3F800000;
	@%p16 bra 	BB21_26;
	bra.uni 	BB21_24;

BB21_26:
	mov.f32 	%f145, %f51;
	bra.uni 	BB21_27;

BB21_24:
	mov.f32 	%f131, 0f00000000;
	setp.geu.f32	%p17, %f17, 0f40000000;
	mov.f32 	%f145, %f131;
	@%p17 bra 	BB21_27;

	mov.f32 	%f145, %f21;

BB21_27:
	mov.f32 	%f52, %f145;
	tex.3d.v4.f32.f32	{%f132, %f133, %f134, %f135}, [InterpolateVolumeCubicNonLinear_param_1, volume_sampler_linear, {%f20, %f41, %f32, %f32}];
	mul.f32 	%f136, %f40, %f52;
	fma.rn.f32 	%f140, %f136, %f132, %f50;
	add.f32 	%f139, %f139, 0f3F800000;
	setp.lt.f32	%p18, %f139, 0f40200000;
	@%p18 bra 	BB21_7;

	add.f32 	%f137, %f137, 0f3F800000;
	setp.lt.f32	%p19, %f137, 0f40200000;
	@%p19 bra 	BB21_2;

	shl.b32 	%r27, %r4, 2;
	add.s32 	%r28, %r5, %r27;
	st.global.f32 	[%r28], %f140;

BB21_30:
	ret;
}

	// .globl	RescaleVolumeNearest
.entry RescaleVolumeNearest(
	.param .u32 .ptr .global .align 4 RescaleVolumeNearest_param_0,
	.param .texref RescaleVolumeNearest_param_1,
	.param .f32 RescaleVolumeNearest_param_2,
	.param .f32 RescaleVolumeNearest_param_3,
	.param .f32 RescaleVolumeNearest_param_4,
	.param .u32 RescaleVolumeNearest_param_5,
	.param .u32 RescaleVolumeNearest_param_6,
	.param .u32 RescaleVolumeNearest_param_7
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [RescaleVolumeNearest_param_0];
	ld.param.f32 	%f1, [RescaleVolumeNearest_param_2];
	ld.param.f32 	%f2, [RescaleVolumeNearest_param_3];
	ld.param.f32 	%f3, [RescaleVolumeNearest_param_4];
	ld.param.u32 	%r5, [RescaleVolumeNearest_param_5];
	ld.param.u32 	%r6, [RescaleVolumeNearest_param_6];
	ld.param.u32 	%r7, [RescaleVolumeNearest_param_7];
	mov.b32	%r8, %envreg3;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mad.lo.s32 	%r11, %r9, %r10, %r8;
	mov.u32 	%r12, %tid.x;
	add.s32 	%r1, %r11, %r12;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %ntid.y;
	mov.b32	%r15, %envreg4;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %tid.y;
	add.s32 	%r2, %r16, %r17;
	mov.u32 	%r18, %ctaid.z;
	mov.u32 	%r19, %ntid.z;
	mov.b32	%r20, %envreg5;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %tid.z;
	add.s32 	%r3, %r21, %r22;
	setp.ge.s32	%p1, %r1, %r5;
	setp.ge.s32	%p2, %r2, %r6;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r7;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB22_2;

	cvt.rn.f32.s32	%f4, %r1;
	fma.rn.f32 	%f5, %f4, %f1, 0f3F000000;
	cvt.rn.f32.s32	%f6, %r2;
	fma.rn.f32 	%f7, %f6, %f2, 0f3F000000;
	cvt.rn.f32.s32	%f8, %r3;
	fma.rn.f32 	%f9, %f8, %f3, 0f3F000000;
	tex.3d.v4.f32.f32	{%f10, %f11, %f12, %f13}, [RescaleVolumeNearest_param_1, volume_sampler_nearest, {%f5, %f7, %f9, %f9}];
	mad.lo.s32 	%r23, %r3, %r6, %r2;
	mad.lo.s32 	%r24, %r23, %r5, %r1;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r26, %r4, %r25;
	st.global.f32 	[%r26], %f10;

BB22_2:
	ret;
}

	// .globl	RescaleVolumeLinear
.entry RescaleVolumeLinear(
	.param .u32 .ptr .global .align 4 RescaleVolumeLinear_param_0,
	.param .texref RescaleVolumeLinear_param_1,
	.param .f32 RescaleVolumeLinear_param_2,
	.param .f32 RescaleVolumeLinear_param_3,
	.param .f32 RescaleVolumeLinear_param_4,
	.param .u32 RescaleVolumeLinear_param_5,
	.param .u32 RescaleVolumeLinear_param_6,
	.param .u32 RescaleVolumeLinear_param_7
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r4, [RescaleVolumeLinear_param_0];
	ld.param.f32 	%f1, [RescaleVolumeLinear_param_2];
	ld.param.f32 	%f2, [RescaleVolumeLinear_param_3];
	ld.param.f32 	%f3, [RescaleVolumeLinear_param_4];
	ld.param.u32 	%r5, [RescaleVolumeLinear_param_5];
	ld.param.u32 	%r6, [RescaleVolumeLinear_param_6];
	ld.param.u32 	%r7, [RescaleVolumeLinear_param_7];
	mov.b32	%r8, %envreg3;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %ntid.x;
	mad.lo.s32 	%r11, %r9, %r10, %r8;
	mov.u32 	%r12, %tid.x;
	add.s32 	%r1, %r11, %r12;
	mov.u32 	%r13, %ctaid.y;
	mov.u32 	%r14, %ntid.y;
	mov.b32	%r15, %envreg4;
	mad.lo.s32 	%r16, %r13, %r14, %r15;
	mov.u32 	%r17, %tid.y;
	add.s32 	%r2, %r16, %r17;
	mov.u32 	%r18, %ctaid.z;
	mov.u32 	%r19, %ntid.z;
	mov.b32	%r20, %envreg5;
	mad.lo.s32 	%r21, %r18, %r19, %r20;
	mov.u32 	%r22, %tid.z;
	add.s32 	%r3, %r21, %r22;
	setp.ge.s32	%p1, %r1, %r5;
	setp.ge.s32	%p2, %r2, %r6;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r7;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB23_2;

	cvt.rn.f32.s32	%f4, %r1;
	fma.rn.f32 	%f5, %f4, %f1, 0f3F000000;
	cvt.rn.f32.s32	%f6, %r2;
	fma.rn.f32 	%f7, %f6, %f2, 0f3F000000;
	cvt.rn.f32.s32	%f8, %r3;
	fma.rn.f32 	%f9, %f8, %f3, 0f3F000000;
	tex.3d.v4.f32.f32	{%f10, %f11, %f12, %f13}, [RescaleVolumeLinear_param_1, volume_sampler_linear, {%f5, %f7, %f9, %f9}];
	mad.lo.s32 	%r23, %r3, %r6, %r2;
	mad.lo.s32 	%r24, %r23, %r5, %r1;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r26, %r4, %r25;
	st.global.f32 	[%r26], %f10;

BB23_2:
	ret;
}

	// .globl	RescaleVolumeCubic
.entry RescaleVolumeCubic(
	.param .u32 .ptr .global .align 4 RescaleVolumeCubic_param_0,
	.param .texref RescaleVolumeCubic_param_1,
	.param .f32 RescaleVolumeCubic_param_2,
	.param .f32 RescaleVolumeCubic_param_3,
	.param .f32 RescaleVolumeCubic_param_4,
	.param .u32 RescaleVolumeCubic_param_5,
	.param .u32 RescaleVolumeCubic_param_6,
	.param .u32 RescaleVolumeCubic_param_7
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<3>;


	ld.param.u32 	%r5, [RescaleVolumeCubic_param_0];
	ld.param.f32 	%f56, [RescaleVolumeCubic_param_2];
	ld.param.f32 	%f57, [RescaleVolumeCubic_param_3];
	ld.param.f32 	%f58, [RescaleVolumeCubic_param_4];
	ld.param.u32 	%r6, [RescaleVolumeCubic_param_5];
	ld.param.u32 	%r7, [RescaleVolumeCubic_param_6];
	ld.param.u32 	%r8, [RescaleVolumeCubic_param_7];
	mov.b32	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r12, %r10, %r11, %r9;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %ntid.y;
	mov.b32	%r16, %envreg4;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %tid.y;
	add.s32 	%r2, %r17, %r18;
	mov.u32 	%r19, %ctaid.z;
	mov.u32 	%r20, %ntid.z;
	mov.b32	%r21, %envreg5;
	mad.lo.s32 	%r22, %r19, %r20, %r21;
	mov.u32 	%r23, %tid.z;
	add.s32 	%r3, %r22, %r23;
	setp.ge.s32	%p1, %r1, %r6;
	setp.ge.s32	%p2, %r2, %r7;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32	%p4, %r3, %r8;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB24_30;

	mad.lo.s32 	%r24, %r3, %r7, %r2;
	mad.lo.s32 	%r4, %r24, %r6, %r1;
	cvt.rn.f32.s32	%f67, %r1;
	cvt.rn.f32.s32	%f68, %r2;
	cvt.rn.f32.s32	%f69, %r3;
	fma.rn.f32 	%f70, %f67, %f56, 0f3F000000;
	fma.rn.f32 	%f71, %f68, %f57, 0f3F000000;
	fma.rn.f32 	%f72, %f69, %f58, 0f3F000000;
	add.f32 	%f64, %f72, 0fBF000000;
	add.f32 	%f62, %f71, 0fBF000000;
	add.f32 	%f60, %f70, 0fBF000000;
	// inline asm
	cvt.rmi.f32.f32 	%f59, %f60;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f61, %f62;
	// inline asm
	// inline asm
	cvt.rmi.f32.f32 	%f63, %f64;
	// inline asm
	sub.f32 	%f1, %f64, %f63;
	sub.f32 	%f3, %f62, %f61;
	sub.f32 	%f73, %f60, %f59;
	add.f32 	%f2, %f63, 0f3F000000;
	add.f32 	%f4, %f61, 0f3F000000;
	add.f32 	%f74, %f59, 0f3F000000;
	mov.f32 	%f137, 0fBF800000;
	sub.f32 	%f75, %f137, %f73;
	abs.f32 	%f5, %f75;
	mov.f32 	%f76, 0f40000000;
	sub.f32 	%f6, %f76, %f5;
	mul.f32 	%f77, %f5, 0f3F000000;
	mul.f32 	%f78, %f5, %f77;
	neg.f32 	%f7, %f78;
	add.f32 	%f8, %f74, 0fBF800000;
	mov.f32 	%f79, 0f00000000;
	sub.f32 	%f80, %f79, %f73;
	abs.f32 	%f9, %f80;
	sub.f32 	%f10, %f76, %f9;
	mul.f32 	%f81, %f9, 0f3F000000;
	mul.f32 	%f82, %f9, %f81;
	neg.f32 	%f11, %f82;
	add.f32 	%f12, %f74, 0f00000000;
	mov.f32 	%f83, 0f3F800000;
	sub.f32 	%f84, %f83, %f73;
	abs.f32 	%f13, %f84;
	sub.f32 	%f14, %f76, %f13;
	mul.f32 	%f85, %f13, 0f3F000000;
	mul.f32 	%f86, %f13, %f85;
	neg.f32 	%f15, %f86;
	add.f32 	%f16, %f74, 0f3F800000;
	sub.f32 	%f87, %f76, %f73;
	abs.f32 	%f17, %f87;
	sub.f32 	%f18, %f76, %f17;
	mul.f32 	%f88, %f17, 0f3F000000;
	mul.f32 	%f89, %f17, %f88;
	neg.f32 	%f19, %f89;
	add.f32 	%f20, %f74, 0f40000000;
	mul.f32 	%f90, %f18, %f18;
	mul.f32 	%f91, %f18, %f90;
	div.full.f32 	%f21, %f91, 0f40C00000;
	mul.f32 	%f92, %f14, %f14;
	mul.f32 	%f93, %f14, %f92;
	div.full.f32 	%f22, %f93, 0f40C00000;
	mul.f32 	%f94, %f10, %f10;
	mul.f32 	%f95, %f10, %f94;
	div.full.f32 	%f23, %f95, 0f40C00000;
	mul.f32 	%f96, %f6, %f6;
	mul.f32 	%f97, %f6, %f96;
	div.full.f32 	%f24, %f97, 0f40C00000;
	fma.rn.f32 	%f42, %f7, %f6, 0f3F2AAAAB;
	fma.rn.f32 	%f45, %f11, %f10, 0f3F2AAAAB;
	fma.rn.f32 	%f48, %f15, %f14, 0f3F2AAAAB;
	fma.rn.f32 	%f51, %f19, %f18, 0f3F2AAAAB;

BB24_2:
	sub.f32 	%f98, %f137, %f1;
	abs.f32 	%f27, %f98;
	sub.f32 	%f28, %f76, %f27;
	setp.lt.f32	%p6, %f27, 0f3F800000;
	@%p6 bra 	BB24_5;
	bra.uni 	BB24_3;

BB24_5:
	mul.f32 	%f103, %f27, 0fBF000000;
	mul.f32 	%f104, %f27, %f103;
	fma.rn.f32 	%f30, %f104, %f28, 0f3F2AAAAB;
	mov.f32 	%f138, %f30;
	bra.uni 	BB24_6;

BB24_3:
	setp.geu.f32	%p7, %f27, 0f40000000;
	mov.f32 	%f138, %f79;
	@%p7 bra 	BB24_6;

	mul.f32 	%f101, %f28, %f28;
	mul.f32 	%f102, %f28, %f101;
	div.full.f32 	%f29, %f102, 0f40C00000;
	mov.f32 	%f138, %f29;

BB24_6:
	mov.f32 	%f31, %f138;
	add.f32 	%f32, %f2, %f137;
	mov.f32 	%f139, 0fBF800000;

BB24_7:
	sub.f32 	%f106, %f139, %f3;
	abs.f32 	%f35, %f106;
	sub.f32 	%f36, %f76, %f35;
	setp.lt.f32	%p8, %f35, 0f3F800000;
	@%p8 bra 	BB24_10;
	bra.uni 	BB24_8;

BB24_10:
	mul.f32 	%f111, %f35, 0fBF000000;
	mul.f32 	%f112, %f35, %f111;
	fma.rn.f32 	%f141, %f112, %f36, 0f3F2AAAAB;
	bra.uni 	BB24_11;

BB24_8:
	mov.f32 	%f141, 0f00000000;
	setp.geu.f32	%p9, %f35, 0f40000000;
	@%p9 bra 	BB24_11;

	mul.f32 	%f109, %f36, %f36;
	mul.f32 	%f110, %f36, %f109;
	div.full.f32 	%f141, %f110, 0f40C00000;

BB24_11:
	mul.f32 	%f40, %f31, %f141;
	add.f32 	%f41, %f4, %f139;
	setp.lt.f32	%p10, %f5, 0f3F800000;
	@%p10 bra 	BB24_14;
	bra.uni 	BB24_12;

BB24_14:
	mov.f32 	%f142, %f42;
	bra.uni 	BB24_15;

BB24_12:
	mov.f32 	%f113, 0f00000000;
	setp.geu.f32	%p11, %f5, 0f40000000;
	mov.f32 	%f142, %f113;
	@%p11 bra 	BB24_15;

	mov.f32 	%f142, %f24;

BB24_15:
	mov.f32 	%f43, %f142;
	tex.3d.v4.f32.f32	{%f114, %f115, %f116, %f117}, [RescaleVolumeCubic_param_1, volume_sampler_linear, {%f8, %f41, %f32, %f32}];
	mul.f32 	%f118, %f40, %f43;
	fma.rn.f32 	%f44, %f118, %f114, %f140;
	setp.lt.f32	%p12, %f9, 0f3F800000;
	@%p12 bra 	BB24_18;
	bra.uni 	BB24_16;

BB24_18:
	mov.f32 	%f143, %f45;
	bra.uni 	BB24_19;

BB24_16:
	mov.f32 	%f119, 0f00000000;
	setp.geu.f32	%p13, %f9, 0f40000000;
	mov.f32 	%f143, %f119;
	@%p13 bra 	BB24_19;

	mov.f32 	%f143, %f23;

BB24_19:
	mov.f32 	%f46, %f143;
	tex.3d.v4.f32.f32	{%f120, %f121, %f122, %f123}, [RescaleVolumeCubic_param_1, volume_sampler_linear, {%f12, %f41, %f32, %f32}];
	mul.f32 	%f124, %f40, %f46;
	fma.rn.f32 	%f47, %f124, %f120, %f44;
	setp.lt.f32	%p14, %f13, 0f3F800000;
	@%p14 bra 	BB24_22;
	bra.uni 	BB24_20;

BB24_22:
	mov.f32 	%f144, %f48;
	bra.uni 	BB24_23;

BB24_20:
	mov.f32 	%f125, 0f00000000;
	setp.geu.f32	%p15, %f13, 0f40000000;
	mov.f32 	%f144, %f125;
	@%p15 bra 	BB24_23;

	mov.f32 	%f144, %f22;

BB24_23:
	mov.f32 	%f49, %f144;
	tex.3d.v4.f32.f32	{%f126, %f127, %f128, %f129}, [RescaleVolumeCubic_param_1, volume_sampler_linear, {%f16, %f41, %f32, %f32}];
	mul.f32 	%f130, %f40, %f49;
	fma.rn.f32 	%f50, %f130, %f126, %f47;
	setp.lt.f32	%p16, %f17, 0f3F800000;
	@%p16 bra 	BB24_26;
	bra.uni 	BB24_24;

BB24_26:
	mov.f32 	%f145, %f51;
	bra.uni 	BB24_27;

BB24_24:
	mov.f32 	%f131, 0f00000000;
	setp.geu.f32	%p17, %f17, 0f40000000;
	mov.f32 	%f145, %f131;
	@%p17 bra 	BB24_27;

	mov.f32 	%f145, %f21;

BB24_27:
	mov.f32 	%f52, %f145;
	tex.3d.v4.f32.f32	{%f132, %f133, %f134, %f135}, [RescaleVolumeCubic_param_1, volume_sampler_linear, {%f20, %f41, %f32, %f32}];
	mul.f32 	%f136, %f40, %f52;
	fma.rn.f32 	%f140, %f136, %f132, %f50;
	add.f32 	%f139, %f139, 0f3F800000;
	setp.lt.f32	%p18, %f139, 0f40200000;
	@%p18 bra 	BB24_7;

	add.f32 	%f137, %f137, 0f3F800000;
	setp.lt.f32	%p19, %f137, 0f40200000;
	@%p19 bra 	BB24_2;

	shl.b32 	%r25, %r4, 2;
	add.s32 	%r26, %r5, %r25;
	st.global.f32 	[%r26], %f140;

BB24_30:
	ret;
}

	// .globl	CopyT1VolumeToMNI
.entry CopyT1VolumeToMNI(
	.param .u32 .ptr .global .align 4 CopyT1VolumeToMNI_param_0,
	.param .u32 .ptr .global .align 4 CopyT1VolumeToMNI_param_1,
	.param .u32 CopyT1VolumeToMNI_param_2,
	.param .u32 CopyT1VolumeToMNI_param_3,
	.param .u32 CopyT1VolumeToMNI_param_4,
	.param .u32 CopyT1VolumeToMNI_param_5,
	.param .u32 CopyT1VolumeToMNI_param_6,
	.param .u32 CopyT1VolumeToMNI_param_7,
	.param .u32 CopyT1VolumeToMNI_param_8,
	.param .u32 CopyT1VolumeToMNI_param_9,
	.param .u32 CopyT1VolumeToMNI_param_10,
	.param .u32 CopyT1VolumeToMNI_param_11,
	.param .f32 CopyT1VolumeToMNI_param_12
)
{
	.reg .pred 	%p<28>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<106>;
	.reg .f64 	%fd<46>;


	ld.param.u32 	%r17, [CopyT1VolumeToMNI_param_0];
	ld.param.u32 	%r18, [CopyT1VolumeToMNI_param_1];
	ld.param.u32 	%r19, [CopyT1VolumeToMNI_param_2];
	ld.param.u32 	%r20, [CopyT1VolumeToMNI_param_3];
	ld.param.u32 	%r21, [CopyT1VolumeToMNI_param_4];
	ld.param.u32 	%r22, [CopyT1VolumeToMNI_param_5];
	ld.param.u32 	%r23, [CopyT1VolumeToMNI_param_6];
	ld.param.u32 	%r24, [CopyT1VolumeToMNI_param_7];
	ld.param.u32 	%r25, [CopyT1VolumeToMNI_param_8];
	ld.param.u32 	%r26, [CopyT1VolumeToMNI_param_9];
	ld.param.u32 	%r27, [CopyT1VolumeToMNI_param_10];
	ld.param.u32 	%r28, [CopyT1VolumeToMNI_param_11];
	ld.param.f32 	%f5, [CopyT1VolumeToMNI_param_12];
	mov.b32	%r29, %envreg3;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mad.lo.s32 	%r32, %r30, %r31, %r29;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r1, %r32, %r33;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.b32	%r36, %envreg4;
	mad.lo.s32 	%r37, %r34, %r35, %r36;
	mov.u32 	%r38, %tid.y;
	add.s32 	%r2, %r37, %r38;
	mov.u32 	%r39, %ctaid.z;
	mov.u32 	%r40, %ntid.z;
	mov.b32	%r41, %envreg5;
	mad.lo.s32 	%r42, %r39, %r40, %r41;
	mov.u32 	%r43, %tid.z;
	add.s32 	%r3, %r42, %r43;
	setp.gt.s32	%p1, %r25, 0;
	@%p1 bra 	BB25_4;
	bra.uni 	BB25_1;

BB25_4:
	cvt.rn.f32.s32	%f7, %r25;
	cvt.f64.f32	%fd25, %f7;
	mul.f64 	%fd42, %fd25, 0d3FE0000000000000;
	abs.f64 	%fd6, %fd42;
	setp.ge.f64	%p4, %fd6, 0d4330000000000000;
	@%p4 bra 	BB25_6;

	add.f64 	%fd26, %fd6, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd27, %fd26;
	setp.lt.f64	%p5, %fd6, 0d3FE0000000000000;
	selp.f64	%fd28, 0d0000000000000000, %fd27, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd42;
	}
	and.b32  	%r55, %r54, -2147483648;
	or.b32  	%r56, %r53, %r55;
	mov.b64 	%fd42, {%r52, %r56};

BB25_6:
	cvt.rzi.s32.f64	%r57, %fd42;
	add.s32 	%r5, %r57, %r1;
	mov.u32 	%r99, %r1;
	mov.u32 	%r98, %r5;
	bra.uni 	BB25_7;

BB25_1:
	// inline asm
	abs.s32 	%r44, %r25;
	// inline asm
	cvt.rn.f32.u32	%f6, %r44;
	cvt.f64.f32	%fd21, %f6;
	mul.f64 	%fd41, %fd21, 0d3FE0000000000000;
	abs.f64 	%fd2, %fd41;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB25_3;

	add.f64 	%fd22, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd23, %fd22;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd24, 0d0000000000000000, %fd23, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd24;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd24;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd41;
	}
	and.b32  	%r49, %r48, -2147483648;
	or.b32  	%r50, %r47, %r49;
	mov.b64 	%fd41, {%r46, %r50};

BB25_3:
	cvt.rzi.s32.f64	%r51, %fd41;
	add.s32 	%r99, %r51, %r1;
	mov.u32 	%r98, %r1;

BB25_7:
	setp.gt.s32	%p6, %r26, 0;
	@%p6 bra 	BB25_11;
	bra.uni 	BB25_8;

BB25_11:
	cvt.rn.f32.s32	%f9, %r26;
	cvt.f64.f32	%fd33, %f9;
	mul.f64 	%fd44, %fd33, 0d3FE0000000000000;
	abs.f64 	%fd14, %fd44;
	setp.ge.f64	%p9, %fd14, 0d4330000000000000;
	@%p9 bra 	BB25_13;

	add.f64 	%fd34, %fd14, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd35, %fd34;
	setp.lt.f64	%p10, %fd14, 0d3FE0000000000000;
	selp.f64	%fd36, 0d0000000000000000, %fd35, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd44;
	}
	and.b32  	%r69, %r68, -2147483648;
	or.b32  	%r70, %r67, %r69;
	mov.b64 	%fd44, {%r66, %r70};

BB25_13:
	cvt.rzi.s32.f64	%r71, %fd44;
	add.s32 	%r9, %r71, %r2;
	mov.u32 	%r102, %r2;
	mov.u32 	%r101, %r9;
	bra.uni 	BB25_14;

BB25_8:
	// inline asm
	abs.s32 	%r58, %r26;
	// inline asm
	cvt.rn.f32.u32	%f8, %r58;
	cvt.f64.f32	%fd29, %f8;
	mul.f64 	%fd43, %fd29, 0d3FE0000000000000;
	abs.f64 	%fd10, %fd43;
	setp.ge.f64	%p7, %fd10, 0d4330000000000000;
	@%p7 bra 	BB25_10;

	add.f64 	%fd30, %fd10, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd31, %fd30;
	setp.lt.f64	%p8, %fd10, 0d3FE0000000000000;
	selp.f64	%fd32, 0d0000000000000000, %fd31, %p8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r60, %temp}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd43;
	}
	and.b32  	%r63, %r62, -2147483648;
	or.b32  	%r64, %r61, %r63;
	mov.b64 	%fd43, {%r60, %r64};

BB25_10:
	cvt.rzi.s32.f64	%r65, %fd43;
	add.s32 	%r102, %r65, %r2;
	mov.u32 	%r101, %r2;

BB25_14:
	cvt.rn.f32.s32	%f10, %r28;
	div.full.f32 	%f1, %f10, %f5;
	abs.f32 	%f11, %f1;
	mov.b32 	 %r72, %f1;
	and.b32  	%r73, %r72, -2147483648;
	or.b32  	%r74, %r73, 1056964608;
	mov.b32 	 %f12, %r74;
	add.f32 	%f13, %f1, %f12;
	cvt.rzi.f32.f32	%f14, %f13;
	setp.gt.f32	%p11, %f11, 0f4B000000;
	selp.f32	%f17, %f1, %f14, %p11;
	setp.geu.f32	%p12, %f11, 0f3F000000;
	@%p12 bra 	BB25_16;

	cvt.rzi.f32.f32	%f17, %f1;

BB25_16:
	setp.gt.s32	%p13, %r27, 0;
	@%p13 bra 	BB25_20;
	bra.uni 	BB25_17;

BB25_20:
	add.s32 	%r13, %r3, %r27;
	mov.u32 	%r104, %r3;
	mov.u32 	%r105, %r13;
	bra.uni 	BB25_21;

BB25_17:
	// inline asm
	abs.s32 	%r75, %r27;
	// inline asm
	cvt.rn.f32.u32	%f15, %r75;
	cvt.f64.f32	%fd37, %f15;
	mul.f64 	%fd45, %fd37, 0d3FE0000000000000;
	abs.f64 	%fd18, %fd45;
	setp.ge.f64	%p14, %fd18, 0d4330000000000000;
	@%p14 bra 	BB25_19;

	add.f64 	%fd38, %fd18, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd39, %fd38;
	setp.lt.f64	%p15, %fd18, 0d3FE0000000000000;
	selp.f64	%fd40, 0d0000000000000000, %fd39, %p15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r77, %temp}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd45;
	}
	and.b32  	%r80, %r79, -2147483648;
	or.b32  	%r81, %r78, %r80;
	mov.b64 	%fd45, {%r77, %r81};

BB25_19:
	cvt.rzi.s32.f64	%r82, %fd45;
	add.s32 	%r104, %r82, %r3;
	mov.u32 	%r105, %r3;

BB25_21:
	cvt.rzi.s32.f32	%r83, %f17;
	setp.ge.s32	%p16, %r101, %r23;
	setp.ge.s32	%p17, %r98, %r22;
	or.pred  	%p18, %p17, %p16;
	add.s32 	%r16, %r105, %r83;
	setp.ge.s32	%p19, %r16, %r24;
	or.pred  	%p20, %p18, %p19;
	setp.ge.s32	%p21, %r99, %r19;
	or.pred  	%p22, %p20, %p21;
	setp.ge.s32	%p23, %r102, %r20;
	or.pred  	%p24, %p22, %p23;
	setp.ge.s32	%p25, %r104, %r21;
	or.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB25_24;

	or.b32  	%r84, %r98, %r99;
	or.b32  	%r85, %r84, %r102;
	or.b32  	%r86, %r85, %r101;
	or.b32  	%r87, %r86, %r16;
	or.b32  	%r88, %r87, %r104;
	setp.lt.s32	%p27, %r88, 0;
	@%p27 bra 	BB25_24;

	mad.lo.s32 	%r89, %r104, %r20, %r102;
	mad.lo.s32 	%r90, %r89, %r19, %r99;
	mad.lo.s32 	%r91, %r16, %r23, %r101;
	mad.lo.s32 	%r92, %r91, %r22, %r98;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r94, %r18, %r93;
	ld.global.f32 	%f16, [%r94];
	shl.b32 	%r95, %r90, 2;
	add.s32 	%r96, %r17, %r95;
	st.global.f32 	[%r96], %f16;

BB25_24:
	ret;
}

	// .globl	CopyEPIVolumeToT1
.entry CopyEPIVolumeToT1(
	.param .u32 .ptr .global .align 4 CopyEPIVolumeToT1_param_0,
	.param .u32 .ptr .global .align 4 CopyEPIVolumeToT1_param_1,
	.param .u32 CopyEPIVolumeToT1_param_2,
	.param .u32 CopyEPIVolumeToT1_param_3,
	.param .u32 CopyEPIVolumeToT1_param_4,
	.param .u32 CopyEPIVolumeToT1_param_5,
	.param .u32 CopyEPIVolumeToT1_param_6,
	.param .u32 CopyEPIVolumeToT1_param_7,
	.param .u32 CopyEPIVolumeToT1_param_8,
	.param .u32 CopyEPIVolumeToT1_param_9,
	.param .u32 CopyEPIVolumeToT1_param_10,
	.param .u32 CopyEPIVolumeToT1_param_11,
	.param .f32 CopyEPIVolumeToT1_param_12
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<112>;
	.reg .f64 	%fd<55>;


	ld.param.u32 	%r17, [CopyEPIVolumeToT1_param_0];
	ld.param.u32 	%r18, [CopyEPIVolumeToT1_param_1];
	ld.param.u32 	%r19, [CopyEPIVolumeToT1_param_2];
	ld.param.u32 	%r20, [CopyEPIVolumeToT1_param_3];
	ld.param.u32 	%r21, [CopyEPIVolumeToT1_param_4];
	ld.param.u32 	%r22, [CopyEPIVolumeToT1_param_5];
	ld.param.u32 	%r23, [CopyEPIVolumeToT1_param_6];
	ld.param.u32 	%r24, [CopyEPIVolumeToT1_param_7];
	ld.param.u32 	%r25, [CopyEPIVolumeToT1_param_8];
	ld.param.u32 	%r26, [CopyEPIVolumeToT1_param_9];
	ld.param.u32 	%r27, [CopyEPIVolumeToT1_param_10];
	ld.param.u32 	%r28, [CopyEPIVolumeToT1_param_11];
	ld.param.f32 	%f5, [CopyEPIVolumeToT1_param_12];
	mov.b32	%r29, %envreg3;
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mad.lo.s32 	%r32, %r30, %r31, %r29;
	mov.u32 	%r33, %tid.x;
	add.s32 	%r1, %r32, %r33;
	mov.u32 	%r34, %ctaid.y;
	mov.u32 	%r35, %ntid.y;
	mov.b32	%r36, %envreg4;
	mad.lo.s32 	%r37, %r34, %r35, %r36;
	mov.u32 	%r38, %tid.y;
	add.s32 	%r2, %r37, %r38;
	mov.u32 	%r39, %ctaid.z;
	mov.u32 	%r40, %ntid.z;
	mov.b32	%r41, %envreg5;
	mad.lo.s32 	%r42, %r39, %r40, %r41;
	mov.u32 	%r43, %tid.z;
	add.s32 	%r3, %r42, %r43;
	setp.gt.s32	%p1, %r25, 0;
	@%p1 bra 	BB26_4;
	bra.uni 	BB26_1;

BB26_4:
	cvt.rn.f32.s32	%f7, %r25;
	cvt.f64.f32	%fd29, %f7;
	mul.f64 	%fd50, %fd29, 0d3FE0000000000000;
	abs.f64 	%fd6, %fd50;
	setp.ge.f64	%p4, %fd6, 0d4330000000000000;
	@%p4 bra 	BB26_6;

	add.f64 	%fd30, %fd6, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd31, %fd30;
	setp.lt.f64	%p5, %fd6, 0d3FE0000000000000;
	selp.f64	%fd32, 0d0000000000000000, %fd31, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd50;
	}
	and.b32  	%r55, %r54, -2147483648;
	or.b32  	%r56, %r53, %r55;
	mov.b64 	%fd50, {%r52, %r56};

BB26_6:
	cvt.rzi.s32.f64	%r57, %fd50;
	add.s32 	%r5, %r57, %r1;
	mov.u32 	%r105, %r1;
	mov.u32 	%r104, %r5;
	bra.uni 	BB26_7;

BB26_1:
	// inline asm
	abs.s32 	%r44, %r25;
	// inline asm
	cvt.rn.f32.u32	%f6, %r44;
	cvt.f64.f32	%fd25, %f6;
	mul.f64 	%fd49, %fd25, 0d3FE0000000000000;
	abs.f64 	%fd2, %fd49;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB26_3;

	add.f64 	%fd26, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd27, %fd26;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd28, 0d0000000000000000, %fd27, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd49;
	}
	and.b32  	%r49, %r48, -2147483648;
	or.b32  	%r50, %r47, %r49;
	mov.b64 	%fd49, {%r46, %r50};

BB26_3:
	cvt.rzi.s32.f64	%r51, %fd49;
	add.s32 	%r105, %r51, %r1;
	mov.u32 	%r104, %r1;

BB26_7:
	setp.gt.s32	%p6, %r26, 0;
	@%p6 bra 	BB26_11;
	bra.uni 	BB26_8;

BB26_11:
	cvt.rn.f32.s32	%f9, %r26;
	cvt.f64.f32	%fd37, %f9;
	mul.f64 	%fd52, %fd37, 0d3FE0000000000000;
	abs.f64 	%fd14, %fd52;
	setp.ge.f64	%p9, %fd14, 0d4330000000000000;
	@%p9 bra 	BB26_13;

	add.f64 	%fd38, %fd14, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd39, %fd38;
	setp.lt.f64	%p10, %fd14, 0d3FE0000000000000;
	selp.f64	%fd40, 0d0000000000000000, %fd39, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd52;
	}
	and.b32  	%r69, %r68, -2147483648;
	or.b32  	%r70, %r67, %r69;
	mov.b64 	%fd52, {%r66, %r70};

BB26_13:
	cvt.rzi.s32.f64	%r71, %fd52;
	add.s32 	%r9, %r71, %r2;
	mov.u32 	%r108, %r2;
	mov.u32 	%r107, %r9;
	bra.uni 	BB26_14;

BB26_8:
	// inline asm
	abs.s32 	%r58, %r26;
	// inline asm
	cvt.rn.f32.u32	%f8, %r58;
	cvt.f64.f32	%fd33, %f8;
	mul.f64 	%fd51, %fd33, 0d3FE0000000000000;
	abs.f64 	%fd10, %fd51;
	setp.ge.f64	%p7, %fd10, 0d4330000000000000;
	@%p7 bra 	BB26_10;

	add.f64 	%fd34, %fd10, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd35, %fd34;
	setp.lt.f64	%p8, %fd10, 0d3FE0000000000000;
	selp.f64	%fd36, 0d0000000000000000, %fd35, %p8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r60, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd51;
	}
	and.b32  	%r63, %r62, -2147483648;
	or.b32  	%r64, %r61, %r63;
	mov.b64 	%fd51, {%r60, %r64};

BB26_10:
	cvt.rzi.s32.f64	%r65, %fd51;
	add.s32 	%r108, %r65, %r2;
	mov.u32 	%r107, %r2;

BB26_14:
	cvt.rn.f32.s32	%f10, %r28;
	div.full.f32 	%f1, %f10, %f5;
	abs.f32 	%f11, %f1;
	mov.b32 	 %r72, %f1;
	and.b32  	%r73, %r72, -2147483648;
	or.b32  	%r74, %r73, 1056964608;
	mov.b32 	 %f12, %r74;
	add.f32 	%f13, %f1, %f12;
	cvt.rzi.f32.f32	%f14, %f13;
	setp.gt.f32	%p11, %f11, 0f4B000000;
	selp.f32	%f18, %f1, %f14, %p11;
	setp.geu.f32	%p12, %f11, 0f3F000000;
	@%p12 bra 	BB26_16;

	cvt.rzi.f32.f32	%f18, %f1;

BB26_16:
	setp.gt.s32	%p13, %r27, 0;
	@%p13 bra 	BB26_20;
	bra.uni 	BB26_17;

BB26_20:
	cvt.rn.f32.s32	%f16, %r27;
	cvt.f64.f32	%fd45, %f16;
	mul.f64 	%fd54, %fd45, 0d3FE0000000000000;
	abs.f64 	%fd22, %fd54;
	setp.ge.f64	%p16, %fd22, 0d4330000000000000;
	@%p16 bra 	BB26_22;

	add.f64 	%fd46, %fd22, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd47, %fd46;
	setp.lt.f64	%p17, %fd22, 0d3FE0000000000000;
	selp.f64	%fd48, 0d0000000000000000, %fd47, %p17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r83, %temp}, %fd48;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r84}, %fd48;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd54;
	}
	and.b32  	%r86, %r85, -2147483648;
	or.b32  	%r87, %r84, %r86;
	mov.b64 	%fd54, {%r83, %r87};

BB26_22:
	cvt.rzi.s32.f64	%r88, %fd54;
	add.s32 	%r13, %r88, %r3;
	mov.u32 	%r110, %r3;
	mov.u32 	%r111, %r13;
	bra.uni 	BB26_23;

BB26_17:
	// inline asm
	abs.s32 	%r75, %r27;
	// inline asm
	cvt.rn.f32.u32	%f15, %r75;
	cvt.f64.f32	%fd41, %f15;
	mul.f64 	%fd53, %fd41, 0d3FE0000000000000;
	abs.f64 	%fd18, %fd53;
	setp.ge.f64	%p14, %fd18, 0d4330000000000000;
	@%p14 bra 	BB26_19;

	add.f64 	%fd42, %fd18, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd43, %fd42;
	setp.lt.f64	%p15, %fd18, 0d3FE0000000000000;
	selp.f64	%fd44, 0d0000000000000000, %fd43, %p15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r77, %temp}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd53;
	}
	and.b32  	%r80, %r79, -2147483648;
	or.b32  	%r81, %r78, %r80;
	mov.b64 	%fd53, {%r77, %r81};

BB26_19:
	cvt.rzi.s32.f64	%r82, %fd53;
	add.s32 	%r110, %r82, %r3;
	mov.u32 	%r111, %r3;

BB26_23:
	cvt.rzi.s32.f32	%r89, %f18;
	setp.ge.s32	%p18, %r107, %r23;
	setp.ge.s32	%p19, %r104, %r22;
	or.pred  	%p20, %p19, %p18;
	add.s32 	%r16, %r111, %r89;
	setp.ge.s32	%p21, %r16, %r24;
	or.pred  	%p22, %p20, %p21;
	setp.ge.s32	%p23, %r105, %r19;
	or.pred  	%p24, %p22, %p23;
	setp.ge.s32	%p25, %r108, %r20;
	or.pred  	%p26, %p24, %p25;
	setp.ge.s32	%p27, %r110, %r21;
	or.pred  	%p28, %p26, %p27;
	@%p28 bra 	BB26_26;

	or.b32  	%r90, %r104, %r105;
	or.b32  	%r91, %r90, %r108;
	or.b32  	%r92, %r91, %r107;
	or.b32  	%r93, %r92, %r16;
	or.b32  	%r94, %r93, %r110;
	setp.lt.s32	%p29, %r94, 0;
	@%p29 bra 	BB26_26;

	mad.lo.s32 	%r95, %r110, %r20, %r108;
	mad.lo.s32 	%r96, %r95, %r19, %r105;
	mad.lo.s32 	%r97, %r16, %r23, %r107;
	mad.lo.s32 	%r98, %r97, %r22, %r104;
	shl.b32 	%r99, %r98, 2;
	add.s32 	%r100, %r18, %r99;
	ld.global.f32 	%f17, [%r100];
	shl.b32 	%r101, %r96, 2;
	add.s32 	%r102, %r17, %r101;
	st.global.f32 	[%r102], %f17;

BB26_26:
	ret;
}

	// .globl	CopyVolumeToNew
.entry CopyVolumeToNew(
	.param .u32 .ptr .global .align 4 CopyVolumeToNew_param_0,
	.param .u32 .ptr .global .align 4 CopyVolumeToNew_param_1,
	.param .u32 CopyVolumeToNew_param_2,
	.param .u32 CopyVolumeToNew_param_3,
	.param .u32 CopyVolumeToNew_param_4,
	.param .u32 CopyVolumeToNew_param_5,
	.param .u32 CopyVolumeToNew_param_6,
	.param .u32 CopyVolumeToNew_param_7,
	.param .u32 CopyVolumeToNew_param_8,
	.param .u32 CopyVolumeToNew_param_9,
	.param .u32 CopyVolumeToNew_param_10,
	.param .u32 CopyVolumeToNew_param_11,
	.param .f32 CopyVolumeToNew_param_12,
	.param .u32 CopyVolumeToNew_param_13
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<19>;
	.reg .b32 	%r<114>;
	.reg .f64 	%fd<55>;


	ld.param.u32 	%r17, [CopyVolumeToNew_param_0];
	ld.param.u32 	%r18, [CopyVolumeToNew_param_1];
	ld.param.u32 	%r19, [CopyVolumeToNew_param_2];
	ld.param.u32 	%r20, [CopyVolumeToNew_param_3];
	ld.param.u32 	%r21, [CopyVolumeToNew_param_4];
	ld.param.u32 	%r22, [CopyVolumeToNew_param_5];
	ld.param.u32 	%r23, [CopyVolumeToNew_param_6];
	ld.param.u32 	%r24, [CopyVolumeToNew_param_7];
	ld.param.u32 	%r25, [CopyVolumeToNew_param_8];
	ld.param.u32 	%r26, [CopyVolumeToNew_param_9];
	ld.param.u32 	%r27, [CopyVolumeToNew_param_10];
	ld.param.u32 	%r28, [CopyVolumeToNew_param_11];
	ld.param.f32 	%f5, [CopyVolumeToNew_param_12];
	ld.param.u32 	%r29, [CopyVolumeToNew_param_13];
	mov.b32	%r30, %envreg3;
	mov.u32 	%r31, %ctaid.x;
	mov.u32 	%r32, %ntid.x;
	mad.lo.s32 	%r33, %r31, %r32, %r30;
	mov.u32 	%r34, %tid.x;
	add.s32 	%r1, %r33, %r34;
	mov.u32 	%r35, %ctaid.y;
	mov.u32 	%r36, %ntid.y;
	mov.b32	%r37, %envreg4;
	mad.lo.s32 	%r38, %r35, %r36, %r37;
	mov.u32 	%r39, %tid.y;
	add.s32 	%r2, %r38, %r39;
	mov.u32 	%r40, %ctaid.z;
	mov.u32 	%r41, %ntid.z;
	mov.b32	%r42, %envreg5;
	mad.lo.s32 	%r43, %r40, %r41, %r42;
	mov.u32 	%r44, %tid.z;
	add.s32 	%r3, %r43, %r44;
	setp.gt.s32	%p1, %r25, 0;
	@%p1 bra 	BB27_4;
	bra.uni 	BB27_1;

BB27_4:
	cvt.rn.f32.s32	%f7, %r25;
	cvt.f64.f32	%fd29, %f7;
	mul.f64 	%fd50, %fd29, 0d3FE0000000000000;
	abs.f64 	%fd6, %fd50;
	setp.ge.f64	%p4, %fd6, 0d4330000000000000;
	@%p4 bra 	BB27_6;

	add.f64 	%fd30, %fd6, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd31, %fd30;
	setp.lt.f64	%p5, %fd6, 0d3FE0000000000000;
	selp.f64	%fd32, 0d0000000000000000, %fd31, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd50;
	}
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r54, %r56;
	mov.b64 	%fd50, {%r53, %r57};

BB27_6:
	cvt.rzi.s32.f64	%r58, %fd50;
	add.s32 	%r5, %r58, %r1;
	mov.u32 	%r107, %r1;
	mov.u32 	%r106, %r5;
	bra.uni 	BB27_7;

BB27_1:
	// inline asm
	abs.s32 	%r45, %r25;
	// inline asm
	cvt.rn.f32.u32	%f6, %r45;
	cvt.f64.f32	%fd25, %f6;
	mul.f64 	%fd49, %fd25, 0d3FE0000000000000;
	abs.f64 	%fd2, %fd49;
	setp.ge.f64	%p2, %fd2, 0d4330000000000000;
	@%p2 bra 	BB27_3;

	add.f64 	%fd26, %fd2, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd27, %fd26;
	setp.lt.f64	%p3, %fd2, 0d3FE0000000000000;
	selp.f64	%fd28, 0d0000000000000000, %fd27, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd28;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd49;
	}
	and.b32  	%r50, %r49, -2147483648;
	or.b32  	%r51, %r48, %r50;
	mov.b64 	%fd49, {%r47, %r51};

BB27_3:
	cvt.rzi.s32.f64	%r52, %fd49;
	add.s32 	%r107, %r52, %r1;
	mov.u32 	%r106, %r1;

BB27_7:
	setp.gt.s32	%p6, %r26, 0;
	@%p6 bra 	BB27_11;
	bra.uni 	BB27_8;

BB27_11:
	cvt.rn.f32.s32	%f9, %r26;
	cvt.f64.f32	%fd37, %f9;
	mul.f64 	%fd52, %fd37, 0d3FE0000000000000;
	abs.f64 	%fd14, %fd52;
	setp.ge.f64	%p9, %fd14, 0d4330000000000000;
	@%p9 bra 	BB27_13;

	add.f64 	%fd38, %fd14, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd39, %fd38;
	setp.lt.f64	%p10, %fd14, 0d3FE0000000000000;
	selp.f64	%fd40, 0d0000000000000000, %fd39, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r67, %temp}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd40;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd52;
	}
	and.b32  	%r70, %r69, -2147483648;
	or.b32  	%r71, %r68, %r70;
	mov.b64 	%fd52, {%r67, %r71};

BB27_13:
	cvt.rzi.s32.f64	%r72, %fd52;
	add.s32 	%r9, %r72, %r2;
	mov.u32 	%r110, %r2;
	mov.u32 	%r109, %r9;
	bra.uni 	BB27_14;

BB27_8:
	// inline asm
	abs.s32 	%r59, %r26;
	// inline asm
	cvt.rn.f32.u32	%f8, %r59;
	cvt.f64.f32	%fd33, %f8;
	mul.f64 	%fd51, %fd33, 0d3FE0000000000000;
	abs.f64 	%fd10, %fd51;
	setp.ge.f64	%p7, %fd10, 0d4330000000000000;
	@%p7 bra 	BB27_10;

	add.f64 	%fd34, %fd10, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd35, %fd34;
	setp.lt.f64	%p8, %fd10, 0d3FE0000000000000;
	selp.f64	%fd36, 0d0000000000000000, %fd35, %p8;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r61, %temp}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd36;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r63}, %fd51;
	}
	and.b32  	%r64, %r63, -2147483648;
	or.b32  	%r65, %r62, %r64;
	mov.b64 	%fd51, {%r61, %r65};

BB27_10:
	cvt.rzi.s32.f64	%r66, %fd51;
	add.s32 	%r110, %r66, %r2;
	mov.u32 	%r109, %r2;

BB27_14:
	cvt.rn.f32.s32	%f10, %r28;
	div.full.f32 	%f1, %f10, %f5;
	abs.f32 	%f11, %f1;
	mov.b32 	 %r73, %f1;
	and.b32  	%r74, %r73, -2147483648;
	or.b32  	%r75, %r74, 1056964608;
	mov.b32 	 %f12, %r75;
	add.f32 	%f13, %f1, %f12;
	cvt.rzi.f32.f32	%f14, %f13;
	setp.gt.f32	%p11, %f11, 0f4B000000;
	selp.f32	%f18, %f1, %f14, %p11;
	setp.geu.f32	%p12, %f11, 0f3F000000;
	@%p12 bra 	BB27_16;

	cvt.rzi.f32.f32	%f18, %f1;

BB27_16:
	setp.gt.s32	%p13, %r27, 0;
	@%p13 bra 	BB27_20;
	bra.uni 	BB27_17;

BB27_20:
	cvt.rn.f32.s32	%f16, %r27;
	cvt.f64.f32	%fd45, %f16;
	mul.f64 	%fd54, %fd45, 0d3FE0000000000000;
	abs.f64 	%fd22, %fd54;
	setp.ge.f64	%p16, %fd22, 0d4330000000000000;
	@%p16 bra 	BB27_22;

	add.f64 	%fd46, %fd22, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd47, %fd46;
	setp.lt.f64	%p17, %fd22, 0d3FE0000000000000;
	selp.f64	%fd48, 0d0000000000000000, %fd47, %p17;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r84, %temp}, %fd48;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd48;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd54;
	}
	and.b32  	%r87, %r86, -2147483648;
	or.b32  	%r88, %r85, %r87;
	mov.b64 	%fd54, {%r84, %r88};

BB27_22:
	cvt.rzi.s32.f64	%r89, %fd54;
	add.s32 	%r13, %r89, %r3;
	mov.u32 	%r112, %r3;
	mov.u32 	%r113, %r13;
	bra.uni 	BB27_23;

BB27_17:
	// inline asm
	abs.s32 	%r76, %r27;
	// inline asm
	cvt.rn.f32.u32	%f15, %r76;
	cvt.f64.f32	%fd41, %f15;
	mul.f64 	%fd53, %fd41, 0d3FE0000000000000;
	abs.f64 	%fd18, %fd53;
	setp.ge.f64	%p14, %fd18, 0d4330000000000000;
	@%p14 bra 	BB27_19;

	add.f64 	%fd42, %fd18, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd43, %fd42;
	setp.lt.f64	%p15, %fd18, 0d3FE0000000000000;
	selp.f64	%fd44, 0d0000000000000000, %fd43, %p15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r78, %temp}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r79}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd53;
	}
	and.b32  	%r81, %r80, -2147483648;
	or.b32  	%r82, %r79, %r81;
	mov.b64 	%fd53, {%r78, %r82};

BB27_19:
	cvt.rzi.s32.f64	%r83, %fd53;
	add.s32 	%r112, %r83, %r3;
	mov.u32 	%r113, %r3;

BB27_23:
	cvt.rzi.s32.f32	%r90, %f18;
	setp.ge.s32	%p18, %r109, %r23;
	setp.ge.s32	%p19, %r106, %r22;
	or.pred  	%p20, %p19, %p18;
	add.s32 	%r16, %r113, %r90;
	setp.ge.s32	%p21, %r16, %r24;
	or.pred  	%p22, %p20, %p21;
	setp.ge.s32	%p23, %r107, %r19;
	or.pred  	%p24, %p22, %p23;
	setp.ge.s32	%p25, %r110, %r20;
	or.pred  	%p26, %p24, %p25;
	setp.ge.s32	%p27, %r112, %r21;
	or.pred  	%p28, %p26, %p27;
	@%p28 bra 	BB27_26;

	or.b32  	%r91, %r106, %r107;
	or.b32  	%r92, %r91, %r110;
	or.b32  	%r93, %r92, %r109;
	or.b32  	%r94, %r93, %r16;
	or.b32  	%r95, %r94, %r112;
	setp.lt.s32	%p29, %r95, 0;
	@%p29 bra 	BB27_26;

	mad.lo.s32 	%r96, %r29, %r21, %r112;
	mad.lo.s32 	%r97, %r96, %r20, %r110;
	mad.lo.s32 	%r98, %r97, %r19, %r107;
	mad.lo.s32 	%r99, %r16, %r23, %r109;
	mad.lo.s32 	%r100, %r99, %r22, %r106;
	shl.b32 	%r101, %r100, 2;
	add.s32 	%r102, %r18, %r101;
	ld.global.f32 	%f17, [%r102];
	shl.b32 	%r103, %r98, 2;
	add.s32 	%r104, %r17, %r103;
	st.global.f32 	[%r104], %f17;

BB27_26:
	ret;
}


  